
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>5.4. Transformers para NER &#8212; Anonimización aplicada al ámbito médico</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="5.3. Código" href="a_code.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo-serikat.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Anonimización aplicada al ámbito médico</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Introducción
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_evaluation.html">
   1. Evaluación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_data.html">
   2. Clinical Corpus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_training.html">
   3. Entrenamiento
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   4. Referencias
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="a.html">
   5. Apéndice
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="a_transfer_learning.html">
     5.1. El concepto de transfer learning en NLP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="a_training.html">
     5.2. Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="a_code.html">
     5.3. Código
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.4. Transformers para NER
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/GuiGel/MedDocAn/cdti?urlpath=tree/cdti/meddocan/a_transformers.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/GuiGel/MedDocAn"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/GuiGel/MedDocAn/issues/new?title=Issue%20on%20page%20%2Fmeddocan/a_transformers.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/meddocan/a_transformers.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../_sources/meddocan/a_transformers.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   5.4.1. Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformadores-multi-idiomas">
   5.4.2. Transformadores multi-idiomas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#una-mirada-mas-cercana-a-la-tokenizacion">
   5.4.3. Una mirada más cercana a la tokenización
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-tuberia-del-tokenizador">
     5.4.3.1. La tubería del tokenizador
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-tokenizador-sentencepiece">
     5.4.3.2. El Tokenizador SentencePiece
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformadores-para-el-ner">
   5.4.4. Transformadores para el NER
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#la-anatomia-del-modelo-de-clase-de-transformers">
   5.4.5. La anatomía del modelo de clase de Transformers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cuerpo-y-cabeza">
     5.4.5.1. Cuerpo y cabeza
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#crear-un-modelopersonalizado-para-la-clasificacion-de-tokens">
     5.4.5.2. Crear un ModeloPersonalizado para la Clasificación de Tokens
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cargar-un-modelo-personalizado">
     5.4.5.3. Cargar un modelo personalizado
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tokenizar-textos-para-el-ner">
   5.4.6. Tokenizar Textos para el NER
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#medidas-de-performance">
   5.4.7. Medidas de Performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ajuste-fino-de-xlm-roberta">
   5.4.8. Ajuste fino de XLM-Roberta
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-del-error">
   5.4.9. Análisis del Error
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Transformers para NER</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   5.4.1. Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformadores-multi-idiomas">
   5.4.2. Transformadores multi-idiomas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#una-mirada-mas-cercana-a-la-tokenizacion">
   5.4.3. Una mirada más cercana a la tokenización
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-tuberia-del-tokenizador">
     5.4.3.1. La tubería del tokenizador
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-tokenizador-sentencepiece">
     5.4.3.2. El Tokenizador SentencePiece
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformadores-para-el-ner">
   5.4.4. Transformadores para el NER
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#la-anatomia-del-modelo-de-clase-de-transformers">
   5.4.5. La anatomía del modelo de clase de Transformers
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cuerpo-y-cabeza">
     5.4.5.1. Cuerpo y cabeza
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#crear-un-modelopersonalizado-para-la-clasificacion-de-tokens">
     5.4.5.2. Crear un ModeloPersonalizado para la Clasificación de Tokens
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cargar-un-modelo-personalizado">
     5.4.5.3. Cargar un modelo personalizado
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tokenizar-textos-para-el-ner">
   5.4.6. Tokenizar Textos para el NER
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#medidas-de-performance">
   5.4.7. Medidas de Performance
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ajuste-fino-de-xlm-roberta">
   5.4.8. Ajuste fino de XLM-Roberta
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-del-error">
   5.4.9. Análisis del Error
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="transformers-para-ner">
<span id="transformers"></span><h1><span class="section-number">5.4. </span>Transformers para NER<a class="headerlink" href="#transformers-para-ner" title="Permalink to this headline">#</a></h1>
<p>Para dar una introducción práctica a los transformadores, exploraremos cómo un modelo de transformador llamado XLM-RoBERTa nos permite realizar esta tarea.  Al igual que el BERT, este modelo utiliza la técnica del lenguaje enmascarado como objetivo de preentrenamiento, pero se entrena conjuntamente con textos en más de cien idiomas. Gracias al preentrenamiento en enormes corpus de muchas lenguas, estos transformadores multilingües permiten la transferencia multilingüe sin necesidad de hornear. Esto significa que un modelo perfeccionado en una lengua puede aplicarse a otras lenguas sin necesidad de formación adicional. Aunque no es nuestro objetivo en esta sección, sería interesante probarlo en nuestros datos jurídicos multilingües, especialmente en los datos catalanes. Además, este es el modelo utilizado por los autores de Flert <span id="id1">[<a class="reference internal" href="bibliography.html#id6" title="Stefan Schweter and A. Akbik. Flert: document-level features for named entity recognition. ArXiv, 2020.">Schweter and Akbik, 2020</a>]</span> y tenemos resultados para él en 3 de las configuraciones probadas.</p>
<div class="cell tag_remove-input tag_remove-output docutils container">
</div>
<section id="dataset">
<h2><span class="section-number">5.4.1. </span>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">asdict</span><span class="p">,</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">DefaultDict</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">ClassLabel</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetDict</span><span class="p">,</span> <span class="n">Features</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Value</span>
<span class="kn">from</span> <span class="nn">spacy.tokens</span> <span class="kn">import</span> <span class="n">Token</span>

<span class="kn">from</span> <span class="nn">meddocan.data</span> <span class="kn">import</span> <span class="n">ArchiveFolder</span>
<span class="kn">from</span> <span class="nn">meddocan.data.docs_iterators</span> <span class="kn">import</span> <span class="n">GsDocs</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Rows</span><span class="p">:</span>
    <span class="n">origin</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">text</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">input_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">ner_tags</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_tok_tag</span><span class="p">(</span><span class="n">token</span><span class="p">:</span> <span class="n">Token</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tag</span> <span class="o">:=</span> <span class="n">token</span><span class="o">.</span><span class="n">ent_iob_</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">]:</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="s2">&quot;O&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">token</span><span class="o">.</span><span class="n">ent_type_</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">text</span>
    <span class="k">if</span> <span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">codecs</span><span class="o">.</span><span class="n">BOM_UTF8</span><span class="p">:</span>

        <span class="c1"># Remove the detected BOOM.</span>
        <span class="c1"># If not removed, the ﻿ sign sometimes appears in</span>
        <span class="c1"># documents in BIO format opened by vscode.</span>

        <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">return</span> <span class="n">text</span><span class="p">,</span> <span class="n">tag</span>


<span class="k">def</span> <span class="nf">meddocan</span><span class="p">(</span><span class="n">folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
    <span class="n">archives</span> <span class="o">=</span> <span class="p">(</span><span class="n">ArchiveFolder</span><span class="o">.</span><span class="n">train</span><span class="p">,</span> <span class="n">ArchiveFolder</span><span class="o">.</span><span class="n">dev</span><span class="p">,</span> <span class="n">ArchiveFolder</span><span class="o">.</span><span class="n">test</span><span class="p">)</span>

    <span class="n">label_count</span> <span class="o">=</span> <span class="n">DefaultDict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">dataset_dict</span> <span class="o">=</span> <span class="n">DefaultDict</span><span class="p">(</span><span class="n">Dataset</span><span class="p">)</span>
    <span class="n">store_rows</span> <span class="o">=</span> <span class="n">DefaultDict</span><span class="p">(</span><span class="n">Rows</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">archive</span> <span class="ow">in</span> <span class="n">archives</span><span class="p">:</span>
        <span class="n">brat_docs</span> <span class="o">=</span> <span class="n">GsDocs</span><span class="p">(</span><span class="n">archive</span><span class="p">)</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">Rows</span><span class="p">()</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">brat_doc</span> <span class="ow">in</span> <span class="n">brat_docs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">brat_doc</span><span class="o">.</span><span class="n">doc</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>

                <span class="c1"># Read text and corresponding tag -&gt; TOKEN, IOB</span>
                <span class="n">tokens</span><span class="p">,</span> <span class="n">iobs</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="n">get_tok_tag</span><span class="p">,</span> <span class="n">sent</span><span class="p">))</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">tokens</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>  <span class="c1"># Remove &quot;\n&quot; tokens</span>
                    <span class="n">tokens</span><span class="p">,</span> <span class="n">iobs</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">iobs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">iob</span> <span class="ow">in</span> <span class="n">iobs</span><span class="p">:</span>
                    <span class="n">label_count</span><span class="p">[</span><span class="n">iob</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">input_ids</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Get index of the key in label_count</span>
                <span class="n">llc</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">label_count</span><span class="p">)</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">llc</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">iobs</span><span class="p">))</span>

                <span class="c1"># Fill Rows Dataclass</span>
                <span class="n">rows</span><span class="o">.</span><span class="n">origin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">brat_doc</span><span class="o">.</span><span class="n">brat_files_pair</span><span class="o">.</span><span class="n">txt</span><span class="o">.</span><span class="n">at</span><span class="p">)</span>
                <span class="n">rows</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sent</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
                <span class="n">rows</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
                <span class="n">rows</span><span class="o">.</span><span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
                <span class="n">rows</span><span class="o">.</span><span class="n">ner_tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

        <span class="n">store_rows</span><span class="p">[</span><span class="n">archive</span><span class="o">.</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="n">rows</span>

    <span class="n">features</span> <span class="o">=</span> <span class="n">Features</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;origin&quot;</span><span class="p">:</span> <span class="n">Value</span><span class="p">(</span><span class="s2">&quot;string&quot;</span><span class="p">),</span>
            <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">Value</span><span class="p">(</span><span class="s2">&quot;string&quot;</span><span class="p">),</span>
            <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">Value</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">),</span>
            <span class="s2">&quot;tokens&quot;</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">(</span><span class="n">Value</span><span class="p">(</span><span class="s2">&quot;string&quot;</span><span class="p">)),</span>
            <span class="s2">&quot;ner_tags&quot;</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">(</span>
                <span class="n">ClassLabel</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">label_count</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">archive</span> <span class="ow">in</span> <span class="n">archives</span><span class="p">:</span>
        <span class="n">dataset_dict</span><span class="p">[</span><span class="n">archive</span><span class="o">.</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
            <span class="n">asdict</span><span class="p">(</span><span class="n">store_rows</span><span class="p">[</span><span class="n">archive</span><span class="o">.</span><span class="n">value</span><span class="p">]),</span> <span class="n">features</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">archive</span><span class="o">.</span><span class="n">value</span>
        <span class="p">)</span>

    <span class="c1"># Construct the DatasetDict</span>
    <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span><span class="n">dataset_dict</span><span class="p">)</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">meddocan</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Utilizamos la función <code class="docutils literal notranslate"><span class="pre">meddocan()</span></code> para crear una instancia de <code class="docutils literal notranslate"><span class="pre">datasets.DatasetDict</span></code> llamada <code class="docutils literal notranslate"><span class="pre">ds</span></code>. Nuestro diccionario de conjunto de datos está compuesto de 3 <code class="docutils literal notranslate"><span class="pre">datasets.Dataset</span></code>, el de entrenamiento, de validación y de test.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">ds</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="n">split</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Aquí utilizamos el método <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> para evitar un sesgue accidental en los datos.</p>
<p>Veamos cuántos ejemplos o frases tenemos por cada dataset accediendo al atributo <code class="docutils literal notranslate"><span class="pre">Dataset.num_rows</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">num_rows</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;number of example per split&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train</th>
      <th>dev</th>
      <th>test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>number of example per split</th>
      <td>10312</td>
      <td>5268</td>
      <td>5155</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Veamos un ejemplo extraído del conjunto de datos</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">11</span><span class="p">:</span><span class="mi">12</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Sentence&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sentence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>origin</th>
      <td>train/brat/S0212-71992006000800008-1.txt</td>
    </tr>
    <tr>
      <th>text</th>
      <td>NASS: 99 00154744 02.</td>
    </tr>
    <tr>
      <th>input_ids</th>
      <td>3651</td>
    </tr>
    <tr>
      <th>tokens</th>
      <td>[NASS, :, 99, 00154744, 02, .]</td>
    </tr>
    <tr>
      <th>ner_tags</th>
      <td>[0, 0, 4, 5, 5, 0]</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>En nuestros objetos <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, las claves de nuestro ejemplo corresponden a los nombres de las columnas de una tabla Arrow <a class="footnote-reference brackets" href="#id7" id="id2">4</a>, mientras que los valores denotan las entradas de cada columna. En particular, vemos que la columna <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> corresponde a la asignación de cada entidad a un ID de clase. Esto es un poco críptico para el ojo humano, así que vamos a crear una nueva columna con las conocidas etiquetas. Para ello, lo primero que hay que observar es que nuestro objeto <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> tiene un atributo <code class="docutils literal notranslate"><span class="pre">features</span></code> que especifica los tipos de datos subyacentes asociados a cada columna:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>origin: Value(dtype=&#39;string&#39;, id=None)
text: Value(dtype=&#39;string&#39;, id=None)
input_ids: Value(dtype=&#39;int32&#39;, id=None)
tokens: Sequence(feature=Value(dtype=&#39;string&#39;, id=None), length=-1, id=None)
ner_tags: Sequence(feature=ClassLabel(num_classes=45, names=[&#39;O&#39;, &#39;B-NOMBRE_SUJETO_ASISTENCIA&#39;, &#39;I-NOMBRE_SUJETO_ASISTENCIA&#39;, &#39;B-ID_SUJETO_ASISTENCIA&#39;, &#39;B-ID_ASEGURAMIENTO&#39;, &#39;I-ID_ASEGURAMIENTO&#39;, &#39;B-CALLE&#39;, &#39;I-CALLE&#39;, &#39;B-TERRITORIO&#39;, &#39;B-FECHAS&#39;, &#39;I-FECHAS&#39;, &#39;B-PAIS&#39;, &#39;B-EDAD_SUJETO_ASISTENCIA&#39;, &#39;I-EDAD_SUJETO_ASISTENCIA&#39;, &#39;B-SEXO_SUJETO_ASISTENCIA&#39;, &#39;B-NOMBRE_PERSONAL_SANITARIO&#39;, &#39;I-NOMBRE_PERSONAL_SANITARIO&#39;, &#39;B-ID_TITULACION_PERSONAL_SANITARIO&#39;, &#39;I-ID_TITULACION_PERSONAL_SANITARIO&#39;, &#39;B-CORREO_ELECTRONICO&#39;, &#39;I-CORREO_ELECTRONICO&#39;, &#39;B-HOSPITAL&#39;, &#39;I-HOSPITAL&#39;, &#39;B-FAMILIARES_SUJETO_ASISTENCIA&#39;, &#39;I-FAMILIARES_SUJETO_ASISTENCIA&#39;, &#39;I-TERRITORIO&#39;, &#39;B-OTROS_SUJETO_ASISTENCIA&#39;, &#39;B-INSTITUCION&#39;, &#39;I-INSTITUCION&#39;, &#39;I-PAIS&#39;, &#39;B-NUMERO_TELEFONO&#39;, &#39;I-NUMERO_TELEFONO&#39;, &#39;B-ID_CONTACTO_ASISTENCIAL&#39;, &#39;B-NUMERO_FAX&#39;, &#39;I-NUMERO_FAX&#39;, &#39;B-CENTRO_SALUD&#39;, &#39;I-CENTRO_SALUD&#39;, &#39;I-ID_SUJETO_ASISTENCIA&#39;, &#39;I-OTROS_SUJETO_ASISTENCIA&#39;, &#39;B-PROFESION&#39;, &#39;I-PROFESION&#39;, &#39;I-SEXO_SUJETO_ASISTENCIA&#39;, &#39;B-ID_EMPLEO_PERSONAL_SANITARIO&#39;, &#39;I-ID_EMPLEO_PERSONAL_SANITARIO&#39;, &#39;I-ID_CONTACTO_ASISTENCIAL&#39;], id=None), length=-1, id=None)
</pre></div>
</div>
</div>
</div>
<p>La clase <code class="docutils literal notranslate"><span class="pre">Sequence</span></code> especifica que el campo contiene una lista de características, que en el caso de <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> corresponde a una lista de características <code class="docutils literal notranslate"><span class="pre">ClassLabel</span></code>. Seleccionemos esta característica del conjunto de entrenamiento de la siguiente manera:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tags</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ClassLabel(num_classes=45, names=[&#39;O&#39;, &#39;B-NOMBRE_SUJETO_ASISTENCIA&#39;, &#39;I-NOMBRE_SUJETO_ASISTENCIA&#39;, &#39;B-ID_SUJETO_ASISTENCIA&#39;, &#39;B-ID_ASEGURAMIENTO&#39;, &#39;I-ID_ASEGURAMIENTO&#39;, &#39;B-CALLE&#39;, &#39;I-CALLE&#39;, &#39;B-TERRITORIO&#39;, &#39;B-FECHAS&#39;, &#39;I-FECHAS&#39;, &#39;B-PAIS&#39;, &#39;B-EDAD_SUJETO_ASISTENCIA&#39;, &#39;I-EDAD_SUJETO_ASISTENCIA&#39;, &#39;B-SEXO_SUJETO_ASISTENCIA&#39;, &#39;B-NOMBRE_PERSONAL_SANITARIO&#39;, &#39;I-NOMBRE_PERSONAL_SANITARIO&#39;, &#39;B-ID_TITULACION_PERSONAL_SANITARIO&#39;, &#39;I-ID_TITULACION_PERSONAL_SANITARIO&#39;, &#39;B-CORREO_ELECTRONICO&#39;, &#39;I-CORREO_ELECTRONICO&#39;, &#39;B-HOSPITAL&#39;, &#39;I-HOSPITAL&#39;, &#39;B-FAMILIARES_SUJETO_ASISTENCIA&#39;, &#39;I-FAMILIARES_SUJETO_ASISTENCIA&#39;, &#39;I-TERRITORIO&#39;, &#39;B-OTROS_SUJETO_ASISTENCIA&#39;, &#39;B-INSTITUCION&#39;, &#39;I-INSTITUCION&#39;, &#39;I-PAIS&#39;, &#39;B-NUMERO_TELEFONO&#39;, &#39;I-NUMERO_TELEFONO&#39;, &#39;B-ID_CONTACTO_ASISTENCIAL&#39;, &#39;B-NUMERO_FAX&#39;, &#39;I-NUMERO_FAX&#39;, &#39;B-CENTRO_SALUD&#39;, &#39;I-CENTRO_SALUD&#39;, &#39;I-ID_SUJETO_ASISTENCIA&#39;, &#39;I-OTROS_SUJETO_ASISTENCIA&#39;, &#39;B-PROFESION&#39;, &#39;I-PROFESION&#39;, &#39;I-SEXO_SUJETO_ASISTENCIA&#39;, &#39;B-ID_EMPLEO_PERSONAL_SANITARIO&#39;, &#39;I-ID_EMPLEO_PERSONAL_SANITARIO&#39;, &#39;I-ID_CONTACTO_ASISTENCIAL&#39;], id=None)
</pre></div>
</div>
</div>
</div>
<p>Podemos utilizar el método <code class="docutils literal notranslate"><span class="pre">ClassLabel.int2str()</span></code> para crear una nueva columna en nuestro conjunto de entrenamiento con nombres de clases para cada etiqueta. Usaremos el método <code class="docutils literal notranslate"><span class="pre">map()</span></code> para devolver un <code class="docutils literal notranslate"><span class="pre">dict</span></code> con la clave correspondiente al nuevo nombre de la columna y el valor como una <code class="docutils literal notranslate"><span class="pre">lista</span></code> de nombres de clases:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_tag_names</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;ner_tags_str&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">tags</span><span class="o">.</span><span class="n">int2str</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]]}</span>

<span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">create_tag_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "ad78bd8f978c4d9fb969b346b0037800"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "fba56f2f8c574ff39514e9ac4e9c2dba"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "50798f411d57499e972643e9b2c402c2"}
</script></div>
</div>
<p>Ahora que tenemos nuestras etiquetas en formato legible para el ser humano, veamos cómo se alinean los tokens y las etiquetas para el primer ejemplo del conjunto de entrenamiento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ds_example</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">12</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">ds_example</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">],</span> <span class="n">ds_example</span><span class="p">[</span><span class="s2">&quot;ner_tags_str&quot;</span><span class="p">]],</span> <span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;Tags&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Tokens</th>
      <td>Fecha</td>
      <td>de</td>
      <td>nacimiento</td>
      <td>:</td>
      <td>01</td>
      <td>/</td>
      <td>01</td>
      <td>/</td>
      <td>1987</td>
      <td>.</td>
    </tr>
    <tr>
      <th>Tags</th>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>B-FECHAS</td>
      <td>I-FECHAS</td>
      <td>I-FECHAS</td>
      <td>I-FECHAS</td>
      <td>I-FECHAS</td>
      <td>O</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Como comprobación rápida de que no tenemos ningún desequilibrio inusual en las etiquetas, calculemos las frecuencias de cada entidad en cada división:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">split2freqs</span> <span class="o">=</span> <span class="n">DefaultDict</span><span class="p">(</span><span class="n">Counter</span><span class="p">)</span>
<span class="k">for</span> <span class="n">split</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;ner_tags_str&quot;</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">row</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">):</span>
                <span class="n">tag_type</span> <span class="o">=</span> <span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">split2freqs</span><span class="p">[</span><span class="n">split</span><span class="p">][</span><span class="n">tag_type</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">class_freq</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">split2freqs</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">class_freq</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train</th>
      <th>dev</th>
      <th>test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ID_CONTACTO_ASISTENCIAL</th>
      <td>77.0</td>
      <td>32.0</td>
      <td>39.0</td>
    </tr>
    <tr>
      <th>PAIS</th>
      <td>713.0</td>
      <td>347.0</td>
      <td>363.0</td>
    </tr>
    <tr>
      <th>FECHAS</th>
      <td>1231.0</td>
      <td>724.0</td>
      <td>611.0</td>
    </tr>
    <tr>
      <th>NOMBRE_PERSONAL_SANITARIO</th>
      <td>1000.0</td>
      <td>497.0</td>
      <td>501.0</td>
    </tr>
    <tr>
      <th>INSTITUCION</th>
      <td>98.0</td>
      <td>72.0</td>
      <td>67.0</td>
    </tr>
    <tr>
      <th>CALLE</th>
      <td>862.0</td>
      <td>434.0</td>
      <td>413.0</td>
    </tr>
    <tr>
      <th>TERRITORIO</th>
      <td>1875.0</td>
      <td>987.0</td>
      <td>956.0</td>
    </tr>
    <tr>
      <th>CORREO_ELECTRONICO</th>
      <td>469.0</td>
      <td>241.0</td>
      <td>249.0</td>
    </tr>
    <tr>
      <th>EDAD_SUJETO_ASISTENCIA</th>
      <td>1035.0</td>
      <td>521.0</td>
      <td>518.0</td>
    </tr>
    <tr>
      <th>SEXO_SUJETO_ASISTENCIA</th>
      <td>925.0</td>
      <td>455.0</td>
      <td>461.0</td>
    </tr>
    <tr>
      <th>ID_ASEGURAMIENTO</th>
      <td>391.0</td>
      <td>194.0</td>
      <td>198.0</td>
    </tr>
    <tr>
      <th>NOMBRE_SUJETO_ASISTENCIA</th>
      <td>1009.0</td>
      <td>503.0</td>
      <td>502.0</td>
    </tr>
    <tr>
      <th>ID_TITULACION_PERSONAL_SANITARIO</th>
      <td>471.0</td>
      <td>226.0</td>
      <td>234.0</td>
    </tr>
    <tr>
      <th>ID_SUJETO_ASISTENCIA</th>
      <td>567.0</td>
      <td>292.0</td>
      <td>283.0</td>
    </tr>
    <tr>
      <th>HOSPITAL</th>
      <td>255.0</td>
      <td>140.0</td>
      <td>130.0</td>
    </tr>
    <tr>
      <th>FAMILIARES_SUJETO_ASISTENCIA</th>
      <td>243.0</td>
      <td>92.0</td>
      <td>81.0</td>
    </tr>
    <tr>
      <th>PROFESION</th>
      <td>24.0</td>
      <td>4.0</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>CENTRO_SALUD</th>
      <td>6.0</td>
      <td>2.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>NUMERO_TELEFONO</th>
      <td>58.0</td>
      <td>25.0</td>
      <td>26.0</td>
    </tr>
    <tr>
      <th>NUMERO_FAX</th>
      <td>15.0</td>
      <td>6.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>OTROS_SUJETO_ASISTENCIA</th>
      <td>9.0</td>
      <td>6.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>ID_EMPLEO_PERSONAL_SANITARIO</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Esto tiene buena pinta: las distribuciones de las frecuencias de nuestras etiquetas son más o menos las mismas para cada división, por lo que los conjuntos de validación y prueba deberían proporcionar una buena medida de la capacidad de generalización de nuestro etiquetador NER. A continuación, vamos a ver algunos transformadores multilingües populares y cómo se pueden adaptar para abordar nuestra tarea NER.</p>
</section>
<section id="transformadores-multi-idiomas">
<h2><span class="section-number">5.4.2. </span>Transformadores multi-idiomas<a class="headerlink" href="#transformadores-multi-idiomas" title="Permalink to this headline">#</a></h2>
<p>Los transformadores multilingües tienen arquitecturas y procedimientos de entrenamiento similares a los de sus homólogos monolingües, con la salvedad de que el corpus utilizado para el preentrenamiento consta de documentos en muchos idiomas. Una característica notable de este enfoque es que, a pesar de no recibir información explícita para diferenciar entre las lenguas, las representaciones lingüísticas resultantes son capaces de generalizar bien entre las lenguas para una variedad de tareas posteriores. En algunos casos, esta capacidad de transferencia entre lenguas puede producir resultados que compiten con los de los modelos monolingües, lo que evita la necesidad de entrenar un modelo por lengua.</p>
<p>Para medir el progreso de la transferencia multilingüe para la NER, se suelen utilizar los conjuntos de datos <code class="docutils literal notranslate"><span class="pre">CoNLL-2002</span></code> y <code class="docutils literal notranslate"><span class="pre">CoNLL-2003</span></code> como referencia para el inglés, el holandés, el español y el alemán. Esta referencia consiste en artículos de noticias anotados con las categorías <code class="docutils literal notranslate"><span class="pre">LOC</span></code>, <code class="docutils literal notranslate"><span class="pre">PER</span></code> y <code class="docutils literal notranslate"><span class="pre">ORG</span></code> que son diferentes de las categorías de MEDDOCAN. Los modelos de transformadores multilingües suelen evaluarse de tres maneras diferentes:</p>
<dl class="simple myst">
<dt>en</dt><dd><p>Se ajustan a los datos de entrenamiento en inglés y luego se evalúan en el conjunto de pruebas de cada idioma. En el caso de nuestros datos jurídicos se podría ajustar sobre nuestros datos en castellano y evaluar sobre nuestros datos en catalán.</p>
</dd>
<dt>Cada uno de ellos</dt><dd><p>Se ajustan y evalúan en datos de prueba monolingües para medir el rendimiento por idioma.</p>
</dd>
<dt>Todos</dt><dd><p>Ajuste fino en todos los datos de entrenamiento para evaluar en todos los conjuntos de prueba de cada idioma.</p>
</dd>
</dl>
<p>En nuestra tarea NER solo nos centraremos en la primera parte. Uno de los primeros transformadores multilingües fue mBERT, que utiliza la misma arquitectura y el mismo objetivo de preentrenamiento que BERT, pero añade artículos de Wikipedia de muchos idiomas al corpus de preentrenamiento. Desde entonces, mBERT ha sido sustituido por XLM-RoBERTa (o XLM-R para abreviar), por lo que ese es el modelo que consideraremos en este capítulo.</p>
<p>XLM-R sólo utiliza MLM (Masked Language Model) como objetivo de preentrenamiento para 100 idiomas, pero se distingue por el enorme tamaño de su corpus de preentrenamiento en comparación con sus predecesores: Volcados de Wikipedia para cada idioma y 2,5 terabytes de datos de Common Crawl de la web. Este corpus es varios órdenes de magnitud más grande que los utilizados en modelos anteriores y proporciona un aumento significativo de la señal para las lenguas con pocos recursos, como el birmano y el suajili, donde sólo existe un pequeño número de artículos de Wikipedia.</p>
<p>La parte RoBERTa <span id="id3">[<a class="reference internal" href="bibliography.html#id27" title="Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: a robustly optimized bert pretraining approach. ArXiv, 2019.">Liu <em>et al.</em>, 2019</a>]</span> del nombre del modelo hace referencia al hecho de que el enfoque de preentrenamiento es el mismo que el de los modelos monolingües RoBERTa. Los desarrolladores de RoBERTa mejoraron varios aspectos de BERT, en particular eliminando por completo la tarea de predicción de la siguiente frase. XLM-R también abandona los embeddings lingüísticos utilizados en XLM y utiliza SentencePiece <span id="id4">[<a class="reference internal" href="bibliography.html#id28" title="Taku Kudo and John Richardson. Sentencepiece: a simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP. 2018.">Kudo and Richardson, 2018</a>]</span> para tokenizar los textos en bruto directamente. Además de su naturaleza multilingüe, una diferencia notable entre XLM-R y RoBERTa es el tamaño de los respectivos vocabularios: ¡250.000 tokens frente a 55.000!</p>
<p>XLM-R es una gran opción para las tareas de NLU multilingüe. En la siguiente sección, exploraremos cómo puede tokenizar eficientemente en castellano.</p>
</section>
<section id="una-mirada-mas-cercana-a-la-tokenizacion">
<h2><span class="section-number">5.4.3. </span>Una mirada más cercana a la tokenización<a class="headerlink" href="#una-mirada-mas-cercana-a-la-tokenizacion" title="Permalink to this headline">#</a></h2>
<p>En lugar de utilizar un tokenizador de WordPiece, XLM-R utiliza un tokenizador llamado SentencePiece que está entrenado en el texto original de los cien idiomas. Para ver cómo se compara SentencePiece con WordPiece, carguemos los tokenizadores BERT para el castellano y XLM-R de la forma habitual con HuggingFace <em>Transformers</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">bert_model_name</span> <span class="o">=</span> <span class="s2">&quot;dccuchile/bert-base-spanish-wwm-cased&quot;</span>
<span class="n">xlmr_model_name</span> <span class="o">=</span> <span class="s2">&quot;xlm-roberta-large&quot;</span>
<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">bert_model_name</span><span class="p">)</span>
<span class="n">xlmr_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">xlmr_model_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Al codificar una pequeña secuencia de texto también podemos recuperar los tokens especiales que cada modelo utilizó durante el preentrenamiento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Jack Sparrow ama Nueva York!&quot;</span>
<span class="n">bert_tokens</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span><span class="p">()</span>
<span class="n">xlmr_tokens</span> <span class="o">=</span> <span class="n">xlmr_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">bert_tokens</span><span class="p">,</span> <span class="n">xlmr_tokens</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;BERT&quot;</span><span class="p">,</span> <span class="s2">&quot;XLM-R&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>BERT</th>
      <td>[CLS]</td>
      <td>J</td>
      <td>##ac</td>
      <td>##k</td>
      <td>Spar</td>
      <td>##ro</td>
      <td>##w</td>
      <td>ama</td>
      <td>Nueva</td>
      <td>Yor</td>
      <td>##k</td>
      <td>!</td>
      <td>[SEP]</td>
    </tr>
    <tr>
      <th>XLM-R</th>
      <td>&lt;s&gt;</td>
      <td>▁Jack</td>
      <td>▁Spar</td>
      <td>row</td>
      <td>▁ama</td>
      <td>▁Nueva</td>
      <td>▁York</td>
      <td>!</td>
      <td>&lt;/s&gt;</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Aquí vemos que en lugar de los tokens <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> y <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> que BERT utiliza para las tareas de clasificación de frases, XLM-R utiliza <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code> y <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code> para denotar el comienzo y el final de una secuencia. Estos tokens se añaden en la etapa final de la tokenización, como veremos a continuación.</p>
<section id="la-tuberia-del-tokenizador">
<h3><span class="section-number">5.4.3.1. </span>La tubería del tokenizador<a class="headerlink" href="#la-tuberia-del-tokenizador" title="Permalink to this headline">#</a></h3>
<p>Hasta ahora hemos tratado la tokenización como una única operación que transforma las cadenas en enteros que podemos pasar por el modelo. Esto no es del todo exacto, y si echamos un vistazo más de cerca podemos ver que en realidad es una tubería de procesamiento completa que normalmente consta de cuatro pasos, como se muestra en la <a class="reference internal" href="#brat-annotator-visualization"><span class="std std-numref">Fig. 5.8</span></a>.</p>
<figure class="align-center" id="brat-annotator-visualization">
<img alt="../_images/transformers_tokenizer-pipeline.png" src="../_images/transformers_tokenizer-pipeline.png" />
<figcaption>
<p><span class="caption-number">Fig. 5.8 </span><span class="caption-text">Las etapas en la tubería de tokenización</span><a class="headerlink" href="#brat-annotator-visualization" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Veamos con más detalle cada paso del procesamiento e ilustremos su efecto con la frase de ejemplo “¡Jack Sparrow ama Nueva York!”:</p>
<dl class="simple myst">
<dt>Normalización</dt><dd><p>Este paso corresponde al conjunto de operaciones que se aplican a una cadena sin procesar para hacerla más “limpia”. Las operaciones más comunes son la eliminación de los espacios en blanco y de los caracteres acentuados. La <a class="reference external" href="https://unicode.org/reports/tr15/">normalización Unicode</a> es otra operación de normalización común que aplican muchos tokenizadores para hacer frente al hecho de que a menudo existen varias formas de escribir el mismo carácter. Esto puede hacer que dos versiones de la “misma” cadena (es decir, con la misma secuencia de caracteres abstractos) parezcan diferentes; los esquemas de normalización Unicode como NFC, NFD, NFKC y NFKD sustituyen las diversas formas de escribir el mismo carácter por formas estándar. Otro ejemplo de normalización es el de las minúsculas. Si se espera que el modelo sólo acepte y utilice caracteres en minúsculas, esta técnica puede utilizarse para reducir el tamaño del vocabulario que requiere. Después de la normalización, nuestra cadena de ejemplo se vería como “¡Jack Sparrow ama Nueva York!”.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>En le caso del corpus Meddocan, hemos quitado los espacios así como los saltos a la linea a la hora de entrenar el modelo con Flair.</p>
</div>
<dl class="simple myst">
<dt><strong>Pretokenización</strong></dt><dd><p>Este paso divide un texto en objetos más pequeños que dan un límite superior a lo que serán sus tokens al final del entrenamiento (Es la tarea de la cual se encargar el <code class="docutils literal notranslate"><span class="pre">meddocan.language.pipeline.meddocan_pipeline</span></code>. Una buena manera de pensar en esto es que el pretokenizador dividirá el texto en “palabras”, y los tokens finales serán partes de esas palabras. En los idiomas que lo permiten (el inglés, el alemán y muchos idiomas indoeuropeos), las cadenas pueden dividirse en palabras a partir de los espacios en blanco y la puntuación. Por ejemplo, este paso podría transformar nuestras [“Jack”, “Sparrow”, “ama”, “Nueva”, “York”, “!”]. A continuación, estas palabras son más sencillas de dividir en subpalabras con los algoritmos Byte-Pair Encoding (BPE) o Unigram en el siguiente paso de la cadena. Sin embargo, la división en “palabras” no siempre es una operación trivial y determinista, ni siquiera una operación que tenga sentido. Por ejemplo, en lenguas como el chino, el japonés o el coreano, la agrupación de símbolos en unidades semánticas como las palabras indoeuropeas puede ser una operación no determinista con varios grupos igualmente válidos. En este caso, podría ser mejor no pretokenizar el texto y, en su lugar, utilizar una biblioteca específica del idioma para la pretokenización.</p>
</dd>
<dt><strong>Modelo de tokenización</strong></dt><dd><p>Una vez normalizados y pre-tokenizados los textos de entrada, el tokenizador aplica un modelo de división de subpalabras a las palabras. Esta es la parte de la cadena de producción que debe ser entrenada en su corpus (o que ha sido entrenada si estamos utilizando un tokenizador pre-entrenado como en nuestro caso). La función del modelo es dividir las palabras en subpalabras para reducir el tamaño del vocabulario e intentar reducir el número de tokens fuera del vocabulario. Existen varios algoritmos de tokenización de subpalabras, como BPE, Unigram y WordPiece. Por ejemplo, nuestro ejemplo en marcha podría ser como [jack, spa, rrow, ama, new, york, !] después de aplicar el modelo de tokenización. Debemos tener en cuenta que en este punto ya no tenemos una lista de cadenas, sino una lista de enteros (IDs de entrada); para mantener el ejemplo ilustrativo, hemos mantenido las palabras pero hemos eliminado las comillas para indicar la transformación.</p>
</dd>
<dt><strong>Posprocesamiento</strong></dt><dd><p>Este es el último paso del proceso de tokenización, en el que se pueden aplicar algunas transformaciones adicionales a la lista de tokens, por ejemplo, añadiendo tokens especiales al principio o al final de la secuencia de índices de entrada. Por ejemplo, un tokenizador de estilo BERT añadiría tokens de clasificación y separadores: [CLS, jack, spa, rrow, ama, new, york, !, SEP]. Esta secuencia (recordemos que será una secuencia de enteros, no los tokens que se ven aquí) se puede introducir en el modelo.</p>
</dd>
</dl>
<p>Volviendo a nuestra comparación de XLM-R y BERT, ahora entendemos que SentencePiece añade &lt;s&gt; y &lt;\s&gt; en lugar de [CLS] y [SEP] en el paso de posprocesamiento (como convención, seguiremos usando [CLS] y [SEP] en las ilustraciones gráficas). Volvamos al tokenizador SentencePiece para ver qué lo hace especial.</p>
</section>
<section id="el-tokenizador-sentencepiece">
<h3><span class="section-number">5.4.3.2. </span>El Tokenizador SentencePiece<a class="headerlink" href="#el-tokenizador-sentencepiece" title="Permalink to this headline">#</a></h3>
<p>El tokenizador de SentencePiece se basa en un tipo de segmentación de subpalabras llamado Unigram y codifica cada texto de entrada como una secuencia de caracteres Unicode. Esta última característica es especialmente útil para los corpus multilingües, ya que permite a SentencePiece ser agnóstico respecto a los acentos, la puntuación y el hecho de que muchos idiomas, como el japonés, no tienen caracteres de espacio en blanco. Otra característica especial de SentencePiece es que a los espacios en blanco se les asigna el símbolo Unicode U+2581, o el carácter ▁, también llamado carácter de cuarto de bloque inferior. Esto permite a SentencePiece destokenizar una secuencia sin ambigüedades y sin depender de pretokenizadores específicos del idioma. En nuestro ejemplo de la sección anterior, por ejemplo, podemos ver que WordPiece ha perdido la información de que no hay espacios en blanco entre “York” y “!”. Por el contrario, SentencePiece conserva los espacios en blanco en el texto tokenizado, de modo que podemos volver a convertirlo en texto crudo sin ambigüedades:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">xlmr_tokens</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;</span><span class="se">\u2581</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;&lt;s&gt; Jack Sparrow ama Nueva York!&lt;/s&gt;&#39;
</pre></div>
</div>
</div>
</div>
<p>Ahora que entendemos cómo funciona SentencePiece, vamos a ver cómo podemos codificar nuestro sencillo ejemplo en una forma adecuada para NER. Lo primero que hay que hacer es cargar el modelo preentrenado con una cabeza de clasificación de tokens. Pero en lugar de cargar esta cabeza directamente desde HuggingFace Transformers, ¡la construiremos nosotros mismos! Profundizando en la API de HuggingFace Transformers, podemos hacer esto con sólo unos pocos pasos.</p>
</section>
</section>
<section id="transformadores-para-el-ner">
<h2><span class="section-number">5.4.4. </span>Transformadores para el NER<a class="headerlink" href="#transformadores-para-el-ner" title="Permalink to this headline">#</a></h2>
<p>Aquí presentamos el modelo de transformador lineal. Cuando se utiliza BERT y otros transformadores de sólo codificación la representación de cada token de entrada individual se introduce en la una capa totalmente conectada para dar salida a la entidad del token. Por este motivo, la NER se suele plantear como una tarea de clasificación de tokens. El proceso se parece al diagrama de la Figura <a class="reference internal" href="#ner-achitecture"><span class="std std-numref">Fig. 5.9</span></a>.</p>
<figure class="align-center" id="ner-achitecture">
<img alt="../_images/transformers_ner-architecture.png" src="../_images/transformers_ner-architecture.png" />
<figcaption>
<p><span class="caption-number">Fig. 5.9 </span><span class="caption-text">Ajuste de un transformador basado en un codificador para el reconocimiento de entidades con nombre</span><a class="headerlink" href="#ner-achitecture" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Hasta aquí todo bien, pero ¿cómo debemos tratar las subpalabras en una tarea de clasificación de tokens? Por ejemplo, el nombre “Christa” de la figura <a class="reference internal" href="#ner-achitecture"><span class="std std-numref">Fig. 5.9</span></a> está tokenizado en las subpalabras “Chr” y “##ista”, así que ¿a cuál(es) debe asignarse la etiqueta B-PER?</p>
<p>En el documento del BERT <span id="id5">[<a class="reference internal" href="bibliography.html#id5" title="Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: pre-training of deep bidirectional transformers for language understanding. ArXiv, 2019.">Devlin <em>et al.</em>, 2019</a>]</span>, los autores asignaron esta etiqueta a la primera subpalabra (“Chr” en nuestro ejemplo) e ignoraron la siguiente (“##ista”). Esta es la convención que adoptaremos aquí, e indicaremos las subpalabras ignoradas con <code class="docutils literal notranslate"><span class="pre">IGN</span></code>. Posteriormente, podemos propagar fácilmente la etiqueta predicha de la primera subpalabra a las subpalabras siguientes en el paso de posprocesamiento. También podríamos haber optado por incluir la representación de la subpalabra “##ista” asignándole una copia de la etiqueta <code class="docutils literal notranslate"><span class="pre">B-LOC</span></code>, pero esto viola el formato IOB2.</p>
<p>Afortunadamente, todos los aspectos de la arquitectura que hemos visto en BERT se trasladan a XLM-R, ya que su arquitectura se basa en RoBERTa, ¡que es idéntica a BERT!</p>
</section>
<section id="la-anatomia-del-modelo-de-clase-de-transformers">
<h2><span class="section-number">5.4.5. </span>La anatomía del modelo de clase de Transformers<a class="headerlink" href="#la-anatomia-del-modelo-de-clase-de-transformers" title="Permalink to this headline">#</a></h2>
<p><em>Transformers</em> está organizada en torno a clases dedicadas a cada arquitectura y tarea. Las clases del modelo asociadas a las diferentes tareas se nombran según la convención <code class="docutils literal notranslate"><span class="pre">&lt;NombreDelModelo&gt;For&lt;Tarea&gt;</span></code>, o <code class="docutils literal notranslate"><span class="pre">AutoModelFor&lt;Tarea&gt;</span></code> cuando se utilizan las clases AutoModel.</p>
<p>Para profundizar en la API de la libaría <em>Transformers</em>, vamos a ampliar el modelo existente para resolver nuestro problema de NLP. Podemos cargar los pesos de los modelos pre-entrenados y tenemos acceso a funciones de ayuda específicas para cada tarea. Esto nos permite construir modelos personalizados para objetivos específicos con muy poca sobrecarga. En esta sección, veremos cómo podemos implementar nuestro propio modelo personalizado.</p>
<section id="cuerpo-y-cabeza">
<h3><span class="section-number">5.4.5.1. </span>Cuerpo y cabeza<a class="headerlink" href="#cuerpo-y-cabeza" title="Permalink to this headline">#</a></h3>
<p>El concepto principal que hace que los transformers sean tan versátiles es la división de la arquitectura en un cuerpo y una cabeza. Ya hemos visto <a class="reference internal" href="a_transfer_learning.html#transfer-learning-anexo"><span class="std std-numref">Section 5.1</span></a> que cuando pasamos de la tarea de preentrenamiento a la tarea posterior, tenemos que sustituir la última capa del modelo por una que sea adecuada para la tarea. Esta última capa se llama cabeza del modelo; es la parte que es específica de la tarea. El resto del modelo se denomina cuerpo, e incluye las capas de embeddings de tokens y de transformación que son independientes de la tarea. Esta estructura se refleja también en el código de la librería <em>Transformers</em>: el cuerpo de un modelo se implementa en una clase como <code class="docutils literal notranslate"><span class="pre">BertModel</span></code> o <code class="docutils literal notranslate"><span class="pre">GPT2Model</span></code> que devuelve los estados ocultos de la última capa. Los modelos específicos de tareas como <code class="docutils literal notranslate"><span class="pre">BertForMaskedLM</span></code> o <code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code> utilizan el modelo base y añaden la cabeza necesaria sobre los estados ocultos, como se muestra en la Figura <a class="reference internal" href="#bert-body-head"><span class="std std-numref">Fig. 5.10</span></a>.</p>
<figure class="align-center" id="bert-body-head">
<img alt="../_images/transformers_bert-body-head.png" src="../_images/transformers_bert-body-head.png" />
<figcaption>
<p><span class="caption-number">Fig. 5.10 </span><span class="caption-text">La clase <code class="docutils literal notranslate"><span class="pre">BertModel</span></code> sólo contiene el cuerpo del modelo, mientras que las clases <code class="docutils literal notranslate"><span class="pre">BertFor&lt;Task&gt;</span></code> combinan el cuerpo con una cabeza dedicada a una tarea determinada</span><a class="headerlink" href="#bert-body-head" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Como vamos a ver a continuación, esta separación de cuerpos y cabezas nos permite construir una cabeza personalizada para cualquier tarea y simplemente montarla sobre un modelo pre-entrenado.</p>
</section>
<section id="crear-un-modelopersonalizado-para-la-clasificacion-de-tokens">
<h3><span class="section-number">5.4.5.2. </span>Crear un ModeloPersonalizado para la Clasificación de Tokens<a class="headerlink" href="#crear-un-modelopersonalizado-para-la-clasificacion-de-tokens" title="Permalink to this headline">#</a></h3>
<p>Hagamos el ejercicio de construir un cabezal de clasificación de tokens personalizado para XLM-R. Dado que XLM-R utiliza la misma arquitectura de modelo que RoBERTa, utilizaremos RoBERTa como modelo base, pero aumentado con ajustes específicos para XLM-R. Tenga en cuenta que este es un ejercicio educativo para mostrarle cómo construir un modelo personalizado que se podría modificar fácilmente. Para la clasificación de tokens, ya existe una clase <code class="docutils literal notranslate"><span class="pre">XLMRobertaForTokenClassification</span></code> que se puede importar desde huggingface <em>Transformers</em>.</p>
<p>Para empezar, necesitamos una estructura de datos que represente nuestro etiquetador XLM-R NER. Como primera aproximación, necesitaremos un objeto de configuración para inicializar el modelo y una función <code class="docutils literal notranslate"><span class="pre">forward()</span></code> para generar las salidas. Sigamos adelante y construyamos nuestra clase XLM-R para la clasificación de tokens:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">XLMRobertaConfig</span>
<span class="kn">from</span> <span class="nn">transformers.modeling_outputs</span> <span class="kn">import</span> <span class="n">TokenClassifierOutput</span>
<span class="kn">from</span> <span class="nn">transformers.models.roberta.modeling_roberta</span> <span class="kn">import</span> <span class="n">RobertaModel</span>
<span class="kn">from</span> <span class="nn">transformers.models.roberta.modeling_roberta</span> <span class="kn">import</span> <span class="n">RobertaPreTrainedModel</span>

<span class="k">class</span> <span class="nc">XLMRobertaForTokenClassification</span><span class="p">(</span><span class="n">RobertaPreTrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">XLMRobertaConfig</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>
        <span class="c1"># Load model body</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span> <span class="o">=</span> <span class="n">RobertaModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Set up token classification head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>
        <span class="c1"># Load and initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Use model body to get encoder representations</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roberta</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                               <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># Apply classifier to encoder representation</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span>
        <span class="c1"># Calculate losses</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Return model output object</span>
        <span class="k">return</span> <span class="n">TokenClassifierOutput</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
                                     <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
                                     <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La <code class="docutils literal notranslate"><span class="pre">config_class</span></code> asegura que se usen los parámetros estándar de XLM-R cuando inicializamos un nuevo modelo. Si queremos cambiar los parámetros por defecto, podemos hacerlo sobrescribiendo los ajustes por defecto en la configuración. Con el método <code class="docutils literal notranslate"><span class="pre">super()</span></code> llamamos a la función de inicialización de la clase <code class="docutils literal notranslate"><span class="pre">RobertaPreTrainedModel</span></code>. Esta clase abstracta se encarga de la inicialización o carga de los pesos pre-entrenados. Luego cargamos el cuerpo de nuestro modelo, que es <code class="docutils literal notranslate"><span class="pre">RobertaModel</span></code>, y lo extendemos con nuestra propia cabeza de clasificación que consiste en un dropout y una capa feed-forward estándar. Hay que notar que establecemos <code class="docutils literal notranslate"><span class="pre">add_pooling_layer=False</span></code> para asegurarnos de que se devuelven todos los estados ocultos y no sólo el asociado al token [CLS]. Finalmente, inicializamos todos los pesos llamando al método <code class="docutils literal notranslate"><span class="pre">init_weights()</span></code> que heredamos de <code class="docutils literal notranslate"><span class="pre">RobertaPreTrainedModel</span></code>, que cargará los pesos pre-entrenados para el cuerpo del modelo e inicializará aleatoriamente los pesos de nuestra cabeza de clasificación de tokens.</p>
<p>Lo único que queda por hacer es definir lo que el modelo debe hacer en un pase hacia adelante con un método <code class="docutils literal notranslate"><span class="pre">forward()</span></code> como es de uso con <code class="docutils literal notranslate"><span class="pre">Pytorch</span></code> <a class="footnote-reference brackets" href="#id8" id="id6">5</a>. Durante el pase hacia adelante, los datos son primero alimentados a través del cuerpo del modelo. Hay un número de variables de entrada, pero las únicas que necesitamos por ahora son <code class="docutils literal notranslate"><span class="pre">input_ids</span></code> y <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code>. El estado oculto, que forma parte de la salida del cuerpo del modelo, se alimenta entonces a través de las capas de “dropout” y clasificación. Si también proporcionamos etiquetas en el pase forward, podemos calcular directamente la pérdida. Si hay una máscara de atención, tenemos que hacer un poco más de trabajo para asegurarnos de que sólo calculamos la pérdida de los tokens no enmascarados. Por último, envolvemos todas las salidas en un objeto <code class="docutils literal notranslate"><span class="pre">TokenClassifierOutput</span></code> que nos permite acceder a los elementos de una tupla.</p>
<p>Con sólo implementar dos funciones de una clase simple, podemos construir nuestro propio modelo de transformador personalizado. Y como heredamos de un <code class="docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>, obtenemos instantáneamente acceso a todas las utilidades de huggingface Transformer, como <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code>. Veamos cómo podemos cargar los pesos pre-entrenados en nuestro modelo personalizado.</p>
</section>
<section id="cargar-un-modelo-personalizado">
<h3><span class="section-number">5.4.5.3. </span>Cargar un modelo personalizado<a class="headerlink" href="#cargar-un-modelo-personalizado" title="Permalink to this headline">#</a></h3>
<p>Ahora estamos listos para cargar nuestro modelo de clasificación de tokens. Necesitaremos proporcionar alguna información adicional más allá del nombre del modelo, incluyendo las etiquetas que usaremos para etiquetar cada entidad y el mapeo de cada etiqueta a un ID y viceversa. Toda esta información puede derivarse de nuestra variable tags, que como objeto <code class="docutils literal notranslate"><span class="pre">ClassLabel</span></code> tiene un atributo names que podemos utilizar para derivar el mapeo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index2tag</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">tag</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">)}</span>
<span class="n">tag2index</span> <span class="o">=</span> <span class="p">{</span><span class="n">tag</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">)}</span>
</pre></div>
</div>
</div>
</div>
<p>Almacenaremos estos mapeos y el atributo <code class="docutils literal notranslate"><span class="pre">tags.num_classes</span></code> en el objeto <code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code>. Pasar argumentos de palabras clave al método <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> anula los valores por defecto:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>

<span class="n">xlmr_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">xlmr_model_name</span><span class="p">,</span>
                                         <span class="n">num_labels</span><span class="o">=</span><span class="n">tags</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                                         <span class="n">id2label</span><span class="o">=</span><span class="n">index2tag</span><span class="p">,</span> <span class="n">label2id</span><span class="o">=</span><span class="n">tag2index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>La clase <code class="docutils literal notranslate"><span class="pre">AutoConfig</span></code> contiene el plano de la arquitectura de un modelo. Cuando cargamos un modelo con <code class="docutils literal notranslate"><span class="pre">AutoModel.from_pretrained(model_ckpt)</span></code>, el archivo de configuración asociado a ese modelo se descarga automáticamente. Sin embargo, si queremos modificar algo como el número de clases o los nombres de las etiquetas, podemos cargar primero la configuración con los parámetros que queremos personalizar.</p>
<p>Ahora, podemos cargar los pesos del modelo como siempre con la función <code class="docutils literal notranslate"><span class="pre">from_pretrained()</span></code> con el argumento adicional config. Hay que tener en cuenta que no hemos implementado la carga de pesos pre-entrenados en nuestra clase de modelo personalizada; la obtenemos gratuitamente al heredar de <code class="docutils literal notranslate"><span class="pre">RobertaPreTrainedModel</span></code>:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">xlmr_model</span> <span class="o">=</span> <span class="p">(</span><span class="n">XLMRobertaForTokenClassification</span>
              <span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">xlmr_model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">xlmr_config</span><span class="p">)</span>
              <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Como comprobación rápida de que hemos inicializado el tokenizador y el modelo correctamente, vamos a probar las predicciones en nuestra pequeña secuencia de entidades conocidas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">xlmr_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">xlmr_tokens</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;Input IDs&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Tokens</th>
      <td>&lt;s&gt;</td>
      <td>▁Jack</td>
      <td>▁Spar</td>
      <td>row</td>
      <td>▁ama</td>
      <td>▁Nueva</td>
      <td>▁York</td>
      <td>!</td>
      <td>&lt;/s&gt;</td>
    </tr>
    <tr>
      <th>Input IDs</th>
      <td>0</td>
      <td>21763</td>
      <td>37456</td>
      <td>15555</td>
      <td>2527</td>
      <td>111191</td>
      <td>5753</td>
      <td>38</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Como se puede ver aquí, los tokens iniciales <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code> y finales <code class="docutils literal notranslate"><span class="pre">&lt;/s&gt;</span></code> reciben los IDs 0 y 2, respectivamente.</p>
<p>Por último, tenemos que pasar las entradas al modelo y extraer las predicciones tomando el argmax para obtener la clase más probable por token:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">xlmr_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">logits</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Número de tokens en la secuencia: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">xlmr_tokens</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Forma de la salida: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Número de tokens en la secuencia: 9
Forma de la salida: torch.Size([1, 9, 45])
</pre></div>
</div>
</div>
</div>
<p>Aquí vemos que los logits tienen la forma <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">num_tokens,</span> <span class="pre">num_tags]</span></code>, y que a cada token se le asigna un logit entre las 42 posibles etiquetas NER. Al enumerar sobre la secuencia, podemos ver rápidamente lo que predice el modelo pre-entrenado:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="p">[</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">xlmr_tokens</span><span class="p">,</span> <span class="n">preds</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;Tags&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Tokens</th>
      <th>Tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>1</th>
      <td>▁Jack</td>
      <td>B-CORREO_ELECTRONICO</td>
    </tr>
    <tr>
      <th>2</th>
      <td>▁Spar</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>3</th>
      <td>row</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>4</th>
      <td>▁ama</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>5</th>
      <td>▁Nueva</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>6</th>
      <td>▁York</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>7</th>
      <td>!</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>8</th>
      <td>&lt;/s&gt;</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>No es de extrañar que nuestra capa de clasificación de tokens con pesos aleatorios deje mucho que desear; ¡afinemos con algunos datos etiquetados para mejorarla! Antes de hacerlo, vamos a envolver los pasos anteriores en una función de ayuda para su uso posterior:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tag_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tags</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="c1"># Get tokens with speecial characters</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">tokens</span><span class="p">()</span>
    <span class="c1"># Encode the sequence into IDs</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Get predictions as distribution over 45 possible classes</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Take argmax to get most likely class per token</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Convert to DataFrame</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="p">[</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">tokens</span><span class="p">,</span> <span class="n">preds</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;Tags&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Antes de poder entrenar el modelo, también necesitamos tokenizar las entradas y preparar las etiquetas. Lo haremos a continuación.</p>
</section>
</section>
<section id="tokenizar-textos-para-el-ner">
<h2><span class="section-number">5.4.6. </span>Tokenizar Textos para el NER<a class="headerlink" href="#tokenizar-textos-para-el-ner" title="Permalink to this headline">#</a></h2>
<p>Ahora que hemos establecido que el tokenizador y el modelo pueden codificar un solo ejemplo, nuestro siguiente paso es tokenizar todo el conjunto de datos para poder pasarlo al modelo XLM-R para su ajuste. HuggingFace Datasets proporciona una forma rápida de tokenizar un objeto <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> con la operación <code class="docutils literal notranslate"><span class="pre">map()</span></code>. Para ello, recordemos que primero tenemos que definir una función con la firma mínima:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">function</span><span class="p">(</span><span class="n">examples</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">]</span>
</pre></div>
</div>
<p>donde <code class="docutils literal notranslate"><span class="pre">ejemplos</span></code> equivale a una porción de un <code class="docutils literal notranslate"><span class="pre">Conjunto</span> <span class="pre">de</span> <span class="pre">datos</span></code>, por ejemplo, <code class="docutils literal notranslate"><span class="pre">ds['train'][:10]</span></code>. Dado que el tokenizador XLM-R devuelve los IDs de entrada para las entradas del modelo, sólo tenemos que aumentar esta información con la máscara de atención y los IDs de las etiquetas que codifican la información sobre qué token está asociado a cada etiqueta NER.</p>
<p>Siguiendo el enfoque adoptado en la documentación de HuggingFace <em>Transformers</em>, veamos cómo funciona esto con nuestro único ejemplo de Meddocan, recogiendo primero las palabras y las etiquetas como listas normales:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">ds_example</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">],</span> <span class="n">ds_example</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>A continuación, se tokeniza cada palabra y se utiliza el argumento <code class="docutils literal notranslate"><span class="pre">is_split_into_words</span></code> para indicar al tokenizador que nuestra secuencia de entrada ya ha sido dividida en palabras:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_input</span> <span class="o">=</span> <span class="n">xlmr_tokenizer</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">xlmr_tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">tokenized_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">tokens</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Tokens</th>
      <td>&lt;s&gt;</td>
      <td>▁Fe</td>
      <td>cha</td>
      <td>▁de</td>
      <td>▁na</td>
      <td>cimiento</td>
      <td>▁:</td>
      <td>▁01</td>
      <td>▁/</td>
      <td>▁01</td>
      <td>▁/</td>
      <td>▁1987</td>
      <td>▁</td>
      <td>.</td>
      <td>&lt;/s&gt;</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>En este ejemplo podemos ver que el tokenizador ha dividido “Fecha” en dos subpalabras, “▁Fe” y “cha”. Como seguimos la convención de que sólo “▁Fe” debe asociarse a la etiqueta <code class="docutils literal notranslate"><span class="pre">B-FECHAS</span></code>, necesitamos una forma de enmascarar las representaciones de las subpalabras después de la primera subpalabra. Afortunadamente, <code class="docutils literal notranslate"><span class="pre">tokenized_input</span></code> es una clase que contiene una función <code class="docutils literal notranslate"><span class="pre">word_ids()</span></code> que puede ayudarnos a conseguirlo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words_id</span> <span class="o">=</span> <span class="n">tokenized_input</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">tokens</span><span class="p">,</span> <span class="n">words_id</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;Word IDs&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Tokens</th>
      <td>&lt;s&gt;</td>
      <td>▁Fe</td>
      <td>cha</td>
      <td>▁de</td>
      <td>▁na</td>
      <td>cimiento</td>
      <td>▁:</td>
      <td>▁01</td>
      <td>▁/</td>
      <td>▁01</td>
      <td>▁/</td>
      <td>▁1987</td>
      <td>▁</td>
      <td>.</td>
      <td>&lt;/s&gt;</td>
    </tr>
    <tr>
      <th>Word IDs</th>
      <td>None</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>9</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Aquí podemos ver que <code class="docutils literal notranslate"><span class="pre">word_ids</span></code> ha asignado a cada subpalabra el índice correspondiente en la secuencia de palabras, por lo que a la primera subpalabra, “▁Fe”, se le asigna el índice 0, mientras que a “cha” se le asigna el índice 1 (ya que “cha” es la segunda palabra en palabras). También podemos ver que los tokens especiales como <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code> y <code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code> se asignan a <code class="docutils literal notranslate"><span class="pre">None</span></code>. Fijemos -100 como etiqueta para estos tokens especiales y las subpalabras que deseamos enmascarar durante el entrenamiento:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">previous_word_idx</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">label_ids</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">word_idx</span> <span class="ow">in</span> <span class="n">words_id</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">word_idx</span> <span class="o">==</span> <span class="n">previous_word_idx</span><span class="p">:</span>
        <span class="n">label_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">word_idx</span> <span class="o">!=</span> <span class="n">previous_word_idx</span><span class="p">:</span>
        <span class="n">label_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_idx</span><span class="p">)</span>
    <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="n">word_idx</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">index2tag</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span> <span class="k">else</span> <span class="s2">&quot;IGN&quot;</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">label_ids</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Token&quot;</span><span class="p">,</span> <span class="s2">&quot;Word IDs&quot;</span><span class="p">,</span> <span class="s2">&quot;Label IDs&quot;</span><span class="p">,</span> <span class="s2">&quot;Labels&quot;</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">tokens</span><span class="p">,</span> <span class="n">words_id</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">],</span> <span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Token</th>
      <th>Word IDs</th>
      <th>Label IDs</th>
      <th>Labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;</td>
      <td>None</td>
      <td>-100</td>
      <td>IGN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>▁Fe</td>
      <td>0</td>
      <td>0</td>
      <td>O</td>
    </tr>
    <tr>
      <th>2</th>
      <td>cha</td>
      <td>0</td>
      <td>-100</td>
      <td>IGN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>▁de</td>
      <td>1</td>
      <td>1</td>
      <td>B-NOMBRE_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>4</th>
      <td>▁na</td>
      <td>2</td>
      <td>2</td>
      <td>I-NOMBRE_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>5</th>
      <td>cimiento</td>
      <td>2</td>
      <td>-100</td>
      <td>IGN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>▁:</td>
      <td>3</td>
      <td>3</td>
      <td>B-ID_SUJETO_ASISTENCIA</td>
    </tr>
    <tr>
      <th>7</th>
      <td>▁01</td>
      <td>4</td>
      <td>4</td>
      <td>B-ID_ASEGURAMIENTO</td>
    </tr>
    <tr>
      <th>8</th>
      <td>▁/</td>
      <td>5</td>
      <td>5</td>
      <td>I-ID_ASEGURAMIENTO</td>
    </tr>
    <tr>
      <th>9</th>
      <td>▁01</td>
      <td>6</td>
      <td>6</td>
      <td>B-CALLE</td>
    </tr>
    <tr>
      <th>10</th>
      <td>▁/</td>
      <td>7</td>
      <td>7</td>
      <td>I-CALLE</td>
    </tr>
    <tr>
      <th>11</th>
      <td>▁1987</td>
      <td>8</td>
      <td>8</td>
      <td>B-TERRITORIO</td>
    </tr>
    <tr>
      <th>12</th>
      <td>▁</td>
      <td>9</td>
      <td>9</td>
      <td>B-FECHAS</td>
    </tr>
    <tr>
      <th>13</th>
      <td>.</td>
      <td>9</td>
      <td>-100</td>
      <td>IGN</td>
    </tr>
    <tr>
      <th>14</th>
      <td>&lt;/s&gt;</td>
      <td>None</td>
      <td>-100</td>
      <td>IGN</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>¿Por qué elegimos -100 como el ID para enmascarar las representaciones de subpalabras? La razón es que en PyTorch la clase de pérdida de entropía cruzada <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code> tiene un atributo llamado <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> cuyo valor es -100. Este índice se ignora durante el entrenamiento, por lo que podemos utilizarlo para ignorar los tokens asociados a subpalabras consecutivas.</p>
</div>
<p>Y ya está. Podemos ver claramente cómo los IDs de las etiquetas se alinean con los tokens, así que vamos a escalar esto a todo el conjunto de datos definiendo una única función que envuelva toda la lógica:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_and_align_labels</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">tokenized_inputs</span> <span class="o">=</span> <span class="n">xlmr_tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;ner_tags&quot;</span><span class="p">]):</span>
        <span class="n">word_ids</span> <span class="o">=</span> <span class="n">tokenized_inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">label_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word_idx</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word_idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">word_idx</span> <span class="o">==</span> <span class="n">previous_word_idx</span><span class="p">:</span>
                <span class="n">label_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">label_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">word_idx</span><span class="p">])</span>
            <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="n">word_idx</span>
        <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)</span>
    <span class="n">tokenized_inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">return</span> <span class="n">tokenized_inputs</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora tenemos todos los ingredientes que necesitamos para codificar cada división, así que vamos a escribir una función sobre la que podamos iterar:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">encode_meddocan_dataset</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">corpus</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_and_align_labels</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                      <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">,</span> <span class="s1">&#39;tokens&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Al aplicar esta función a un objeto <code class="docutils literal notranslate"><span class="pre">DatasetDict</span></code>, obtenemos un objeto <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> codificado por división. Usemos esto para codificar nuestro corpus Meddocan:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">meddocan_encoded</span> <span class="o">=</span> <span class="n">encode_meddocan_dataset</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "fd513ed95e994728b6cba8b26cc4b3a3"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "e4303ac6ad924180a4bcacb767ddf075"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "511d8bf63d704dfc897d560be03ca375"}
</script></div>
</div>
<p>Ahora que tenemos un modelo y un conjunto de datos, tenemos que definir una métrica de performance.</p>
</section>
<section id="medidas-de-performance">
<h2><span class="section-number">5.4.7. </span>Medidas de Performance<a class="headerlink" href="#medidas-de-performance" title="Permalink to this headline">#</a></h2>
<p>Como lo sabemos, la evaluación de un modelo NER es similar a la evaluación de un modelo de clasificación de textos, y es habitual informar de los resultados de precisión, recuperación y puntuación <span class="math notranslate nohighlight">\(F_{1}\)</span>. La única sutileza es que todas las palabras de una entidad deben predecirse correctamente para que la predicción se considere correcta. Para cambiar vamos a utilizar una ingeniosa biblioteca llamada <em>seqeval</em> que está diseñada para este tipo de tareas. Por ejemplo, dadas algunas etiquetas NER y predicciones del modelo, podemos calcular las métricas mediante la función <code class="docutils literal notranslate"><span class="pre">classification_report()</span></code> de <em>seqeval</em>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">seqeval.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;B-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;I-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;I-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">],</span>
          <span class="p">[</span><span class="s2">&quot;B-PER&quot;</span><span class="p">,</span> <span class="s2">&quot;I-PER&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">]]</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">,</span> <span class="s2">&quot;B-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;I-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;I-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;I-MISC&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">],</span>
          <span class="p">[</span><span class="s2">&quot;B-PER&quot;</span><span class="p">,</span> <span class="s2">&quot;I-PER&quot;</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

        MISC       0.00      0.00      0.00         1
         PER       1.00      1.00      1.00         1

   micro avg       0.50      0.50      0.50         2
   macro avg       0.50      0.50      0.50         2
weighted avg       0.50      0.50      0.50         2
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Hay que subrayar que la métricas definida por <code class="docutils literal notranslate"><span class="pre">seqeval</span></code> se calculan aquí a nivel de tokens, mientras que en la evaluación final, véase capítulo <a class="reference internal" href="01_evaluation.html#content-evaluation"><span class="std std-numref">Section 1</span></a> se calculan a nivel de Spans.</p>
</div>
<p>Como podemos ver, <em>seqeval</em> espera las predicciones y las etiquetas como listas de listas, con cada lista correspondiente a un solo ejemplo en nuestros conjuntos de validación o prueba. Para integrar estas métricas durante el entrenamiento, necesitamos una función que pueda tomar las salidas del modelo y convertirlas en las listas que seqeval espera. Lo siguiente hace el truco asegurando que ignoramos los IDs de etiquetas asociados con subpalabras posteriores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">align_predictions</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">labels_list</span><span class="p">,</span> <span class="n">preds_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">example_labels</span><span class="p">,</span> <span class="n">example_preds</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># Ignore label IDs = -100</span>
            <span class="k">if</span> <span class="n">label_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">seq_idx</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">:</span>
                <span class="n">example_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index2tag</span><span class="p">[</span><span class="n">label_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">seq_idx</span><span class="p">]])</span>
                <span class="n">example_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index2tag</span><span class="p">[</span><span class="n">preds</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">seq_idx</span><span class="p">]])</span>

        <span class="n">labels_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">example_labels</span><span class="p">)</span>
        <span class="n">preds_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">example_preds</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">preds_list</span><span class="p">,</span> <span class="n">labels_list</span>
</pre></div>
</div>
</div>
</div>
<p>Una vez que disponemos de una métrica de rendimiento, podemos pasar a entrenar el modelo.</p>
</section>
<section id="ajuste-fino-de-xlm-roberta">
<h2><span class="section-number">5.4.8. </span>Ajuste fino de XLM-Roberta<a class="headerlink" href="#ajuste-fino-de-xlm-roberta" title="Permalink to this headline">#</a></h2>
<p>Ahora tenemos todos los ingredientes para poner afinar nuestro modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">downsample</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">logging_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span><span class="o">*</span><span class="n">downsample</span> <span class="o">//</span> <span class="n">batch_size</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xlmr_model_name</span><span class="si">}</span><span class="s2">-finetuned-meddocan&quot;</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">log_level</span><span class="o">=</span><span class="s2">&quot;error&quot;</span><span class="p">,</span> <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-6</span><span class="p">,</span>
    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span> <span class="n">save_steps</span><span class="o">=</span><span class="mf">1e6</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">disable_tqdm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logging_steps</span><span class="o">=</span><span class="n">logging_steps</span><span class="p">,</span> <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">remove_unused_columns</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Aquí evaluamos las predicciones del modelo en el conjunto de validación al final de cada epoch, ajustamos el decaimiento del peso, y establecemos <code class="docutils literal notranslate"><span class="pre">save_steps</span></code> a un número grande para desactivar el checkpointing y así acelerar el entrenamiento.</p>
<p>También necesitamos decirle al <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> cómo calcular las métricas en el conjunto de validación, así que aquí podemos usar la función <code class="docutils literal notranslate"><span class="pre">align_predictions()</span></code> que definimos antes para extraer las predicciones y las etiquetas en el formato que necesita <em>seqeval</em> para calcular la puntuación F1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">seqeval.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">=</span> <span class="n">align_predictions</span><span class="p">(</span><span class="n">eval_pred</span><span class="o">.</span><span class="n">predictions</span><span class="p">,</span>
                                       <span class="n">eval_pred</span><span class="o">.</span><span class="n">label_ids</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)}</span>
</pre></div>
</div>
</div>
</div>
<p>El último paso es definir un <em>data collator</em> para que podamos rellenar cada secuencia de entrada con la mayor longitud de secuencia en un lote. HuggingFace <em>Transformers</em> proporciona un colador de datos dedicado para la clasificación de tokens que rellenará las etiquetas junto con las entradas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForTokenClassification</span>

<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorForTokenClassification</span><span class="p">(</span><span class="n">xlmr_tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>El relleno de las etiquetas es necesario porque, al contrario que en una tarea de clasificación de texto, las etiquetas son también secuencias. Un detalle importante aquí es que las secuencias de etiquetas se rellenan con el valor -100, que, como hemos visto, es ignorado por las funciones de pérdida de PyTorch.</p>
<p>Inicializamos un nuevo modelo para cada Trainer creando un método <code class="docutils literal notranslate"><span class="pre">model_init()</span></code>. Este método carga un modelo no entrenado y se llama al principio de la llamada a <code class="docutils literal notranslate"><span class="pre">train()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_init</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="p">(</span><span class="n">XLMRobertaForTokenClassification</span>
            <span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">xlmr_model_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">xlmr_config</span><span class="p">)</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora podemos pasar toda esta información junto con los conjuntos de datos codificados al Trainer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">indexes</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">]:</span>
    <span class="n">indexes</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">meddocan_encoded</span><span class="p">[</span><span class="n">split</span><span class="p">]</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="n">downsample</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Usamos el método <code class="docutils literal notranslate"><span class="pre">select()</span></code> para dividir el tamaño de nuestro dataset a fin de acortar el entrenamiento.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model_init</span><span class="o">=</span><span class="n">model_init</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
                  <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
                  <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
                  <span class="n">train_dataset</span><span class="o">=</span><span class="n">meddocan_encoded</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">indexes</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]),</span>
                  <span class="n">eval_dataset</span><span class="o">=</span><span class="n">meddocan_encoded</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">indexes</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">]),</span>
                  <span class="n">tokenizer</span><span class="o">=</span><span class="n">xlmr_tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>y luego ejecutar el bucle de entrenamiento como sigue:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_output</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <div>

      <progress value='387' max='387' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [387/387 07:38, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1.699400</td>
      <td>0.855253</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.659100</td>
      <td>0.556888</td>
      <td>0.129032</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.486100</td>
      <td>0.463781</td>
      <td>0.250000</td>
    </tr>
  </tbody>
</table><p></div></div>
</div>
<p>Vamos a cargar un modelo que hemos entrenado previamente con todos los datos y con 150 iteraciones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">pretrained_loc</span> <span class="o">=</span> <span class="s2">&quot;GuiGel/xlm-roberta-large-finetuned-meddocan&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XLMRobertaForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_loc</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a evaluar este modelo para conocer sus métricas sobre el conjunto de datos de validación.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
<span class="n">evals</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">evals</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<div>

  <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [66/66 00:14]
</div>
</div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>evaluation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>eval_loss</th>
      <td>0.023452</td>
    </tr>
    <tr>
      <th>eval_f1</th>
      <td>0.973730</td>
    </tr>
    <tr>
      <th>eval_runtime</th>
      <td>14.742900</td>
    </tr>
    <tr>
      <th>eval_samples_per_second</th>
      <td>17.839000</td>
    </tr>
    <tr>
      <th>eval_steps_per_second</th>
      <td>4.477000</td>
    </tr>
    <tr>
      <th>epoch</th>
      <td>3.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>El modelo tiene una puntuación <span class="math notranslate nohighlight">\(F_{1}\)</span> micro excelente de 97.39 sobre el conjunto de datos de desarrollo. Para confirmar que nuestro modelo funciona como se espera, vamos a probarlo con una muestra de nuestro dataset de test:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][</span><span class="mi">10</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
<span class="n">tag_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tags</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">xlmr_tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Tokens</th>
      <th>Tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>&lt;s&gt;</td>
      <td>O</td>
    </tr>
    <tr>
      <th>1</th>
      <td>▁Médico</td>
      <td>O</td>
    </tr>
    <tr>
      <th>2</th>
      <td>:</td>
      <td>O</td>
    </tr>
    <tr>
      <th>3</th>
      <td>▁Ne</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>4</th>
      <td>rea</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>5</th>
      <td>▁Sen</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>6</th>
      <td>arri</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>7</th>
      <td>aga</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>8</th>
      <td>▁Ruiz</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>9</th>
      <td>▁de</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>10</th>
      <td>▁la</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>11</th>
      <td>▁Illa</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>12</th>
      <td>▁No</td>
      <td>O</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Col</td>
      <td>O</td>
    </tr>
    <tr>
      <th>14</th>
      <td>:</td>
      <td>O</td>
    </tr>
    <tr>
      <th>15</th>
      <td>▁20</td>
      <td>B-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>16</th>
      <td>▁20</td>
      <td>I-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>17</th>
      <td>▁</td>
      <td>I-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>18</th>
      <td>349</td>
      <td>I-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>19</th>
      <td>13</td>
      <td>I-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>20</th>
      <td>.</td>
      <td>O</td>
    </tr>
    <tr>
      <th>21</th>
      <td>&lt;/s&gt;</td>
      <td>O</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>El resultado parece convincente. Ahora aplicamos el mismo modelo sobre el texto ya tokenizado.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toks</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][</span><span class="mi">10</span><span class="p">][</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">tag_tokens</span><span class="p">(</span><span class="n">toks</span><span class="p">,</span> <span class="n">tags</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="c1"># Get sub tokens from tokens</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">toks</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="c1"># Encode the sequence into IDs</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Get predictions as distribution over 45 possible classes</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Take argmax to get most likely class per token</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Verify that there is the same number of words ids then prediction</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">word_ids</span><span class="p">())</span> <span class="o">==</span> <span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># Associate each tokens with the ner tag of the first subtoken</span>
    <span class="n">words_tag</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="p">[</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">word_idx</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">tokens</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(),</span> <span class="n">preds</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">word_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">previous_word_idx</span> <span class="o">!=</span> <span class="n">word_idx</span> <span class="ow">and</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">words_tag</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
            <span class="n">previous_word_idx</span> <span class="o">=</span> <span class="n">word_idx</span>
    <span class="c1"># Visualize with a DataFrame</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">toks</span><span class="p">,</span> <span class="n">words_tag</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;Ner Tags&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="n">tag_tokens</span><span class="p">(</span><span class="n">toks</span><span class="p">,</span> <span class="n">tags</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">xlmr_tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Tokens</th>
      <th>Ner Tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Médico</td>
      <td>O</td>
    </tr>
    <tr>
      <th>1</th>
      <td>:</td>
      <td>O</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Nerea</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Senarriaga</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Ruiz</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>5</th>
      <td>de</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>6</th>
      <td>la</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Illa</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>8</th>
      <td></td>
      <td>O</td>
    </tr>
    <tr>
      <th>9</th>
      <td>NºCol</td>
      <td>O</td>
    </tr>
    <tr>
      <th>10</th>
      <td>:</td>
      <td>B-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>11</th>
      <td>20</td>
      <td>I-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>12</th>
      <td>20</td>
      <td>I-ID_TITULACION_PERSONAL_SANITARIO</td>
    </tr>
    <tr>
      <th>13</th>
      <td>34913</td>
      <td>O</td>
    </tr>
    <tr>
      <th>14</th>
      <td>.</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Funciona. Pero nunca debemos confiar demasiado en el rendimiento basándonos en un solo ejemplo. En su lugar, debemos realizar una investigación adecuada y exhaustiva de los errores del modelo. En la siguiente sección estudiamos cómo hacerlo para la tarea NER.</p>
</section>
<section id="analisis-del-error">
<h2><span class="section-number">5.4.9. </span>Análisis del Error<a class="headerlink" href="#analisis-del-error" title="Permalink to this headline">#</a></h2>
<p>Vamos a dedicar un minuto a investigar los errores de nuestro modelo. Un análisis exhaustivo de los errores de nuestro modelo es uno de los aspectos más importantes a la hora de entrenar y depurar transformadores (y modelos de aprendizaje automático en general). Hay varios modos de fallo en los que puede parecer que el modelo funciona bien, mientras que en la práctica tiene algunos fallos graves. Algunos ejemplos en los que el entrenamiento puede fallar son</p>
<ul class="simple">
<li><p>Podríamos enmascarar accidentalmente demasiados tokens y también enmascarar algunas de nuestras etiquetas para obtener una caída de pérdidas realmente prometedora.</p></li>
<li><p>La función <code class="docutils literal notranslate"><span class="pre">compute_metrics()</span></code> podría tener un error que sobrestima el verdadero rendimiento.</p></li>
<li><p>Podríamos incluir la clase cero o la entidad O en NER como una clase normal, lo que sesgaría mucho la precisión y la puntuación F1, ya que es la clase mayoritaria por un gran margen.</p></li>
</ul>
<p>Cuando el modelo funciona mucho peor de lo esperado, el examen de los errores puede aportar información útil y revelar fallos que serían difíciles de detectar con sólo mirar el código. E incluso si el modelo funciona bien y no hay errores en el código, el análisis de errores sigue siendo una herramienta útil para entender los puntos fuertes y débiles del modelo. Estos son aspectos que siempre debemos tener en cuenta cuando desplegamos un modelo en un entorno de producción.</p>
<p>Para nuestro análisis, volveremos a utilizar una de las herramientas más potentes de las que disponemos, que es mirar los ejemplos de validación con mayor pérdida. Calcularemos una pérdida por token en la secuencia de muestras.</p>
<p>Definamos un método que podamos aplicar al conjunto de validación:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">cross_entropy</span>

<span class="k">def</span> <span class="nf">forward_pass_with_label</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="c1"># Remove keys from batch</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;origin&quot;</span><span class="p">,</span> <span class="s2">&quot;ner_tags_str&quot;</span><span class="p">],</span>
                 <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>
    <span class="c1"># Convert dict of lists to list of dicts suitable for data collator</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>
    <span class="c1"># Pad inputs and labels and put all tensors on device</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">data_collator</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Pass data through model</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="c1"># logit.size: [batch_size, sequence_length, classes]</span>
        <span class="c1"># Predict class with largest logit value on classes axis</span>
        <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1"># Calculate loss per token after flattening batch dimension with view</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">45</span><span class="p">),</span>
                         <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
    <span class="c1"># Unflatten batch dimension and convert to numpy array</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span><span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;predicted_label&quot;</span><span class="p">:</span> <span class="n">predicted_label</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora podemos aplicar esta función a todo el conjunto de validación utilizando <code class="docutils literal notranslate"><span class="pre">map()</span></code> y cargar todos los datos en un <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> para su posterior análisis:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">valid_set</span> <span class="o">=</span> <span class="n">meddocan_encoded</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">]</span>
<span class="n">valid_set</span> <span class="o">=</span> <span class="n">valid_set</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">forward_pass_with_label</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                          <span class="n">remove_columns</span><span class="o">=</span> <span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;origin&quot;</span><span class="p">,</span> <span class="s2">&quot;ner_tags_str&quot;</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">valid_set</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter &#39;function&#39;=&lt;function forward_pass_with_label at 0x7f4200b550d0&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed.
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "ff167df7964f49449de7936068eccac7"}
</script></div>
</div>
<p>Los tokens y las etiquetas siguen codificados con sus IDs, así que vamos a mapear los tokens y las etiquetas de nuevo a cadenas para que sea más fácil leer los resultados. A los tokens de relleno con etiqueta -100 les asignamos una etiqueta especial, <code class="docutils literal notranslate"><span class="pre">IGN</span></code>, para poder filtrarlos después. También nos deshacemos de todo el relleno en los campos <code class="docutils literal notranslate"><span class="pre">loss</span></code> y <code class="docutils literal notranslate"><span class="pre">predicted_label</span></code> truncándolos a la longitud de las entradas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index2tag</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;IGN&quot;</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;input_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">xlmr_tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;predicted_label&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;predicted_label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">index2tag</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">index2tag</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;predicted_label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;predicted_label&#39;</span><span class="p">][:</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>input_ids</th>
      <td>[0, 853, 4007, 3911, 196, 152, 59629, 21713, 3...</td>
    </tr>
    <tr>
      <th>attention_mask</th>
      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>
    </tr>
    <tr>
      <th>labels</th>
      <td>[IGN, O, IGN, IGN, O, O, B-NOMBRE_PERSONAL_SAN...</td>
    </tr>
    <tr>
      <th>loss</th>
      <td>[0.0, 5.960463e-07, 0.0, 0.0, 7.152555e-07, 5....</td>
    </tr>
    <tr>
      <th>predicted_label</th>
      <td>[I-CORREO_ELECTRONICO, O, O, O, O, O, B-NOMBRE...</td>
    </tr>
    <tr>
      <th>input_tokens</th>
      <td>[&lt;s&gt;, ▁Re, mit, ido, ▁por, ▁:, ▁Roberto, ▁Gall...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Cada columna contiene una lista de tokens, etiquetas, etiquetas previstas, etc. para cada muestra. Echemos un vistazo a los tokens individualmente descomponiendo estas listas. La función <code class="docutils literal notranslate"><span class="pre">pandas.Series.explode()</span></code> nos permite hacer exactamente eso en una línea creando una fila para cada elemento de la lista original de filas. Como todas las listas de una fila tienen la misma longitud, podemos hacer esto en paralelo para todas las columnas. También eliminamos las fichas de relleno que llamamos <code class="docutils literal notranslate"><span class="pre">IGN</span></code>, ya que su pérdida es cero de todos modos. Finalmente, convertimos las pérdidas, que siguen siendo objetos <code class="docutils literal notranslate"><span class="pre">numpy.Array</span></code>, en floats estándar:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_tokens</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="o">.</span><span class="n">explode</span><span class="p">)</span>
<span class="n">df_tokens</span> <span class="o">=</span> <span class="n">df_tokens</span><span class="p">[</span><span class="n">df_tokens</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;IGN&quot;</span><span class="p">]</span>
<span class="n">df_tokens</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_tokens</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">df_tokens</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>input_ids</th>
      <th>attention_mask</th>
      <th>labels</th>
      <th>loss</th>
      <th>predicted_label</th>
      <th>input_tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>853</td>
      <td>1</td>
      <td>O</td>
      <td>0.0</td>
      <td>O</td>
      <td>▁Re</td>
    </tr>
    <tr>
      <th>0</th>
      <td>196</td>
      <td>1</td>
      <td>O</td>
      <td>0.0</td>
      <td>O</td>
      <td>▁por</td>
    </tr>
    <tr>
      <th>0</th>
      <td>152</td>
      <td>1</td>
      <td>O</td>
      <td>0.0</td>
      <td>O</td>
      <td>▁:</td>
    </tr>
    <tr>
      <th>0</th>
      <td>59629</td>
      <td>1</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
      <td>0.0</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
      <td>▁Roberto</td>
    </tr>
    <tr>
      <th>0</th>
      <td>21713</td>
      <td>1</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>0.0</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>▁Gall</td>
    </tr>
    <tr>
      <th>0</th>
      <td>52813</td>
      <td>1</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>0.0</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>▁Pina</td>
    </tr>
    <tr>
      <th>0</th>
      <td>46348</td>
      <td>1</td>
      <td>O</td>
      <td>0.0</td>
      <td>O</td>
      <td>▁Corre</td>
    </tr>
    <tr>
      <th>0</th>
      <td>37697</td>
      <td>1</td>
      <td>O</td>
      <td>0.0</td>
      <td>O</td>
      <td>▁electrónico</td>
    </tr>
    <tr>
      <th>0</th>
      <td>152</td>
      <td>1</td>
      <td>O</td>
      <td>0.0</td>
      <td>O</td>
      <td>▁:</td>
    </tr>
    <tr>
      <th>0</th>
      <td>2062</td>
      <td>1</td>
      <td>B-CORREO_ELECTRONICO</td>
      <td>0.0</td>
      <td>B-CORREO_ELECTRONICO</td>
      <td>▁ro</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Con los datos en esta forma, ahora podemos agruparlos por los tokens de entrada y agregar las pérdidas de cada token con el recuento, la media y la suma. Por último, ordenamos los datos agregados por la suma de las pérdidas y vemos qué tokens han acumulado más pérdidas en el conjunto de validación:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span>
    <span class="n">df_tokens</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;input_tokens&quot;</span><span class="p">)[[</span><span class="s2">&quot;loss&quot;</span><span class="p">]]</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">droplevel</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Get rid of multi-level columns</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>input_tokens</th>
      <th>count</th>
      <th>mean</th>
      <th>sum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>▁</td>
      <td>15810</td>
      <td>0.02</td>
      <td>311.82</td>
    </tr>
    <tr>
      <th>1</th>
      <td>▁de</td>
      <td>8298</td>
      <td>0.03</td>
      <td>260.94</td>
    </tr>
    <tr>
      <th>2</th>
      <td>▁-</td>
      <td>942</td>
      <td>0.14</td>
      <td>132.84</td>
    </tr>
    <tr>
      <th>3</th>
      <td>▁28</td>
      <td>275</td>
      <td>0.41</td>
      <td>112.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>▁y</td>
      <td>2999</td>
      <td>0.03</td>
      <td>77.20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>▁años</td>
      <td>586</td>
      <td>0.10</td>
      <td>61.03</td>
    </tr>
    <tr>
      <th>6</th>
      <td>▁la</td>
      <td>3082</td>
      <td>0.02</td>
      <td>49.23</td>
    </tr>
    <tr>
      <th>7</th>
      <td>▁C</td>
      <td>270</td>
      <td>0.18</td>
      <td>48.58</td>
    </tr>
    <tr>
      <th>8</th>
      <td>▁/</td>
      <td>2150</td>
      <td>0.02</td>
      <td>47.57</td>
    </tr>
    <tr>
      <th>9</th>
      <td>▁pareja</td>
      <td>9</td>
      <td>5.22</td>
      <td>46.96</td>
    </tr>
    <tr>
      <th>10</th>
      <td>▁dos</td>
      <td>130</td>
      <td>0.34</td>
      <td>43.59</td>
    </tr>
    <tr>
      <th>11</th>
      <td>▁meses</td>
      <td>247</td>
      <td>0.16</td>
      <td>40.67</td>
    </tr>
    <tr>
      <th>12</th>
      <td>▁en</td>
      <td>2194</td>
      <td>0.02</td>
      <td>39.84</td>
    </tr>
    <tr>
      <th>13</th>
      <td>▁Merc</td>
      <td>3</td>
      <td>12.83</td>
      <td>38.49</td>
    </tr>
    <tr>
      <th>14</th>
      <td>▁5</td>
      <td>193</td>
      <td>0.20</td>
      <td>37.66</td>
    </tr>
    <tr>
      <th>15</th>
      <td>▁matern</td>
      <td>4</td>
      <td>9.03</td>
      <td>36.12</td>
    </tr>
    <tr>
      <th>16</th>
      <td>▁Mu</td>
      <td>55</td>
      <td>0.60</td>
      <td>32.98</td>
    </tr>
    <tr>
      <th>17</th>
      <td>▁año</td>
      <td>62</td>
      <td>0.53</td>
      <td>32.95</td>
    </tr>
    <tr>
      <th>18</th>
      <td>▁Edi</td>
      <td>5</td>
      <td>6.38</td>
      <td>31.89</td>
    </tr>
    <tr>
      <th>19</th>
      <td>▁13</td>
      <td>96</td>
      <td>0.32</td>
      <td>31.17</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Podemos observar varios patrones en esta lista:</p>
<ul class="simple">
<li><p>El espacio en blanco tiene la mayor pérdida total, lo que no es sorprendente, ya que también es el símbolo más común de la lista. Sin embargo, su pérdida media es menor que la de los demás símbolos de la lista a parte de [“de”, “y”, “la”, “/”, “en”] que se encuentra en el mismo caso. Esto significa que el modelo no tiene problemas para clasificarlos.</p></li>
<li><p>Palabras como “-”, “años”, “meses” y “dos” aparecen con relativa frecuencia. A menudo aparecen junto a entidades con nombre y a veces forman parte de ellas, lo que explica que el modelo pueda confundirlas.</p></li>
<li><p>Algunas palabras como “pareja”, “Merc”, “matern” o “Edi” tienen una pérdida media muy alta a la vez que son poco frecuentes. Los investigaremos más a fondo.</p></li>
</ul>
<p>También podemos agrupar las identificaciones de las etiquetas y observar las pérdidas de cada clase:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_loss_per_tag</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">df_tokens</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">)[[</span><span class="s2">&quot;loss&quot;</span><span class="p">]]</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">droplevel</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">mean_loss_per_tag</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>labels</th>
      <th>count</th>
      <th>mean</th>
      <th>sum</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>I-ID_EMPLEO_PERSONAL_SANITARIO</td>
      <td>4</td>
      <td>21.28</td>
      <td>85.10</td>
    </tr>
    <tr>
      <th>1</th>
      <td>B-ID_EMPLEO_PERSONAL_SANITARIO</td>
      <td>1</td>
      <td>21.22</td>
      <td>21.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>10</td>
      <td>12.72</td>
      <td>127.17</td>
    </tr>
    <tr>
      <th>3</th>
      <td>I-PROFESION</td>
      <td>5</td>
      <td>11.96</td>
      <td>59.78</td>
    </tr>
    <tr>
      <th>4</th>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
      <td>6</td>
      <td>10.12</td>
      <td>60.71</td>
    </tr>
    <tr>
      <th>5</th>
      <td>I-SEXO_SUJETO_ASISTENCIA</td>
      <td>1</td>
      <td>9.06</td>
      <td>9.06</td>
    </tr>
    <tr>
      <th>6</th>
      <td>B-PROFESION</td>
      <td>4</td>
      <td>4.47</td>
      <td>17.88</td>
    </tr>
    <tr>
      <th>7</th>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>36</td>
      <td>3.35</td>
      <td>120.53</td>
    </tr>
    <tr>
      <th>8</th>
      <td>I-ID_SUJETO_ASISTENCIA</td>
      <td>32</td>
      <td>3.11</td>
      <td>99.40</td>
    </tr>
    <tr>
      <th>9</th>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>92</td>
      <td>2.15</td>
      <td>197.69</td>
    </tr>
    <tr>
      <th>10</th>
      <td>B-INSTITUCION</td>
      <td>72</td>
      <td>2.07</td>
      <td>148.87</td>
    </tr>
    <tr>
      <th>11</th>
      <td>I-INSTITUCION</td>
      <td>181</td>
      <td>1.67</td>
      <td>302.14</td>
    </tr>
    <tr>
      <th>12</th>
      <td>I-PAIS</td>
      <td>2</td>
      <td>0.60</td>
      <td>1.20</td>
    </tr>
    <tr>
      <th>13</th>
      <td>B-HOSPITAL</td>
      <td>140</td>
      <td>0.58</td>
      <td>81.40</td>
    </tr>
    <tr>
      <th>14</th>
      <td>I-NUMERO_FAX</td>
      <td>10</td>
      <td>0.46</td>
      <td>4.64</td>
    </tr>
    <tr>
      <th>15</th>
      <td>I-HOSPITAL</td>
      <td>443</td>
      <td>0.42</td>
      <td>188.15</td>
    </tr>
    <tr>
      <th>16</th>
      <td>I-ID_ASEGURAMIENTO</td>
      <td>390</td>
      <td>0.38</td>
      <td>147.02</td>
    </tr>
    <tr>
      <th>17</th>
      <td>I-TERRITORIO</td>
      <td>207</td>
      <td>0.31</td>
      <td>64.50</td>
    </tr>
    <tr>
      <th>18</th>
      <td>B-ID_ASEGURAMIENTO</td>
      <td>194</td>
      <td>0.26</td>
      <td>50.40</td>
    </tr>
    <tr>
      <th>19</th>
      <td>B-ID_SUJETO_ASISTENCIA</td>
      <td>292</td>
      <td>0.25</td>
      <td>74.45</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Vemos que <code class="docutils literal notranslate"><span class="pre">I-ID_EMPLEO_PERSONAL_SANITARIO</span></code> junto con <code class="docutils literal notranslate"><span class="pre">ID_EMPLEO_PERSONAL_SANITARIO</span></code> tiene la mayor pérdida media, lo que significa que determinar el inicio del ID de un empleo de personal sanitario supone un reto para nuestro modelo.</p>
<p>¡Es un resultado esperado sabiendo que la etiqueta <code class="docutils literal notranslate"><span class="pre">ID_EMPLEO_PERSONAL_SANITARIO</span></code> no aparece en nuestro conjunto de datos de entrenamiento!</p>
<p>Para comprobar que las clases menos representada en el conjunto de datos de entrenamiento son también las que tienen una pérdida media más elevada vamos a calcular la frecuencia de las clases en el conjunto de entrenamiento y visualizar la en la nueva columna <code class="docutils literal notranslate"><span class="pre">train_freq</span></code> de <code class="docutils literal notranslate"><span class="pre">mean_loss_per_tag</span></code>.</p>
<div class="cell tag_hide-ouput docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_set</span> <span class="o">=</span> <span class="n">meddocan_encoded</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">train_set</span><span class="o">.</span><span class="n">to_pandas</span><span class="p">()[[</span><span class="s2">&quot;labels&quot;</span><span class="p">]]</span>
<span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">index2tag</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="o">.</span><span class="n">explode</span><span class="p">)</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;IGN&quot;</span><span class="p">]</span>
<span class="n">train_labels_freq</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ahora visualizamos por cada etiqueta su frecuencia en la columna <code class="docutils literal notranslate"><span class="pre">train_freq</span></code> de <code class="docutils literal notranslate"><span class="pre">mean_loss_per_tag</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_loss_per_tag</span><span class="p">[</span><span class="s2">&quot;train_freq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_loss_per_tag</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">train_labels_freq</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">mean_loss_per_tag</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>labels</th>
      <th>count</th>
      <th>mean</th>
      <th>sum</th>
      <th>train_freq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>I-ID_EMPLEO_PERSONAL_SANITARIO</td>
      <td>4</td>
      <td>21.28</td>
      <td>85.10</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>B-ID_EMPLEO_PERSONAL_SANITARIO</td>
      <td>1</td>
      <td>21.22</td>
      <td>21.22</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>10</td>
      <td>12.72</td>
      <td>127.17</td>
      <td>23</td>
    </tr>
    <tr>
      <th>3</th>
      <td>I-PROFESION</td>
      <td>5</td>
      <td>11.96</td>
      <td>59.78</td>
      <td>28</td>
    </tr>
    <tr>
      <th>4</th>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
      <td>6</td>
      <td>10.12</td>
      <td>60.71</td>
      <td>9</td>
    </tr>
    <tr>
      <th>5</th>
      <td>I-SEXO_SUJETO_ASISTENCIA</td>
      <td>1</td>
      <td>9.06</td>
      <td>9.06</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>B-PROFESION</td>
      <td>4</td>
      <td>4.47</td>
      <td>17.88</td>
      <td>24</td>
    </tr>
    <tr>
      <th>7</th>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>36</td>
      <td>3.35</td>
      <td>120.53</td>
      <td>133</td>
    </tr>
    <tr>
      <th>8</th>
      <td>I-ID_SUJETO_ASISTENCIA</td>
      <td>32</td>
      <td>3.11</td>
      <td>99.40</td>
      <td>53</td>
    </tr>
    <tr>
      <th>9</th>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>92</td>
      <td>2.15</td>
      <td>197.69</td>
      <td>243</td>
    </tr>
    <tr>
      <th>10</th>
      <td>B-INSTITUCION</td>
      <td>72</td>
      <td>2.07</td>
      <td>148.87</td>
      <td>98</td>
    </tr>
    <tr>
      <th>11</th>
      <td>I-INSTITUCION</td>
      <td>181</td>
      <td>1.67</td>
      <td>302.14</td>
      <td>222</td>
    </tr>
    <tr>
      <th>12</th>
      <td>I-PAIS</td>
      <td>2</td>
      <td>0.60</td>
      <td>1.20</td>
      <td>22</td>
    </tr>
    <tr>
      <th>13</th>
      <td>B-HOSPITAL</td>
      <td>140</td>
      <td>0.58</td>
      <td>81.40</td>
      <td>255</td>
    </tr>
    <tr>
      <th>14</th>
      <td>I-NUMERO_FAX</td>
      <td>10</td>
      <td>0.46</td>
      <td>4.64</td>
      <td>30</td>
    </tr>
    <tr>
      <th>15</th>
      <td>I-HOSPITAL</td>
      <td>443</td>
      <td>0.42</td>
      <td>188.15</td>
      <td>850</td>
    </tr>
    <tr>
      <th>16</th>
      <td>I-ID_ASEGURAMIENTO</td>
      <td>390</td>
      <td>0.38</td>
      <td>147.02</td>
      <td>769</td>
    </tr>
    <tr>
      <th>17</th>
      <td>I-TERRITORIO</td>
      <td>207</td>
      <td>0.31</td>
      <td>64.50</td>
      <td>275</td>
    </tr>
    <tr>
      <th>18</th>
      <td>B-ID_ASEGURAMIENTO</td>
      <td>194</td>
      <td>0.26</td>
      <td>50.40</td>
      <td>391</td>
    </tr>
    <tr>
      <th>19</th>
      <td>B-ID_SUJETO_ASISTENCIA</td>
      <td>292</td>
      <td>0.25</td>
      <td>74.45</td>
      <td>567</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Comprobamos que <code class="docutils literal notranslate"><span class="pre">ID_EMPLEO_PERSONAL_SANITARIO</span></code> no existe en nuestros datos de entrenamiento y que varias de las etiquetas con una pérdida elevada tienen una frecuencia baja en el conjunto de entrenamiento.</p>
<p>Podemos desglosar esto aún más trazando la matriz de confusión de la clasificación de tokens, donde vemos que el comienzo del sexo de un sujeto de asistencia se confunde a menudo con el token <code class="docutils literal notranslate"><span class="pre">I-SEXO_SUJETO_ASISTENCIA</span></code> posterior, o que el comienzo de otro sujeto de asistencia se confunde con tokens similares como <code class="docutils literal notranslate"><span class="pre">I-FAMILIARES_SUJETO_ASISTENCIA</span></code> o <code class="docutils literal notranslate"><span class="pre">B-ID_SUJETO_ASISTENCIA</span></code> o con <code class="docutils literal notranslate"><span class="pre">O</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">df_tokens</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">],</span> <span class="n">df_tokens</span><span class="p">[</span><span class="s2">&quot;predicted_label&quot;</span><span class="p">],</span> 
                      <span class="n">normalize</span><span class="o">=</span><span class="s2">&quot;true&quot;</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">tags</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">,</span> <span class="n">values_format</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">xticks_rotation</span><span class="o">=</span><span class="s2">&quot;vertical&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Normalized confusion matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a_transformers_129_0.png" src="../_images/a_transformers_129_0.png" />
</div>
</div>
<p>A partir del gráfico, podemos ver que nuestro modelo tiende a confundir también las entidades <code class="docutils literal notranslate"><span class="pre">B-FAMILIARES_SUJETO_ASISTENCIA</span></code> y <code class="docutils literal notranslate"><span class="pre">I-FAMILIARES_SUJETO_ASISTENCIA</span></code> entre ellas y con <code class="docutils literal notranslate"><span class="pre">O</span></code>. Por lo demás, es bastante bueno en la clasificación de las entidades restantes, lo que queda claro por la naturaleza casi diagonal de la matriz de confusión.</p>
<p>Ahora que hemos examinado los errores a nivel de token, pasemos a ver las secuencias con grandes pérdidas. Para este cálculo, volveremos a visitar nuestro DataFrame “no explotado” y calcularemos la pérdida total sumando la pérdida por token. Para ello, escribamos primero una función que nos ayude a mostrar las secuencias de tokens con las etiquetas y las pérdidas:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_rows&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_samples</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])}:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
                <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;predicted_label&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
                <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;input_tokens&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
                <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">df_tmp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;tokens&quot;</span><span class="p">:</span> <span class="n">tokens</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">labels</span><span class="p">,</span>
                               <span class="s2">&quot;preds&quot;</span><span class="p">:</span> <span class="n">preds</span><span class="p">,</span> <span class="s2">&quot;losses&quot;</span><span class="p">:</span> <span class="n">losses</span><span class="p">})</span><span class="o">.</span><span class="n">T</span>
        <span class="k">yield</span> <span class="n">df_tmp</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&quot;total_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">sum</span><span class="p">)</span>
<span class="n">df_tmp</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;total_loss&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">get_samples</span><span class="p">(</span><span class="n">df_tmp</span><span class="p">):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tokens</th>
      <th>labels</th>
      <th>preds</th>
      <th>losses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Respons</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>able</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>▁clínico</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>▁:</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>5</th>
      <td>▁Dr</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>6</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>7</th>
      <td>.</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>8</th>
      <td>▁Lluís</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
      <td>B-NOMBRE_PERSONAL_SANITARIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>9</th>
      <td>▁Valeri</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>10</th>
      <td>o</td>
      <td>IGN</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>11</th>
      <td>▁Un</td>
      <td>I-NOMBRE_PERSONAL_SANITARIO</td>
      <td>O</td>
      <td>17.70</td>
    </tr>
    <tr>
      <th>12</th>
      <td>idad</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>13</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>14</th>
      <td>▁Salud</td>
      <td>B-INSTITUCION</td>
      <td>O</td>
      <td>13.89</td>
    </tr>
    <tr>
      <th>15</th>
      <td>▁Internacional</td>
      <td>I-INSTITUCION</td>
      <td>O</td>
      <td>10.41</td>
    </tr>
    <tr>
      <th>16</th>
      <td>▁B</td>
      <td>I-INSTITUCION</td>
      <td>B-INSTITUCION</td>
      <td>6.92</td>
    </tr>
    <tr>
      <th>17</th>
      <td>ni</td>
      <td>IGN</td>
      <td>I-INSTITUCION</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>18</th>
      <td>M</td>
      <td>IGN</td>
      <td>I-INSTITUCION</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>19</th>
      <td>▁ABS</td>
      <td>I-INSTITUCION</td>
      <td>B-CALLE</td>
      <td>14.19</td>
    </tr>
    <tr>
      <th>20</th>
      <td>▁Sta</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>15.35</td>
    </tr>
    <tr>
      <th>21</th>
      <td>▁Colo</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>14.96</td>
    </tr>
    <tr>
      <th>22</th>
      <td>ma</td>
      <td>IGN</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>23</th>
      <td>▁de</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>15.04</td>
    </tr>
    <tr>
      <th>24</th>
      <td>▁Gra</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>15.60</td>
    </tr>
    <tr>
      <th>25</th>
      <td>menet</td>
      <td>IGN</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>26</th>
      <td>▁6</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>17.21</td>
    </tr>
    <tr>
      <th>27</th>
      <td>▁-</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>17.46</td>
    </tr>
    <tr>
      <th>28</th>
      <td>▁Fondo</td>
      <td>I-INSTITUCION</td>
      <td>I-CALLE</td>
      <td>17.25</td>
    </tr>
    <tr>
      <th>29</th>
      <td>▁C</td>
      <td>B-CALLE</td>
      <td>B-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>30</th>
      <td>▁/</td>
      <td>I-CALLE</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>31</th>
      <td>▁Ja</td>
      <td>I-CALLE</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>32</th>
      <td>cin</td>
      <td>IGN</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>33</th>
      <td>t</td>
      <td>IGN</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>34</th>
      <td>▁Ver</td>
      <td>I-CALLE</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>35</th>
      <td>da</td>
      <td>IGN</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>36</th>
      <td>guer</td>
      <td>IGN</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>37</th>
      <td>▁118</td>
      <td>I-CALLE</td>
      <td>I-CALLE</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>38</th>
      <td>▁08</td>
      <td>B-TERRITORIO</td>
      <td>B-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>39</th>
      <td>92</td>
      <td>IGN</td>
      <td>B-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>40</th>
      <td>3</td>
      <td>IGN</td>
      <td>I-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>41</th>
      <td>▁Santa</td>
      <td>B-TERRITORIO</td>
      <td>B-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>42</th>
      <td>▁Colo</td>
      <td>I-TERRITORIO</td>
      <td>I-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>43</th>
      <td>ma</td>
      <td>IGN</td>
      <td>I-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>44</th>
      <td>▁de</td>
      <td>I-TERRITORIO</td>
      <td>I-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>45</th>
      <td>▁Gra</td>
      <td>I-TERRITORIO</td>
      <td>I-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>46</th>
      <td>menet</td>
      <td>IGN</td>
      <td>I-TERRITORIO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>47</th>
      <td>▁E</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>48</th>
      <td>▁-</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>49</th>
      <td>▁mail</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>50</th>
      <td>▁:</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>51</th>
      <td>▁l</td>
      <td>B-CORREO_ELECTRONICO</td>
      <td>B-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>52</th>
      <td>vale</td>
      <td>IGN</td>
      <td>B-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>53</th>
      <td>rio</td>
      <td>IGN</td>
      <td>B-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>54</th>
      <td>▁</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>55</th>
      <td>.</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>56</th>
      <td>▁b</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>57</th>
      <td>n</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>58</th>
      <td>m</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>59</th>
      <td>▁</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>60</th>
      <td>.</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>61</th>
      <td>▁</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>62</th>
      <td>ics</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>63</th>
      <td>@</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>64</th>
      <td>gen</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>65</th>
      <td>cat</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>66</th>
      <td>▁</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>67</th>
      <td>.</td>
      <td>IGN</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>68</th>
      <td>▁net</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>I-CORREO_ELECTRONICO</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>69</th>
      <td>&lt;/s&gt;</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tokens</th>
      <th>labels</th>
      <th>preds</th>
      <th>losses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>▁Se</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>▁trata</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>▁una</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>▁familia</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>5.40</td>
    </tr>
    <tr>
      <th>5</th>
      <td>▁de</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>17.31</td>
    </tr>
    <tr>
      <th>6</th>
      <td>▁nu</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>7.51</td>
    </tr>
    <tr>
      <th>7</th>
      <td>eve</td>
      <td>IGN</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>8</th>
      <td>▁miembros</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>7.80</td>
    </tr>
    <tr>
      <th>9</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>10</th>
      <td>,</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>11</th>
      <td>▁con</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>12</th>
      <td>▁padres</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>13</th>
      <td>▁de</td>
      <td>O</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>2.72</td>
    </tr>
    <tr>
      <th>14</th>
      <td>▁74</td>
      <td>B-EDAD_SUJETO_ASISTENCIA</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>13.05</td>
    </tr>
    <tr>
      <th>15</th>
      <td>▁y</td>
      <td>O</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>3.41</td>
    </tr>
    <tr>
      <th>16</th>
      <td>▁64</td>
      <td>B-EDAD_SUJETO_ASISTENCIA</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>13.32</td>
    </tr>
    <tr>
      <th>17</th>
      <td>▁años</td>
      <td>I-EDAD_SUJETO_ASISTENCIA</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>14.75</td>
    </tr>
    <tr>
      <th>18</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>19</th>
      <td>.</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>▁No</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>21</th>
      <td>▁hay</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>▁con</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>23</th>
      <td>san</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>24</th>
      <td>guin</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>25</th>
      <td>idad</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>26</th>
      <td>▁y</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>27</th>
      <td>▁proced</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>28</th>
      <td>en</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>29</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>30</th>
      <td>▁localidade</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>31</th>
      <td>s</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>32</th>
      <td>▁diferentes</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>33</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>34</th>
      <td>.</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>35</th>
      <td>▁La</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>36</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>37</th>
      <td>DG</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>38</th>
      <td>▁está</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>39</th>
      <td>▁presente</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>40</th>
      <td>▁en</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>41</th>
      <td>▁la</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>42</th>
      <td>▁madre</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>43</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>44</th>
      <td>,</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>45</th>
      <td>▁tres</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>46</th>
      <td>▁hermano</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>47</th>
      <td>s</td>
      <td>IGN</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>48</th>
      <td>▁y</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>49</th>
      <td>▁su</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>50</th>
      <td>▁hijo</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>51</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>52</th>
      <td>,</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>53</th>
      <td>▁mientras</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>54</th>
      <td>▁que</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>55</th>
      <td>▁el</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>56</th>
      <td>▁A</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>57</th>
      <td>OC</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>58</th>
      <td>▁aparece</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>59</th>
      <td>▁en</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>60</th>
      <td>▁dos</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>15.84</td>
    </tr>
    <tr>
      <th>61</th>
      <td>▁de</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>16.33</td>
    </tr>
    <tr>
      <th>62</th>
      <td>▁los</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>17.50</td>
    </tr>
    <tr>
      <th>63</th>
      <td>▁hermano</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>11.76</td>
    </tr>
    <tr>
      <th>64</th>
      <td>s</td>
      <td>IGN</td>
      <td>I-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>65</th>
      <td>▁pero</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>66</th>
      <td>▁sin</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>67</th>
      <td>▁datos</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>68</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>69</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>70</th>
      <td>DG</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>71</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>72</th>
      <td>.</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>73</th>
      <td>&lt;/s&gt;</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tokens</th>
      <th>labels</th>
      <th>preds</th>
      <th>losses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>▁La</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>▁explora</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ción</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>▁exhaust</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>iva</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>5</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>6</th>
      <td>▁la</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>7</th>
      <td>▁paciente</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>8</th>
      <td>▁durante</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>9</th>
      <td>▁el</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>10</th>
      <td>▁ingreso</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>11</th>
      <td>▁post</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>12</th>
      <td>operator</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>13</th>
      <td>io</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>14</th>
      <td>▁no</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>15</th>
      <td>▁re</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>16</th>
      <td>ve</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>17</th>
      <td>ló</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>18</th>
      <td>▁les</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>19</th>
      <td>iones</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>20</th>
      <td>▁cu</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>21</th>
      <td>tán</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>22</th>
      <td>e</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>23</th>
      <td>as</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>24</th>
      <td>▁compatible</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>25</th>
      <td>s</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>26</th>
      <td>▁con</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>27</th>
      <td>▁melan</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>28</th>
      <td>oma</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>29</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>30</th>
      <td>,</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>31</th>
      <td>▁pero</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>32</th>
      <td>▁sí</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>33</th>
      <td>▁la</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>34</th>
      <td>▁presencia</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>35</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>36</th>
      <td>▁un</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>37</th>
      <td>▁tatuaje</td>
      <td>B-OTROS_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>18.82</td>
    </tr>
    <tr>
      <th>38</th>
      <td>▁en</td>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>19.91</td>
    </tr>
    <tr>
      <th>39</th>
      <td>▁la</td>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>19.28</td>
    </tr>
    <tr>
      <th>40</th>
      <td>▁región</td>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>20.02</td>
    </tr>
    <tr>
      <th>41</th>
      <td>▁pe</td>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>19.99</td>
    </tr>
    <tr>
      <th>42</th>
      <td>ctor</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>43</th>
      <td>al</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>44</th>
      <td>▁izquierda</td>
      <td>I-OTROS_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>19.66</td>
    </tr>
    <tr>
      <th>45</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>46</th>
      <td>▁10</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>47</th>
      <td>▁años</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>48</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>49</th>
      <td>▁anti</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>50</th>
      <td>gü</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>51</th>
      <td>edad</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>52</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>53</th>
      <td>.</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>54</th>
      <td>▁La</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>55</th>
      <td>▁paciente</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>56</th>
      <td>▁fue</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>57</th>
      <td>▁dada</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>58</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>59</th>
      <td>▁alta</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>60</th>
      <td>▁hospital</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>61</th>
      <td>aria</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>62</th>
      <td>▁a</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>63</th>
      <td>▁la</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>64</th>
      <td>▁espera</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>65</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>66</th>
      <td>▁los</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>67</th>
      <td>▁resultados</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>68</th>
      <td>▁del</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>69</th>
      <td>▁estudio</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>70</th>
      <td>▁de</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>71</th>
      <td>▁Ana</td>
      <td>O</td>
      <td>O</td>
      <td>-0.00</td>
    </tr>
    <tr>
      <th>72</th>
      <td>tom</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>73</th>
      <td>ía</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>74</th>
      <td>▁Pat</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>75</th>
      <td>ológica</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>76</th>
      <td>▁</td>
      <td>O</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>77</th>
      <td>.</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>78</th>
      <td>&lt;/s&gt;</td>
      <td>IGN</td>
      <td>O</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Otra cosa que hemos observado antes es que los tokens “pareja”, “Merc” o “matern” tienen una pérdida relativamente alta. Veamos algunos ejemplos de secuencias con el token “pareja”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_rows&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">df_tmp</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;input_tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="se">\u2581</span><span class="s2">pareja&quot;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">get_samples</span><span class="p">(</span><span class="n">df_tmp</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">151</span><span class="p">:</span><span class="mi">165</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">display</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>151</th>
      <th>152</th>
      <th>153</th>
      <th>154</th>
      <th>155</th>
      <th>156</th>
      <th>157</th>
      <th>158</th>
      <th>159</th>
      <th>160</th>
      <th>161</th>
      <th>162</th>
      <th>163</th>
      <th>164</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>tokens</th>
      <td>▁de</td>
      <td>▁de</td>
      <td>terior</td>
      <td>o</td>
      <td>▁de</td>
      <td>▁la</td>
      <td>▁relación</td>
      <td>▁de</td>
      <td>▁pareja</td>
      <td>▁que</td>
      <td>▁manten</td>
      <td>ía</td>
      <td>▁</td>
      <td>,</td>
    </tr>
    <tr>
      <th>labels</th>
      <td>O</td>
      <td>O</td>
      <td>IGN</td>
      <td>IGN</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>O</td>
      <td>IGN</td>
      <td>O</td>
      <td>IGN</td>
    </tr>
    <tr>
      <th>preds</th>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
    </tr>
    <tr>
      <th>losses</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>16.30</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>40</th>
      <th>41</th>
      <th>42</th>
      <th>43</th>
      <th>44</th>
      <th>45</th>
      <th>46</th>
      <th>47</th>
      <th>48</th>
      <th>49</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>tokens</th>
      <td>.</td>
      <td>▁Rela</td>
      <td>ción</td>
      <td>▁de</td>
      <td>▁pareja</td>
      <td>▁estable</td>
      <td>▁desde</td>
      <td>▁hacía</td>
      <td>▁varios</td>
      <td>▁años</td>
    </tr>
    <tr>
      <th>labels</th>
      <td>IGN</td>
      <td>O</td>
      <td>IGN</td>
      <td>O</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
    </tr>
    <tr>
      <th>preds</th>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
    </tr>
    <tr>
      <th>losses</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>16.20</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>40</th>
      <th>41</th>
      <th>42</th>
      <th>43</th>
      <th>44</th>
      <th>45</th>
      <th>46</th>
      <th>47</th>
      <th>48</th>
      <th>49</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>tokens</th>
      <td>o</td>
      <td>▁por</td>
      <td>▁parte</td>
      <td>▁de</td>
      <td>▁su</td>
      <td>▁pareja</td>
      <td>▁</td>
      <td>,</td>
      <td>▁hab</td>
      <td>iendo</td>
    </tr>
    <tr>
      <th>labels</th>
      <td>IGN</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>IGN</td>
      <td>O</td>
      <td>IGN</td>
    </tr>
    <tr>
      <th>preds</th>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>B-FAMILIARES_SUJETO_ASISTENCIA</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
      <td>O</td>
    </tr>
    <tr>
      <th>losses</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>En la primera y segunda frase vemos claramente que el token ” pareja” no se detecta correctamente como <code class="docutils literal notranslate"><span class="pre">ID-OTRO_SUJETO_ASISTENCIA</span></code>.</p>
<p>En nuestro caso, el conjunto de datos ha sido etiquetado por expertos y verificado varias veces, lo que nos permite evitar en lo posible los errores de etiquetado que no somos capaces de detectar ahora.</p>
<p>Con un análisis relativamente sencillo, hemos identificado algunos puntos débiles en nuestro modelo, pero no tanto en el conjunto de datos que parece estar correctamente etiquetado. En un caso de uso real, repetiríamos este paso, limpiando el conjunto de datos, volviendo a entrenar el modelo y analizando los nuevos errores hasta que estuviéramos satisfechos con el rendimiento.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id2">4</a></span></dt>
<dd><p><a class="reference external" href="https://huggingface.co/docs/datasets/about_arrow">https://huggingface.co/docs/datasets/about_arrow</a></p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p><a class="reference external" href="https://pytorch.org/">https://pytorch.org/</a></p>
</dd>
</dl>
</section>
</section>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"1a3716eb50a94968adaa0437eb7e6944": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0bf51a932abf415f922495e223d4960b": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "a02591f92a5b43a09c574d773d0e4ad4": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_1a3716eb50a94968adaa0437eb7e6944", "max": 10312.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_0bf51a932abf415f922495e223d4960b", "value": 10312.0}}, "69c58a397ddf457f89c2a7f18f1bf6d3": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a51a05aa35eb4d5f8105916d1248fb32": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "a20dd8b0e9fb43328b6f668a20102fe6": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_69c58a397ddf457f89c2a7f18f1bf6d3", "placeholder": "\u200b", "style": "IPY_MODEL_a51a05aa35eb4d5f8105916d1248fb32", "value": "100%"}}, "9f9edf26c016499b9cb8c2be5f287119": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "45d96656f0364e8688f15864466e9fbf": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "fea69bbacd204ac39551dd48a9e0fb34": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9f9edf26c016499b9cb8c2be5f287119", "placeholder": "\u200b", "style": "IPY_MODEL_45d96656f0364e8688f15864466e9fbf", "value": " 10312/10312 [00:01&lt;00:00, 5305.10ex/s]"}}, "5ac784b3b0f34836af9c8179a18d05b6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ad78bd8f978c4d9fb969b346b0037800": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_a20dd8b0e9fb43328b6f668a20102fe6", "IPY_MODEL_a02591f92a5b43a09c574d773d0e4ad4", "IPY_MODEL_fea69bbacd204ac39551dd48a9e0fb34"], "layout": "IPY_MODEL_5ac784b3b0f34836af9c8179a18d05b6"}}, "36899aa8165d42bfafdcb976d66625ae": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4124323a5cb34d3bb7b866e277fa73cc": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "8866a6fa47914cbcb36002fa6452e3c9": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_36899aa8165d42bfafdcb976d66625ae", "max": 5268.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_4124323a5cb34d3bb7b866e277fa73cc", "value": 5268.0}}, "30d3ea78bd3c4e02beffe0c8cab43f3c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "484309bb0b20401a8bc0040e13643f4a": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "00bbf19a7f8d4ec484a54f1221d46c55": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_30d3ea78bd3c4e02beffe0c8cab43f3c", "placeholder": "\u200b", "style": "IPY_MODEL_484309bb0b20401a8bc0040e13643f4a", "value": "100%"}}, "58cfd3387b684e688d83009c0297def8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "60bc5834782346b39f9a16e86da5a128": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e997ffe0d97d47419f9486b9d9f981b0": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_58cfd3387b684e688d83009c0297def8", "placeholder": "\u200b", "style": "IPY_MODEL_60bc5834782346b39f9a16e86da5a128", "value": " 5268/5268 [00:01&lt;00:00, 5076.56ex/s]"}}, "c5d6f12cdbda47dba05b1d2384d506a1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fba56f2f8c574ff39514e9ac4e9c2dba": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_00bbf19a7f8d4ec484a54f1221d46c55", "IPY_MODEL_8866a6fa47914cbcb36002fa6452e3c9", "IPY_MODEL_e997ffe0d97d47419f9486b9d9f981b0"], "layout": "IPY_MODEL_c5d6f12cdbda47dba05b1d2384d506a1"}}, "cb8e23322a5749b78a43f4bf24888696": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ce649ab30b8640f6bdcecfcda88fa2cc": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "3bc73ceef0c94740a55de515b6a0571e": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_cb8e23322a5749b78a43f4bf24888696", "max": 5155.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_ce649ab30b8640f6bdcecfcda88fa2cc", "value": 5155.0}}, "bea6587037ff4ec7a3cc5274ec53cfe6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a7267ac3e15a4dcabad34aedab3f65cb": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "d3f2e2645a44481ebc91311348e4ed3d": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bea6587037ff4ec7a3cc5274ec53cfe6", "placeholder": "\u200b", "style": "IPY_MODEL_a7267ac3e15a4dcabad34aedab3f65cb", "value": "100%"}}, "d6d6da940f264742880f842823ec295c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "60be432cfa7e44b7ae55d362962eee04": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "177734423bb04e5ba13b5bb5812b93b9": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d6d6da940f264742880f842823ec295c", "placeholder": "\u200b", "style": "IPY_MODEL_60be432cfa7e44b7ae55d362962eee04", "value": " 5155/5155 [00:01&lt;00:00, 5195.92ex/s]"}}, "a2a0ec41c0a04efcb1a322fea8c1e1ef": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "50798f411d57499e972643e9b2c402c2": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_d3f2e2645a44481ebc91311348e4ed3d", "IPY_MODEL_3bc73ceef0c94740a55de515b6a0571e", "IPY_MODEL_177734423bb04e5ba13b5bb5812b93b9"], "layout": "IPY_MODEL_a2a0ec41c0a04efcb1a322fea8c1e1ef"}}, "27c04b9c78e044b9b219975a9e7313e6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2eda2c1d371b4d619fbb62137e5f981a": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "3d342202f06d45aa90477e8c07c17bac": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_27c04b9c78e044b9b219975a9e7313e6", "max": 11.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_2eda2c1d371b4d619fbb62137e5f981a", "value": 11.0}}, "78142476b51b48118ad401c90f33b093": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "78adfb5f1913421faadb5bdfb02da7da": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e3438d3611694182a5f65e90b1bbd29b": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_78142476b51b48118ad401c90f33b093", "placeholder": "\u200b", "style": "IPY_MODEL_78adfb5f1913421faadb5bdfb02da7da", "value": "100%"}}, "4a1b57d47bf74741b981b7a27c55b9f6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bb5fd43a55184cf8a7fb111f6cb5f305": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f0e503e44bf04ee788cb203e58a591ba": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4a1b57d47bf74741b981b7a27c55b9f6", "placeholder": "\u200b", "style": "IPY_MODEL_bb5fd43a55184cf8a7fb111f6cb5f305", "value": " 11/11 [00:01&lt;00:00,  6.40ba/s]"}}, "967f79020e4544b8b7f7a85aee66713a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fd513ed95e994728b6cba8b26cc4b3a3": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e3438d3611694182a5f65e90b1bbd29b", "IPY_MODEL_3d342202f06d45aa90477e8c07c17bac", "IPY_MODEL_f0e503e44bf04ee788cb203e58a591ba"], "layout": "IPY_MODEL_967f79020e4544b8b7f7a85aee66713a"}}, "3902b9f801d1487a9bcdf575423b00ba": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "18fa01d355124d71a896d2b330242de9": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "a17f824d62794459baf249174c69cd83": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3902b9f801d1487a9bcdf575423b00ba", "max": 6.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_18fa01d355124d71a896d2b330242de9", "value": 6.0}}, "668670e2d86445dd82e514600de5901d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c5e3e3f4f69e4e71b31fe63156a2d3bc": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "4fb3c7c9b28b4452bd3e8c5a6c69824c": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_668670e2d86445dd82e514600de5901d", "placeholder": "\u200b", "style": "IPY_MODEL_c5e3e3f4f69e4e71b31fe63156a2d3bc", "value": "100%"}}, "a8d3b3db0ebe46e8b0cec802a6d71f2d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "bdae69015fcc48ab8002dd1b0461d592": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7de4adf78bce430da3417437100cc203": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_a8d3b3db0ebe46e8b0cec802a6d71f2d", "placeholder": "\u200b", "style": "IPY_MODEL_bdae69015fcc48ab8002dd1b0461d592", "value": " 6/6 [00:00&lt;00:00,  6.76ba/s]"}}, "a49a458e87154c398ee92d2d29f37c77": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e4303ac6ad924180a4bcacb767ddf075": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_4fb3c7c9b28b4452bd3e8c5a6c69824c", "IPY_MODEL_a17f824d62794459baf249174c69cd83", "IPY_MODEL_7de4adf78bce430da3417437100cc203"], "layout": "IPY_MODEL_a49a458e87154c398ee92d2d29f37c77"}}, "b7839e2c0d0d4f03b0265d2c0679b80c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fb067890b3e047fe8f11567828592f45": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "f7d1ca8e1c27461e8bfd719548d3d77e": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b7839e2c0d0d4f03b0265d2c0679b80c", "max": 6.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_fb067890b3e047fe8f11567828592f45", "value": 6.0}}, "6d0c42dc7f46451198b0fe9a6c7aa312": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "0ea0930d8ffc49c0b1247643e4a0e447": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "94ecd54d8c794b1197d652c24e882acb": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_6d0c42dc7f46451198b0fe9a6c7aa312", "placeholder": "\u200b", "style": "IPY_MODEL_0ea0930d8ffc49c0b1247643e4a0e447", "value": "100%"}}, "9c6dce5294de46df87d5c1e9c9d974e4": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "00d0fd3687c74669b6fcbd44813d2004": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2ceb0fe5d35b475c9d04f11fddbc8d23": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_9c6dce5294de46df87d5c1e9c9d974e4", "placeholder": "\u200b", "style": "IPY_MODEL_00d0fd3687c74669b6fcbd44813d2004", "value": " 6/6 [00:00&lt;00:00,  6.50ba/s]"}}, "17b949876f16471a94c4c771f0e07f65": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "511d8bf63d704dfc897d560be03ca375": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_94ecd54d8c794b1197d652c24e882acb", "IPY_MODEL_f7d1ca8e1c27461e8bfd719548d3d77e", "IPY_MODEL_2ceb0fe5d35b475c9d04f11fddbc8d23"], "layout": "IPY_MODEL_17b949876f16471a94c4c771f0e07f65"}}, "5c9307e1d39d4d3bbde302c922782ff4": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "65caa5b66af54969be2d1a5d0da2d02f": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "a5cae4e7820b4767a10fdae4e91bb22d": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_5c9307e1d39d4d3bbde302c922782ff4", "max": 2634.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_65caa5b66af54969be2d1a5d0da2d02f", "value": 2634.0}}, "fcd2efb11b1c4772ac5875b91cb5e02f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5c32c87c8ef3462987a0e09ba7f3ef06": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7b2831ae749546ee9d274545101fbd12": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_fcd2efb11b1c4772ac5875b91cb5e02f", "placeholder": "\u200b", "style": "IPY_MODEL_5c32c87c8ef3462987a0e09ba7f3ef06", "value": "100%"}}, "bf695086f637485fbf65ebd05201394a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2ad2828ddb83431698799fb26923297e": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "22ee201f75714fb6831d09d99cff7c58": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bf695086f637485fbf65ebd05201394a", "placeholder": "\u200b", "style": "IPY_MODEL_2ad2828ddb83431698799fb26923297e", "value": " 2634/2634 [04:34&lt;00:00, 10.15ba/s]"}}, "fd16a2250dcb43adb41636cffc556239": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ff167df7964f49449de7936068eccac7": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_7b2831ae749546ee9d274545101fbd12", "IPY_MODEL_a5cae4e7820b4767a10fdae4e91bb22d", "IPY_MODEL_22ee201f75714fb6831d09d99cff7c58"], "layout": "IPY_MODEL_fd16a2250dcb43adb41636cffc556239"}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./meddocan"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="a_code.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5.3. </span>Código</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Guillaume Gelabert<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>