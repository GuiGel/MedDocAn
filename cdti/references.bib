---
---
@inproceedings{Marimon2019AutomaticDO,
  title={Automatic De-identification of Medical Texts in Spanish: the MEDDOCAN Track, Corpus, Guidelines, Methods and Evaluation of Results},
  author={Montserrat Marimon and Aitor Gonzalez-Agirre and Ander Intxaurrondo and Heidy Rodriguez and Jose Lopez Martin and Marta Villegas and Martin Krallinger},
  booktitle={IberLEF@SEPLN},
  year={2019}
}

@inproceedings{Lange2019NLNDETN,
  title={NLNDE: The Neither-Language-Nor-Domain-Experts' Way of Spanish Medical Document De-Identification},
  author={Lukas Lange and Heike Adel and Jannik Str{\"o}tgen},
  booktitle={IberLEF@SEPLN},
  year={2019}
}

@article{Stubbs2015-fg,
  title     = "Annotating longitudinal clinical narratives for
               de-identification: The 2014 {i2b2/UTHealth} corpus",
  author    = "Stubbs, Amber and Uzuner, {\"O}zlem",
  abstract  = "The 2014 i2b2/UTHealth natural language processing shared task
               featured a track focused on the de-identification of
               longitudinal medical records. For this track, we de-identified a
               set of 1304 longitudinal medical records describing 296
               patients. This corpus was de-identified under a broad
               interpretation of the HIPAA guidelines using double-annotation
               followed by arbitration, rounds of sanity checking, and proof
               reading. The average token-based F1 measure for the annotators
               compared to the gold standard was 0.927. The resulting
               annotations were used both to de-identify the data and to set
               the gold standard for the de-identification track of the 2014
               i2b2/UTHealth shared task. All annotated private health
               information were replaced with realistic surrogates
               automatically and then read over and corrected manually. The
               resulting corpus is the first of its kind made available for
               de-identification research. This corpus was first used for the
               2014 i2b2/UTHealth shared task, during which the systems
               achieved a mean F-measure of 0.872 and a maximum F-measure of
               0.964 using entity-based micro-averaged evaluations.",
  journal   = "J. Biomed. Inform.",
  publisher = "Elsevier BV",
  volume    = "58 Suppl",
  pages     = "S20--S29",
  month     =  dec,
  year      =  2015,
  keywords  = "Annotation; De-identification; HIPAA; Natural language
               processing",
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@article{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@article{Schweter2020FLERTDF,
  title={FLERT: Document-Level Features for Named Entity Recognition},
  author={Stefan Schweter and A. Akbik},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.06993}
}

@inproceedings{Akbik2019FLAIRAE,
  title={FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP},
  author={A. Akbik and Tanja Bergmann and Duncan A. J. Blythe and Kashif Rasul and Stefan Schweter and Roland Vollgraf},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{Lample2019CrosslingualLM,
  title={Cross-lingual Language Model Pretraining},
  author={Guillaume Lample and Alexis Conneau},
  booktitle={NeurIPS},
  year={2019}
}

@article{Huang2015BidirectionalLM,
  title={Bidirectional LSTM-CRF Models for Sequence Tagging},
  author={Zhiheng Huang and Wei Xu and Kai Yu},
  journal={ArXiv},
  year={2015},
  volume={abs/1508.01991}
}

@inproceedings{Akbik2018ContextualSE,
  title={Contextual String Embeddings for Sequence Labeling},
  author={A. Akbik and Duncan A. J. Blythe and Roland Vollgraf},
  booktitle={COLING},
  year={2018}
}

@inproceedings{Mikolov2013EfficientEO,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  booktitle={ICLR},
  year={2013}
}

@inproceedings{holdgraf_evidence_2014,
	address = {Brisbane, Australia, Australia},
	title = {Evidence for {Predictive} {Coding} in {Human} {Auditory} {Cortex}},
	booktitle = {International {Conference} on {Cognitive} {Neuroscience}},
	publisher = {Frontiers in Neuroscience},
	author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Knight, Robert T.},
	year = {2014}
}

@article{holdgraf_rapid_2016,
	title = {Rapid tuning shifts in human auditory cortex enhance speech intelligibility},
	volume = {7},
	issn = {2041-1723},
	url = {http://www.nature.com/doifinder/10.1038/ncomms13654},
	doi = {10.1038/ncomms13654},
	number = {May},
	journal = {Nature Communications},
	author = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Rieger, Jochem W. and Crone, Nathan and Lin, Jack J. and Knight, Robert T. and Theunissen, Frédéric E.},
	year = {2016},
	pages = {13654},
	file = {Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:C\:\\Users\\chold\\Zotero\\storage\\MDQP3JWE\\Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:application/pdf}
}

@inproceedings{holdgraf_portable_2017,
	title = {Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science},
	volume = {Part F1287},
	isbn = {978-1-4503-5272-7},
	doi = {10.1145/3093338.3093370},
	abstract = {© 2017 ACM. There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the effciency and ease with which students can learn. This manuscript details recent advances towards using commonly-Available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benets (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	author = {Holdgraf, Christopher Ramsay and Culich, A. and Rokem, A. and Deniz, F. and Alegro, M. and Ushizima, D.},
	year = {2017},
	keywords = {Teaching, Bootcamps, Cloud computing, Data science, Docker, Pedagogy}
}

@article{holdgraf_encoding_2017,
	title = {Encoding and decoding models in cognitive electrophysiology},
	volume = {11},
	issn = {16625137},
	doi = {10.3389/fnsys.2017.00061},
	abstract = {© 2017 Holdgraf, Rieger, Micheli, Martin, Knight and Theunissen. Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of “Encoding” models, in which stimulus features are used to model brain activity, and “Decoding” models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aimis to provide a practical understanding of predictivemodeling of human brain data and to propose best-practices in conducting these analyses.},
	journal = {Frontiers in Systems Neuroscience},
	author = {Holdgraf, Christopher Ramsay and Rieger, J.W. and Micheli, C. and Martin, S. and Knight, R.T. and Theunissen, F.E.},
	year = {2017},
	keywords = {Decoding models, Encoding models, Electrocorticography (ECoG), Electrophysiology/evoked potentials, Machine learning applied to neuroscience, Natural stimuli, Predictive modeling, Tutorials}
}

@book{ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}

@inproceedings{Wenzek2020CCNetEH,
  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},
  author={Guillaume Wenzek and Marie-Anne Lachaux and Alexis Conneau and Vishrav Chaudhary and Francisco Guzm'an and Armand Joulin and Edouard Grave},
  booktitle={LREC},
  year={2020}
}

@inproceedings{CaneteCFP2020,
  title={Spanish Pre-Trained BERT Model and Evaluation Data},
  author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
  booktitle={PML4DC at ICLR 2020},
  year={2020}
}

@article{Antoniak2018EvaluatingTS,
  title={Evaluating the Stability of Embedding-based Word Similarities},
  author={Maria Antoniak and David Mimno},
  journal={Transactions of the Association for Computational Linguistics},
  year={2018},
  volume={6},
  pages={107-119}
}

@article{Bojanowski2017EnrichingWV,
  title={Enriching Word Vectors with Subword Information},
  author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
  journal={Transactions of the Association for Computational Linguistics},
  year={2017},
  volume={5},
  pages={135-146}
}

@article{Smith2018ADA,
  title={A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay},
  author={Leslie N. Smith},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.09820}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@inproceedings{Loshchilov2019DecoupledWD,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle={NAACL},
  year={2018}
}

@article{Radford2017LearningTG,
  title={Learning to Generate Reviews and Discovering Sentiment},
  author={Alec Radford and Rafal J{\'o}zefowicz and Ilya Sutskever},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.01444}
}

@article{Zhu2015AligningBA,
  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  author={Yukun Zhu and Ryan Kiros and Richard S. Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
  journal={2015 IEEE International Conference on Computer Vision (ICCV)},
  year={2015},
  pages={19-27}
}