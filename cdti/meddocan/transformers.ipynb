{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cb4e22",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb08b18",
   "metadata": {},
   "source": [
    "## Create a custom dataset from a MEDDOCAN corpus instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3399321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import DefaultDict, Dict, List\n",
    "\n",
    "from datasets import ClassLabel, Dataset, DatasetDict, Features, Sequence, Value\n",
    "\n",
    "from meddocan.data import ArchiveFolder\n",
    "from meddocan.data.docs_iterators import GsDocs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Row:\n",
    "    input_ids: str\n",
    "    tokens: List[str]\n",
    "    ner_tags: List[int]\n",
    "\n",
    "\n",
    "archives = (ArchiveFolder.train, ArchiveFolder.dev, ArchiveFolder.test)\n",
    "dataset_dict: Dict[str, Dataset] = {}\n",
    "\n",
    "with TemporaryDirectory() as td:\n",
    "\n",
    "    tags = DefaultDict(int)\n",
    "\n",
    "    # 1. Write corpus in bio format as accepted by Flair\n",
    "    for archive in archives:\n",
    "        brat_docs = GsDocs(archive)\n",
    "        tempfile = Path(td, f\"{archive.value}.csv\")\n",
    "        brat_docs.to_connl03(file=tempfile, write_sentences=True, document_separator_token=None)\n",
    "\n",
    "        # 2. Collect labels for Dataset creation\n",
    "        with tempfile.open() as fp:\n",
    "            for line in fp:\n",
    "                if line.strip():\n",
    "                    _, tag = line.split(\" \")\n",
    "                    tags[tag.strip()] += 1\n",
    "\n",
    "    label_to_id = {k: i for i, k in enumerate(tags)}\n",
    "\n",
    "    features = Features(\n",
    "        {\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=list(label_to_id.keys()))),\n",
    "            \"input_ids\": Value(\"int32\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for archive in archives:\n",
    "        brat_docs = GsDocs(archive)\n",
    "        tempfile = Path(td, f\"{archive.value}.csv\")\n",
    "\n",
    "        # 3. Collect all rows\n",
    "        rows: List[Row] = []\n",
    "\n",
    "        idx = 0\n",
    "        tokens: List[str] = []\n",
    "        ner_tags: List[int] = []\n",
    "\n",
    "        with tempfile.open() as fp:\n",
    "            for line in fp:\n",
    "                if not line.strip():\n",
    "                    row = Row(str(id), tokens, ner_tags)\n",
    "                    rows.append(row)\n",
    "                    idx += 1\n",
    "                    tokens = []\n",
    "                    ner_tags = []\n",
    "                else:\n",
    "                    token, ner_tag = line.split(\" \")\n",
    "                    tokens.append(token)\n",
    "                    ner_tags.append(label_to_id[ner_tag.strip()])\n",
    "\n",
    "        # 4. Create a Dataset from Rows\n",
    "        ld = list(map(asdict, rows))\n",
    "        dl = {k: [dic[k] for dic in LD] for k in ld[0]}\n",
    "        dataset_dict[archive.value] = Dataset.from_dict(dl, features, split=archive.value)\n",
    "\n",
    "# 5. Contruct the dataset\n",
    "dataset = DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e926359",
   "metadata": {},
   "source": [
    "## Use the dataset to train a NER model with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d0fbf",
   "metadata": {},
   "source": [
    "Downsample the train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1f506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.1\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].shuffle(seed=0).select(range(int(dataset[split].num_rows * downsample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f39d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d430e1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number of example per split</th>\n",
       "      <td>515</td>\n",
       "      <td>515</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             train  dev  test\n",
       "number of example per split    515  515   515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({k: v.num_rows for k, v in ds.items()}, index=[\"number of example per split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbcc787",
   "metadata": {},
   "source": [
    "Look at an example extracted from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5faa4460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>2710</td>\n",
       "      <td>2710</td>\n",
       "      <td>2710</td>\n",
       "      <td>2710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>CP</td>\n",
       "      <td>:</td>\n",
       "      <td>28030</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner_tags</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1      2     3\n",
       "input_ids  2710  2710   2710  2710\n",
       "tokens       CP     :  28030     .\n",
       "ner_tags      0     0      8     0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ds[\"train\"][11]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66caf5fa",
   "metadata": {},
   "source": [
    "In our ``Dataset`` objects, the keys of our example correspond to the column names of an Arrow table, while the values denote the entries in each column. In particular, we see that the ``ner_tags`` column corresponds to the mapping of each entity to a class ID. This is a bit cryptic to the human eye, so let’s create a new column with the familiar tags. To do this, the first thing to notice is that our ``Dataset`` object has a features attribute that specifies the underlying data types associated with each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09a79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: Value(dtype='int32', id=None)\n",
      "tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "ner_tags: Sequence(feature=ClassLabel(num_classes=45, names=['O', 'B-NOMBRE_SUJETO_ASISTENCIA', 'I-NOMBRE_SUJETO_ASISTENCIA', 'B-ID_SUJETO_ASISTENCIA', 'B-ID_ASEGURAMIENTO', 'I-ID_ASEGURAMIENTO', 'B-CALLE', 'I-CALLE', 'B-TERRITORIO', 'B-FECHAS', 'I-FECHAS', 'B-PAIS', 'B-EDAD_SUJETO_ASISTENCIA', 'I-EDAD_SUJETO_ASISTENCIA', 'B-SEXO_SUJETO_ASISTENCIA', 'B-NOMBRE_PERSONAL_SANITARIO', 'I-NOMBRE_PERSONAL_SANITARIO', 'B-ID_TITULACION_PERSONAL_SANITARIO', 'I-ID_TITULACION_PERSONAL_SANITARIO', 'B-CORREO_ELECTRONICO', 'I-CORREO_ELECTRONICO', 'B-HOSPITAL', 'I-HOSPITAL', 'B-FAMILIARES_SUJETO_ASISTENCIA', 'I-FAMILIARES_SUJETO_ASISTENCIA', 'I-TERRITORIO', 'B-OTROS_SUJETO_ASISTENCIA', 'B-INSTITUCION', 'I-INSTITUCION', 'I-PAIS', 'B-NUMERO_TELEFONO', 'I-NUMERO_TELEFONO', 'B-ID_CONTACTO_ASISTENCIAL', 'B-NUMERO_FAX', 'I-NUMERO_FAX', 'B-CENTRO_SALUD', 'I-CENTRO_SALUD', 'I-ID_SUJETO_ASISTENCIA', 'I-OTROS_SUJETO_ASISTENCIA', 'B-PROFESION', 'I-PROFESION', 'I-SEXO_SUJETO_ASISTENCIA', 'B-ID_EMPLEO_PERSONAL_SANITARIO', 'I-ID_EMPLEO_PERSONAL_SANITARIO', 'I-ID_CONTACTO_ASISTENCIAL'], id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "for k, v in ds[\"train\"].features.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c8cdd",
   "metadata": {},
   "source": [
    "The ``Sequence`` class specifies that the field contains a list of features, which in the case of ``ner_tags`` corresponds to a list of ``ClassLabel`` features. Let’s pick out this feature from the training set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83033442",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=45, names=['O', 'B-NOMBRE_SUJETO_ASISTENCIA', 'I-NOMBRE_SUJETO_ASISTENCIA', 'B-ID_SUJETO_ASISTENCIA', 'B-ID_ASEGURAMIENTO', 'I-ID_ASEGURAMIENTO', 'B-CALLE', 'I-CALLE', 'B-TERRITORIO', 'B-FECHAS', 'I-FECHAS', 'B-PAIS', 'B-EDAD_SUJETO_ASISTENCIA', 'I-EDAD_SUJETO_ASISTENCIA', 'B-SEXO_SUJETO_ASISTENCIA', 'B-NOMBRE_PERSONAL_SANITARIO', 'I-NOMBRE_PERSONAL_SANITARIO', 'B-ID_TITULACION_PERSONAL_SANITARIO', 'I-ID_TITULACION_PERSONAL_SANITARIO', 'B-CORREO_ELECTRONICO', 'I-CORREO_ELECTRONICO', 'B-HOSPITAL', 'I-HOSPITAL', 'B-FAMILIARES_SUJETO_ASISTENCIA', 'I-FAMILIARES_SUJETO_ASISTENCIA', 'I-TERRITORIO', 'B-OTROS_SUJETO_ASISTENCIA', 'B-INSTITUCION', 'I-INSTITUCION', 'I-PAIS', 'B-NUMERO_TELEFONO', 'I-NUMERO_TELEFONO', 'B-ID_CONTACTO_ASISTENCIAL', 'B-NUMERO_FAX', 'I-NUMERO_FAX', 'B-CENTRO_SALUD', 'I-CENTRO_SALUD', 'I-ID_SUJETO_ASISTENCIA', 'I-OTROS_SUJETO_ASISTENCIA', 'B-PROFESION', 'I-PROFESION', 'I-SEXO_SUJETO_ASISTENCIA', 'B-ID_EMPLEO_PERSONAL_SANITARIO', 'I-ID_EMPLEO_PERSONAL_SANITARIO', 'I-ID_CONTACTO_ASISTENCIAL'], id=None)\n"
     ]
    }
   ],
   "source": [
    "tags = ds[\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b8e15",
   "metadata": {},
   "source": [
    "We can use the ``ClassLabel.int2str()`` method to create a new column in our training set with class names for each tag. We’ll use the ``map()`` method to return a ``dict`` with the key corresponding to the new column name and the value as a ``list`` of class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "567265fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490bbf9f72e34fb8b3c9f645a41aa230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757b67af9138470f8b01ced9e97de105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60de9b6bf5fb4600b106ab37caf50843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "\n",
    "for k in ds:\n",
    "    ds[k] = ds[k].map(create_tag_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578586b",
   "metadata": {},
   "source": [
    "Now that we have our tags in human-readable format, let’s see how the tokens and tags align for the first example in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "227b9e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>Nombre</td>\n",
       "      <td>:</td>\n",
       "      <td>Luis</td>\n",
       "      <td>Miguel</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-NOMBRE_SUJETO_ASISTENCIA</td>\n",
       "      <td>I-NOMBRE_SUJETO_ASISTENCIA</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1                           2                           3  4\n",
       "Tokens  Nombre  :                        Luis                      Miguel  .\n",
       "Tags         O  O  B-NOMBRE_SUJETO_ASISTENCIA  I-NOMBRE_SUJETO_ASISTENCIA  O"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_example = ds[\"train\"][12]\n",
    "pd.DataFrame([ds_example[\"tokens\"], ds_example[\"ner_tags_str\"]], [\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b481d",
   "metadata": {},
   "source": [
    "As a quick check that we don’t have any unusual imbalance in the tags, let’s calculate the frequencies of each entity across each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4db6c27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_ASEGURAMIENTO</th>\n",
       "      <th>NOMBRE_PERSONAL_SANITARIO</th>\n",
       "      <th>ID_TITULACION_PERSONAL_SANITARIO</th>\n",
       "      <th>EDAD_SUJETO_ASISTENCIA</th>\n",
       "      <th>SEXO_SUJETO_ASISTENCIA</th>\n",
       "      <th>NOMBRE_SUJETO_ASISTENCIA</th>\n",
       "      <th>INSTITUCION</th>\n",
       "      <th>TERRITORIO</th>\n",
       "      <th>CALLE</th>\n",
       "      <th>CORREO_ELECTRONICO</th>\n",
       "      <th>HOSPITAL</th>\n",
       "      <th>FECHAS</th>\n",
       "      <th>PAIS</th>\n",
       "      <th>ID_SUJETO_ASISTENCIA</th>\n",
       "      <th>ID_CONTACTO_ASISTENCIAL</th>\n",
       "      <th>NUMERO_TELEFONO</th>\n",
       "      <th>NUMERO_FAX</th>\n",
       "      <th>FAMILIARES_SUJETO_ASISTENCIA</th>\n",
       "      <th>PROFESION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>19</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>19</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>19</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID_ASEGURAMIENTO  NOMBRE_PERSONAL_SANITARIO  \\\n",
       "train                19                         52   \n",
       "dev                  19                         52   \n",
       "test                 19                         52   \n",
       "\n",
       "       ID_TITULACION_PERSONAL_SANITARIO  EDAD_SUJETO_ASISTENCIA  \\\n",
       "train                                26                      59   \n",
       "dev                                  26                      59   \n",
       "test                                 26                      59   \n",
       "\n",
       "       SEXO_SUJETO_ASISTENCIA  NOMBRE_SUJETO_ASISTENCIA  INSTITUCION  \\\n",
       "train                      57                        54            3   \n",
       "dev                        57                        54            3   \n",
       "test                       57                        54            3   \n",
       "\n",
       "       TERRITORIO  CALLE  CORREO_ELECTRONICO  HOSPITAL  FECHAS  PAIS  \\\n",
       "train          91     42                  24        12      55    35   \n",
       "dev            91     42                  24        12      55    35   \n",
       "test           91     42                  24        12      55    35   \n",
       "\n",
       "       ID_SUJETO_ASISTENCIA  ID_CONTACTO_ASISTENCIAL  NUMERO_TELEFONO  \\\n",
       "train                    30                        4                2   \n",
       "dev                      30                        4                2   \n",
       "test                     30                        4                2   \n",
       "\n",
       "       NUMERO_FAX  FAMILIARES_SUJETO_ASISTENCIA  PROFESION  \n",
       "train           2                             4          1  \n",
       "dev             2                             4          1  \n",
       "test            2                             4          1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = DefaultDict(Counter)\n",
    "for split, dataset in ds.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4f4f4",
   "metadata": {},
   "source": [
    "This looks good—the distributions of our tags frequencies are roughly the same for each split, so the validation and test sets should provide a good measure of our NER tagger’s ability to generalize. Next, let’s look at a few popular multilingual transformers and how they can be adapted to tackle our NER task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c58c0",
   "metadata": {},
   "source": [
    "## Multilingual Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4a83a",
   "metadata": {},
   "source": [
    "Multilingual transformers involve similar architectures and training procedures as their monolingual counterparts, except that the corpus used for pretraining consists of documents in many languages. A remarkable feature of this approach is that despite receiving no explicit information to differentiate among the languages, the resulting linguistic representations are able to generalize well across languages for a variety of downstream tasks. In some cases, this ability to perform cross-lingual transfer can produce results that are competitive with those of monolingual models, which circumvents the need to train one model per language!\n",
    "\n",
    "To measure the progress of cross-lingual transfer for NER, the ``CoNLL-2002`` and ``CoNLL-2003`` datasets are often used as a benchmark for English, Dutch, Spanish, and German. This benchmark consists of news articles annotated with categories ``LOC``, ``PER``, and ``ORG`` that are differents from the MEDDOCAN categories. Multilingual transformer models are usually evaluated in three different ways:\n",
    "\n",
    "``en``  \n",
    "    Fine-tune on the English training data and then evaluate on each language’s test set.\n",
    "\n",
    "``each``  \n",
    "    Fine-tune and evaluate on monolingual test data to measure per-language performance.\n",
    "\n",
    "``all``  \n",
    "    Fine-tune on all the training data to evaluate on all on each language’s test set.\n",
    "\n",
    "We will adopt a similar evaluation strategy for our NER task, but first we need to select a model to evaluate. One of the first multilingual transformers was mBERT, which uses the same architecture and pretraining objective as BERT but adds Wikipedia articles from many languages to the pre-training corpus. Since then, mBERT has been superseded by XLM-RoBERTa (or XLM-R for short), so that’s the model we’ll consider in this chapter.\n",
    "\n",
    "XLM-R uses only MLM (Masked Language Model) as a pre-training objective for 100 languages, but is distinguished by the huge size of its pre-training corpus compared to its predecessors: Wikipedia dumps for each language and 2.5 terabytes of Common Crawl data from the web. This corpus is several orders of magnitude larger than the ones used in earlier models and provides a significant boost in signal for low-resource languages like Burmese and Swahili, where only a small number of Wikipedia articles exist.\n",
    "\n",
    "The RoBERTa part of the model’s name refers to the fact that the pre-training approach is the same as for the monolingual RoBERTa models. RoBERTa’s developers improved on several aspects of BERT, in particular by removing the next sentence prediction task altogether.3 XLM-R also drops the language embeddings used in XLM and uses SentencePiece to tokenize the raw texts directly.4 Besides its multilingual nature, a notable difference between XLM-R and RoBERTa is the size of the respective vocabularies: 250,000 tokens versus 55,000!\n",
    "\n",
    "XLM-R is a great choice for multilingual NLU tasks. In the next section, we’ll explore how it can efficiently tokenize across many languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487e34b",
   "metadata": {},
   "source": [
    "## A Closer Look at Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de95139",
   "metadata": {},
   "source": [
    "Instead of using a WordPiece tokenizer, XLM-R uses a tokenizer called SentencePiece that is trained on the raw text of all one hundred languages. To get a feel for how SentencePiece compares to WordPiece, let’s load the BERT and XLM-R tokenizers in the usual way with hugginface Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f1dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-large\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7c6a9",
   "metadata": {},
   "source": [
    "By encoding a small sequence of text we can also retrieve the special tokens that each model used during pre-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9fcd981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>J</td>\n",
       "      <td>##ac</td>\n",
       "      <td>##k</td>\n",
       "      <td>Spar</td>\n",
       "      <td>##ro</td>\n",
       "      <td>##w</td>\n",
       "      <td>ama</td>\n",
       "      <td>Nueva</td>\n",
       "      <td>Yor</td>\n",
       "      <td>##k</td>\n",
       "      <td>!</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLM-R</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁ama</td>\n",
       "      <td>▁Nueva</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2    3     4       5      6    7      8     9     10  \\\n",
       "BERT   [CLS]      J   ##ac  ##k  Spar    ##ro    ##w  ama  Nueva   Yor   ##k   \n",
       "XLM-R    <s>  ▁Jack  ▁Spar  row  ▁ama  ▁Nueva  ▁York    !   </s>  None  None   \n",
       "\n",
       "         11     12  \n",
       "BERT      !  [SEP]  \n",
       "XLM-R  None   None  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Jack Sparrow ama Nueva York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "pd.DataFrame([bert_tokens, xlmr_tokens], [\"BERT\", \"XLM-R\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf015a1",
   "metadata": {},
   "source": [
    "Here we see that instead of the ``[CLS]`` and ``[SEP]`` tokens that BERT uses for sentence classification tasks, XLM-R uses ``<s>`` and ``<\\s>`` to denote the start and end of a sequence. These tokens are added in the final stage of tokenization, as we’ll see next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06235249",
   "metadata": {},
   "source": [
    "## The Tokenizer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13326382",
   "metadata": {},
   "source": [
    "So far we have treated tokenization as a single operation that transforms strings to integers we can pass through the model. This is not entirely accurate, and if we take a closer look we can see that it is actually a full processing pipeline that usually consists of four steps, as shown in Figure 4-1.\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc2ce6",
   "metadata": {},
   "source": [
    "## The Anatomy of the Transformers Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52345ca",
   "metadata": {},
   "source": [
    "Transformers is organized around dedicated classes for each architecture and task. The model classes associated with different tasks are named according to a ``<ModelName>For<Task>`` convention, or ``AutoModelFor<Task>`` when using the AutoModel classes.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22158ed1",
   "metadata": {},
   "source": [
    "### Creating a Custom Model for Token Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c4317",
   "metadata": {},
   "source": [
    "Let’s go through the exercise of building a custom token classification head for XLM-R. Since XLM-R uses the same model architecture as RoBERTa, we will use RoBERTa as the base model, but augmented with settings specific to XLM-R. Note that this is an educational exercise to show you how to build a custom model for your own task. For token classification, an ``XLMRobertaForTokenClassification`` class already exists that you can import from huggingface Transformers. If you want, you can skip to the next section and simply use that one.\n",
    "\n",
    "To get started, we need a data structure that will represent our XLM-R NER tagger. As a first guess, we’ll need a configuration object to initialize the model and a ``forward()`` function to generate the outputs. Let’s go ahead and build our XLM-R class for token classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4974da19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # Load model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3e181",
   "metadata": {},
   "source": [
    "The ``config_class`` ensures that the standard XLM-R settings are used when we initialize a new model. If you want to change the default parameters, you can do this by overwriting the default settings in the configuration. With the ``super()`` method we call the initialization function of the ``RobertaPreTrainedModel`` class. This abstract class handles the initialization or loading of pre-trained weights. Then we load our model body, which is RobertaModel, and extend it with our own classification head consisting of a dropout and a standard feed-forward layer. Note that we set ``add_​pool⁠ing_layer=False`` to ensure all hidden states are returned and not only the one associated with the [CLS] token. Finally, we initialize all the weights by calling the ``init_weights()`` method we inherit from ``RobertaPreTrainedModel``, which will load the pre-trained weights for the model body and randomly initialize the weights of our token classification head.\n",
    "\n",
    "The only thing left to do is to define what the model should do in a forward pass with a ``forward()`` method. During the forward pass, the data is first fed through the model body. There are a number of input variables, but the only ones we need for now are ``input_ids`` and ``attention_mask``. The hidden state, which is part of the model body output, is then fed through the dropout and classification layers. If we also provide labels in the forward pass, we can directly calculate the loss. If there is an attention mask we need to do a little bit more work to make sure we only calculate the loss of the unmasked tokens. Finally, we wrap all the outputs in a ``TokenClassifierOutput`` object that allows us to access elements in a the familiar named tuple from previous chapters.\n",
    "\n",
    "By just implementing two functions of a simple class, we can build our own custom transformer model. And since we inherit from a ``PreTrainedModel``, we instantly get access to all the useful huggingface Transformer utilities, such as ``from_pretrained()``! Let’s have a look how we can load pretrained weights into our custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba99cd",
   "metadata": {},
   "source": [
    "### Loading a Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fcec4",
   "metadata": {},
   "source": [
    "Now we are ready to load our token classification model. We’ll need to provide some additional information beyond the model name, including the tags that we will use to label each entity and the mapping of each tag to an ID and vice versa. All of this information can be derived from our tags variable, which as a ``ClassLabel`` object has a names attribute that we can use to derive the mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "431d4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f057843",
   "metadata": {},
   "source": [
    "We’ll store these mappings and the ``tags.num_classes`` attribute in the ``AutoConfig`` object. Passing keyword arguments to the ``from_pretrained()`` method overrides the default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0a2765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3422a20",
   "metadata": {},
   "source": [
    "The AutoConfig class contains the blueprint of a model’s architecture. When we load a model with ``AutoModel.from_pretrained(model_ckpt)``, the configuration file associated with that model is downloaded automatically. However, if we want to modify something like the number of classes or label names, then we can load the configuration first with the parameters we would like to customize.\n",
    "\n",
    "Now, we can load the model weights as usual with the ``from_pretrained()`` function with the additional config argument. Note that we did not implement loading pretrained weights in our custom model class; we get this for free by inheriting from ``RobertaPreTrainedModel``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c268d85c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dfbb3f0f8e432cb4e37042f2cfdd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'roberta.embeddings.position_ids', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64c01d",
   "metadata": {},
   "source": [
    "As a quick check that we have initialized the tokenizer and model correctly, let’s test the predictions on our small sequence of known entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b6fdf4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁ama</td>\n",
       "      <td>▁Nueva</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>2527</td>\n",
       "      <td>111191</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3     4       5      6   7     8\n",
       "Tokens     <s>  ▁Jack  ▁Spar    row  ▁ama  ▁Nueva  ▁York   !  </s>\n",
       "Input IDs    0  21763  37456  15555  2527  111191   5753  38     2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595be8c",
   "metadata": {},
   "source": [
    "As you can see here, the start ``<s>`` and end ``</s>`` tokens are given the IDs 0 and 2, respectively.\n",
    "\n",
    "Finally, we need to pass the inputs to the model and extract the predictions by taking the argmax to get the most likely class per token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52606c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 9\n",
      "Shape of outputs: torch.Size([1, 9, 45])\n"
     ]
    }
   ],
   "source": [
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of outputs: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901d6b",
   "metadata": {},
   "source": [
    "Here we see that the logits have the shape ``[batch_size, num_tokens, num_tags]``, with each token given a logit among the seven possible NER tags. By enumerating over the sequence, we can quickly see what the pre-trained model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63e5babd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁ama</td>\n",
       "      <td>▁Nueva</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>B-TERRITORIO</td>\n",
       "      <td>I-CALLE</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ID_ASEGURAMIENTO</td>\n",
       "      <td>O</td>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>B-CALLE</td>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>...</td>\n",
       "      <td>B-ID_SUJETO_ASISTENCIA</td>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ID_SUJETO_ASISTENCIA</td>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>I-CALLE</td>\n",
       "      <td>B-TERRITORIO</td>\n",
       "      <td>B-ID_SUJETO_ASISTENCIA</td>\n",
       "      <td>I-ID_ASEGURAMIENTO</td>\n",
       "      <td>B-ID_ASEGURAMIENTO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0                   1             2        3     4   \\\n",
       "Tokens                 <s>               ▁Jack         ▁Spar      row  ▁ama   \n",
       "Tags    I-ID_ASEGURAMIENTO  I-ID_ASEGURAMIENTO  B-TERRITORIO  I-CALLE     O   \n",
       "\n",
       "                        5      6                   7        8   \\\n",
       "Tokens              ▁Nueva  ▁York                   !     </s>   \n",
       "Tags    B-ID_ASEGURAMIENTO      O  I-ID_ASEGURAMIENTO  B-CALLE   \n",
       "\n",
       "                        9   ...                      35                  36  \\\n",
       "Tokens                None  ...                    None                None   \n",
       "Tags    I-ID_ASEGURAMIENTO  ...  B-ID_SUJETO_ASISTENCIA  I-ID_ASEGURAMIENTO   \n",
       "\n",
       "          37                      38                  39       40  \\\n",
       "Tokens  None                    None                None     None   \n",
       "Tags       O  B-ID_SUJETO_ASISTENCIA  I-ID_ASEGURAMIENTO  I-CALLE   \n",
       "\n",
       "                  41                      42                  43  \\\n",
       "Tokens          None                    None                None   \n",
       "Tags    B-TERRITORIO  B-ID_SUJETO_ASISTENCIA  I-ID_ASEGURAMIENTO   \n",
       "\n",
       "                        44  \n",
       "Tokens                None  \n",
       "Tags    B-ID_ASEGURAMIENTO  \n",
       "\n",
       "[2 rows x 45 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42946db",
   "metadata": {},
   "source": [
    "Unsurprisingly, our token classification layer with random weights leaves a lot to be desired; let’s fine-tune on some labeled data to make it better! Before doing so, let’s wrap the preceding steps into a helper function for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a4867de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tag_text(tet, tags, model, tokenizer):\n",
    "    # Get tokens with speecial characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf30299",
   "metadata": {},
   "source": [
    "Before we can train the model, we also need to tokenize the inputs and prepare the labels. We’ll do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674c71f",
   "metadata": {},
   "source": [
    "### Tokenizing Texts for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681ea00",
   "metadata": {},
   "source": [
    "Now that we’ve established that the tokenizer and model can encode a single example, our next step is to tokenize the whole dataset so that we can pass it to the XLM-R model for fine-tuning. HuggingFace Datasets provides a fast way to tokenize a ``Dataset`` object with the ``map()`` operation. To achieve this, recall that we first need to define a function with the minimal signature:\n",
    "\n",
    "```python\n",
    "function(examples: Dict[str, List]) -> Dict[str, List]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debb22b",
   "metadata": {},
   "source": [
    "where ``examples`` is equivalent to a slice of a ``Dataset``, e.g., ``ds['train'][:10]``. Since the XLM-R tokenizer returns the input IDs for the model’s inputs, we just need to augment this information with the attention mask and the label IDs that encode the information about which token is associated with each NER tag.\n",
    "\n",
    "Following the approach taken in the HuggingFace **Transformers documentation**, let’s look at how this works with our single Meddocan example by first collecting the words and tags as ordinary lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0d60e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, label = ds_example[\"tokens\"], ds_example[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897a3e2",
   "metadata": {},
   "source": [
    "Next, we tokenize each word and use the ``is_split_into_words`` argument to tell the tokenizer that our input sequence has already been split into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fb30610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁N</td>\n",
       "      <td>ombre</td>\n",
       "      <td>▁:</td>\n",
       "      <td>▁Luis</td>\n",
       "      <td>▁Miguel</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0   1      2   3      4        5  6  7     8\n",
       "Tokens  <s>  ▁N  ombre  ▁:  ▁Luis  ▁Miguel  ▁  .  </s>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = xlmr_tokenizer(words, is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "pd.DataFrame([tokens], index=[\"Tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8577d5",
   "metadata": {},
   "source": [
    "In this example we can see that the tokenizer has split “Fecha“ into two subwords, “▁Fe” and “cha”. Since we’re following the convention that only “▁Fe” should be associated with the ``B-FECHAS`` label, we need a way to mask the subword representations after the first subword. Fortunately, ``tokenized_input`` is a class that contains a ``word_ids()`` function that can help us achieve this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ee88b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁N</td>\n",
       "      <td>ombre</td>\n",
       "      <td>▁:</td>\n",
       "      <td>▁Luis</td>\n",
       "      <td>▁Miguel</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0   1      2   3      4        5  6  7     8\n",
       "Tokens     <s>  ▁N  ombre  ▁:  ▁Luis  ▁Miguel  ▁  .  </s>\n",
       "Word IDs  None   0      0   1      2        3  4  4  None"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_id = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, words_id], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0f34c",
   "metadata": {},
   "source": [
    "Here we can see that ``word_ids`` has mapped each subword to the corresponding index in the words sequence, so the first subword, “▁Fe”, is assigned the index 0, while “cha” is assigned the index 1 (since “cha” is the second word in words). We can also see that special tokens like ``<s>`` and ``<\\s>`` are mapped to ``None``. Let’s set –100 as the label for these special tokens and the subwords we wish to mask during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a81f482",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁N</td>\n",
       "      <td>ombre</td>\n",
       "      <td>▁:</td>\n",
       "      <td>▁Luis</td>\n",
       "      <td>▁Miguel</td>\n",
       "      <td>▁</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word IDs</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label IDs</th>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels</th>\n",
       "      <td>IGN</td>\n",
       "      <td>O</td>\n",
       "      <td>IGN</td>\n",
       "      <td>B-NOMBRE_SUJETO_ASISTENCIA</td>\n",
       "      <td>I-NOMBRE_SUJETO_ASISTENCIA</td>\n",
       "      <td>B-ID_SUJETO_ASISTENCIA</td>\n",
       "      <td>B-ID_ASEGURAMIENTO</td>\n",
       "      <td>IGN</td>\n",
       "      <td>IGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1      2                           3  \\\n",
       "Token       <s>  ▁N  ombre                          ▁:   \n",
       "Word IDs   None   0      0                           1   \n",
       "Label IDs  -100   0   -100                           1   \n",
       "Labels      IGN   O    IGN  B-NOMBRE_SUJETO_ASISTENCIA   \n",
       "\n",
       "                                    4                       5  \\\n",
       "Token                           ▁Luis                 ▁Miguel   \n",
       "Word IDs                            2                       3   \n",
       "Label IDs                           2                       3   \n",
       "Labels     I-NOMBRE_SUJETO_ASISTENCIA  B-ID_SUJETO_ASISTENCIA   \n",
       "\n",
       "                            6     7     8  \n",
       "Token                       ▁     .  </s>  \n",
       "Word IDs                    4     4  None  \n",
       "Label IDs                   4  -100  -100  \n",
       "Labels     B-ID_ASEGURAMIENTO   IGN   IGN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in words_id:\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(word_idx)\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Token\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "pd.DataFrame([tokens, words_id, label_ids, labels], index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e57c9",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "Why did we choose –100 as the ID to mask subword representations? The reason is that in PyTorch the cross-entropy loss class ``torch.nn.CrossEntropyLoss`` has an attribute called ``ignore_index`` whose value is –100. This index is ignored during training, so we can use it to ignore the tokens associated with consecutive subwords.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b10cfe",
   "metadata": {},
   "source": [
    "And that’s it! We can clearly see how the label IDs align with the tokens, so let’s scale this out to the whole dataset by defining a single function that wraps all the logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4878c2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8ef5a",
   "metadata": {},
   "source": [
    "We now have all the ingredients we need to encode each split, so let’s write a function we can iterate over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f5d8748",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_meddocan_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59dd1f",
   "metadata": {},
   "source": [
    "By applying this function to a ``DatasetDict`` object, we get an encoded ``Dataset`` object per split. Let’s use this to encode our Meddocan corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9a8e66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0544370172c446718644d94d2557b9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b9142f5714ae5ae5e3ef283142751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e21e102020b42fbb7f092f564bebd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meddocan_encoded = encode_meddocan_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79fc43",
   "metadata": {},
   "source": [
    "Now that we have a model and a dataset, we need to define a performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd09564",
   "metadata": {},
   "source": [
    "### Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a074b",
   "metadata": {},
   "source": [
    "Evaluating a NER model is similar to evaluating a text classification model, and it is common to report results for precision, recall, and F1-score. The only subtlety is that all words of an entity need to be predicted correctly in order for a prediction to be counted as correct. Fortunately, there is a nifty library called *seqeval* that is designed for these kinds of tasks. For example, given some placeholder NER tags and model predictions, we can compute the metrics via seqeval’s ``classification_report()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15d16ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678996f1",
   "metadata": {},
   "source": [
    "As we can see, *seqeval* expects the predictions and labels as lists of lists, with each list corresponding to a single example in our validation or test sets. To integrate these metrics during training, we need a function that can take the outputs of the model and convert them into the lists that seqeval expects. The following does the trick by ensuring we ignore the label IDs associated with subsequent subwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c2f95b6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6cd9e5",
   "metadata": {},
   "source": [
    "Equipped with a performance metric, we can move on to actually training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e78d17",
   "metadata": {},
   "source": [
    "### Fine-Tuning XLM-Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5281e4f",
   "metadata": {},
   "source": [
    "We now have all the ingredients to fine-tune our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "942b5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "logging_steps = len(ds[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-meddocan\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size, learning_rate=5e-6,\n",
    "    warmup_ratio=0.05,\n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
    "    logging_steps=logging_steps, push_to_hub=False, remove_unused_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0b8b6",
   "metadata": {},
   "source": [
    "Here we evaluate the model’s predictions on the validation set at the end of every epoch, tweak the weight decay, and set ``save_steps`` to a large number to disable checkpointing and thus speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a530a7d",
   "metadata": {},
   "source": [
    "We also need to tell the ``Trainer`` how to compute metrics on the validation set, so here we can use the ``align_predictions()`` function that we defined earlier to extract the predictions and labels in the format needed by *seqeval* to calculate the F1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1a6f98e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c6e27",
   "metadata": {},
   "source": [
    "The final step is to define a *data collator* so we can pad each input sequence to the largest sequence length in a batch. HuggingFace Transformers provides a dedicated data collator for token classification that will pad the labels along with the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3cb5bbd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ccb19",
   "metadata": {},
   "source": [
    "Padding the labels is necessary because, unlike in a text classification task, the labels are also sequences. One important detail here is that the label sequences are padded with the value –100, which, as we’ve seen, is ignored by PyTorch loss functions.\n",
    "\n",
    "We will train several models in the course of this chapter, so we’ll avoid initializing a new model for every Trainer by creating a ``model_init()`` method. This method loads an untrained model and is called at the beginning of the ``train()`` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "791481d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = (XLMRobertaForTokenClassification\n",
    "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "            .to(device))\n",
    "    model.model_parallel = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d846b5a",
   "metadata": {},
   "source": [
    "We can now pass all this information together with the encoded datasets to the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27b27925",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'amp' from 'apex' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Project/MedDocAn.worktree/master/.venv/lib/python3.8/site-packages/transformers/utils/import_utils.py:1031\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:783\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Project/MedDocAn.worktree/master/.venv/lib/python3.8/site-packages/transformers/trainer.py:171\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_apex_available():\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_greater_or_equal_than_1_6:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'amp' from 'apex' (unknown location)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model_init\u001b[38;5;241m=\u001b[39mmodel_init, args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      4\u001b[0m                   data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m      5\u001b[0m                   compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      6\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mmeddocan_encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m                   eval_dataset\u001b[38;5;241m=\u001b[39mmeddocan_encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m                   tokenizer\u001b[38;5;241m=\u001b[39mxlmr_tokenizer)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/Project/MedDocAn.worktree/master/.venv/lib/python3.8/site-packages/transformers/utils/import_utils.py:1021\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1021\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Project/MedDocAn.worktree/master/.venv/lib/python3.8/site-packages/transformers/utils/import_utils.py:1033\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1036\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'amp' from 'apex' (unknown location)"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=meddocan_encoded[\"train\"],\n",
    "                  eval_dataset=meddocan_encoded[\"dev\"],\n",
    "                  tokenizer=xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727731f7",
   "metadata": {},
   "source": [
    "and then run the training loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecdb7e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bd8f2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Save the final model in order to reuse it if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601824fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
