2022-07-27 08:24:36,717 ----------------------------------------------------------------------------------------------------
2022-07-27 08:24:36,719 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=89, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2022-07-27 08:24:36,748 ----------------------------------------------------------------------------------------------------
2022-07-27 08:24:36,749 Corpus: "Corpus: 10811 train + 5518 dev + 5405 test sentences"
2022-07-27 08:24:36,749 ----------------------------------------------------------------------------------------------------
2022-07-27 08:24:36,749 Parameters:
2022-07-27 08:24:36,749  - learning_rate: "0.000005"
2022-07-27 08:24:36,749  - mini_batch_size: "4"
2022-07-27 08:24:36,749  - patience: "3"
2022-07-27 08:24:36,749  - anneal_factor: "0.5"
2022-07-27 08:24:36,749  - max_epochs: "40"
2022-07-27 08:24:36,749  - shuffle: "True"
2022-07-27 08:24:36,749  - train_with_dev: "False"
2022-07-27 08:24:36,749  - batch_growth_annealing: "False"
2022-07-27 08:24:36,749 ----------------------------------------------------------------------------------------------------
2022-07-27 08:24:36,749 Model training base path: "experiments/corpus_sentence_grid_search_roberta_docstart/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased-context_FT_True_Ly_-1_seed_12_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.1/0"
2022-07-27 08:24:36,750 ----------------------------------------------------------------------------------------------------
2022-07-27 08:24:36,750 Device: cuda:1
2022-07-27 08:24:36,750 ----------------------------------------------------------------------------------------------------
2022-07-27 08:24:36,750 Embeddings storage mode: gpu
2022-07-27 08:24:36,750 ----------------------------------------------------------------------------------------------------
2022-07-27 08:28:05,024 epoch 1 - iter 270/2703 - loss 4.74467113 - samples/sec: 5.19 - lr: 0.000000
2022-07-27 08:31:47,776 epoch 1 - iter 540/2703 - loss 4.67931160 - samples/sec: 4.85 - lr: 0.000000
2022-07-27 08:35:35,482 epoch 1 - iter 810/2703 - loss 4.00624222 - samples/sec: 4.74 - lr: 0.000000
2022-07-27 08:39:25,019 epoch 1 - iter 1080/2703 - loss 3.23659569 - samples/sec: 4.71 - lr: 0.000000
2022-07-27 08:43:05,267 epoch 1 - iter 1350/2703 - loss 2.79420242 - samples/sec: 4.90 - lr: 0.000001
2022-07-27 08:46:51,105 epoch 1 - iter 1620/2703 - loss 2.44386844 - samples/sec: 4.78 - lr: 0.000001
2022-07-27 08:50:36,339 epoch 1 - iter 1890/2703 - loss 2.20908337 - samples/sec: 4.80 - lr: 0.000001
2022-07-27 08:54:28,190 epoch 1 - iter 2160/2703 - loss 1.99337508 - samples/sec: 4.66 - lr: 0.000001
2022-07-27 08:58:13,680 epoch 1 - iter 2430/2703 - loss 1.82377897 - samples/sec: 4.79 - lr: 0.000001
2022-07-27 09:01:52,469 epoch 1 - iter 2700/2703 - loss 1.70837791 - samples/sec: 4.94 - lr: 0.000001
2022-07-27 09:01:55,313 ----------------------------------------------------------------------------------------------------
2022-07-27 09:01:55,313 EPOCH 1 done: loss 1.7059 - lr 0.000001
2022-07-27 09:08:22,393 Evaluating as a multi-label problem: False
2022-07-27 09:08:22,454 DEV : loss 0.22015409171581268 - f1-score (micro avg)  0.6461
2022-07-27 09:08:22,784 BAD EPOCHS (no improvement): 4
2022-07-27 09:08:22,788 saving best model
2022-07-27 09:08:26,441 ----------------------------------------------------------------------------------------------------
2022-07-27 09:12:20,054 epoch 2 - iter 270/2703 - loss 0.49171292 - samples/sec: 4.62 - lr: 0.000001
2022-07-27 09:16:16,108 epoch 2 - iter 540/2703 - loss 0.47016451 - samples/sec: 4.58 - lr: 0.000001
2022-07-27 09:20:01,594 epoch 2 - iter 810/2703 - loss 0.46322521 - samples/sec: 4.79 - lr: 0.000002
2022-07-27 09:23:57,033 epoch 2 - iter 1080/2703 - loss 0.44167930 - samples/sec: 4.59 - lr: 0.000002
2022-07-27 09:27:52,660 epoch 2 - iter 1350/2703 - loss 0.42805981 - samples/sec: 4.58 - lr: 0.000002
2022-07-27 09:31:49,320 epoch 2 - iter 1620/2703 - loss 0.41504877 - samples/sec: 4.56 - lr: 0.000002
2022-07-27 09:35:43,680 epoch 2 - iter 1890/2703 - loss 0.40751806 - samples/sec: 4.61 - lr: 0.000002
2022-07-27 09:39:42,241 epoch 2 - iter 2160/2703 - loss 0.39921510 - samples/sec: 4.53 - lr: 0.000002
2022-07-27 09:43:46,541 epoch 2 - iter 2430/2703 - loss 0.38814617 - samples/sec: 4.42 - lr: 0.000002
2022-07-27 09:47:51,218 epoch 2 - iter 2700/2703 - loss 0.37876527 - samples/sec: 4.41 - lr: 0.000002
2022-07-27 09:47:53,430 ----------------------------------------------------------------------------------------------------
2022-07-27 09:47:53,431 EPOCH 2 done: loss 0.3789 - lr 0.000002
2022-07-27 09:54:17,272 Evaluating as a multi-label problem: False
2022-07-27 09:54:17,345 DEV : loss 0.07128073275089264 - f1-score (micro avg)  0.8245
2022-07-27 09:54:17,717 BAD EPOCHS (no improvement): 4
2022-07-27 09:54:17,721 saving best model
2022-07-27 09:54:43,327 ----------------------------------------------------------------------------------------------------
2022-07-27 09:58:35,850 epoch 3 - iter 270/2703 - loss 0.29078527 - samples/sec: 4.65 - lr: 0.000003
2022-07-27 10:02:35,177 epoch 3 - iter 540/2703 - loss 0.29258020 - samples/sec: 4.51 - lr: 0.000003
2022-07-27 10:06:29,514 epoch 3 - iter 810/2703 - loss 0.29794447 - samples/sec: 4.61 - lr: 0.000003
2022-07-27 10:10:27,226 epoch 3 - iter 1080/2703 - loss 0.29626644 - samples/sec: 4.54 - lr: 0.000003
2022-07-27 10:14:16,632 epoch 3 - iter 1350/2703 - loss 0.29396456 - samples/sec: 4.71 - lr: 0.000003
2022-07-27 10:18:08,589 epoch 3 - iter 1620/2703 - loss 0.29066747 - samples/sec: 4.66 - lr: 0.000003
2022-07-27 10:22:10,371 epoch 3 - iter 1890/2703 - loss 0.28994848 - samples/sec: 4.47 - lr: 0.000003
2022-07-27 10:26:09,412 epoch 3 - iter 2160/2703 - loss 0.29109112 - samples/sec: 4.52 - lr: 0.000003
2022-07-27 10:30:03,434 epoch 3 - iter 2430/2703 - loss 0.29061852 - samples/sec: 4.62 - lr: 0.000004
2022-07-27 10:34:02,710 epoch 3 - iter 2700/2703 - loss 0.28854835 - samples/sec: 4.51 - lr: 0.000004
2022-07-27 10:34:05,039 ----------------------------------------------------------------------------------------------------
2022-07-27 10:34:05,039 EPOCH 3 done: loss 0.2886 - lr 0.000004
2022-07-27 10:40:43,853 Evaluating as a multi-label problem: False
2022-07-27 10:40:43,912 DEV : loss 0.05424628034234047 - f1-score (micro avg)  0.8782
2022-07-27 10:40:44,240 BAD EPOCHS (no improvement): 4
2022-07-27 10:40:44,245 saving best model
2022-07-27 10:41:09,546 ----------------------------------------------------------------------------------------------------
2022-07-27 10:45:11,092 epoch 4 - iter 270/2703 - loss 0.27245033 - samples/sec: 4.47 - lr: 0.000004
2022-07-27 10:49:04,411 epoch 4 - iter 540/2703 - loss 0.27415779 - samples/sec: 4.63 - lr: 0.000004
2022-07-27 10:53:00,626 epoch 4 - iter 810/2703 - loss 0.27047924 - samples/sec: 4.57 - lr: 0.000004
2022-07-27 10:56:57,535 epoch 4 - iter 1080/2703 - loss 0.27098424 - samples/sec: 4.56 - lr: 0.000004
2022-07-27 11:00:50,584 epoch 4 - iter 1350/2703 - loss 0.26982227 - samples/sec: 4.63 - lr: 0.000004
2022-07-27 11:04:47,829 epoch 4 - iter 1620/2703 - loss 0.27017195 - samples/sec: 4.55 - lr: 0.000004
2022-07-27 11:08:42,659 epoch 4 - iter 1890/2703 - loss 0.26964144 - samples/sec: 4.60 - lr: 0.000005
2022-07-27 11:12:37,970 epoch 4 - iter 2160/2703 - loss 0.26913929 - samples/sec: 4.59 - lr: 0.000005
2022-07-27 11:16:27,211 epoch 4 - iter 2430/2703 - loss 0.26783102 - samples/sec: 4.71 - lr: 0.000005
2022-07-27 11:20:30,479 epoch 4 - iter 2700/2703 - loss 0.26724795 - samples/sec: 4.44 - lr: 0.000005
2022-07-27 11:20:32,317 ----------------------------------------------------------------------------------------------------
2022-07-27 11:20:32,317 EPOCH 4 done: loss 0.2672 - lr 0.000005
2022-07-27 11:27:08,646 Evaluating as a multi-label problem: False
2022-07-27 11:27:08,702 DEV : loss 0.04139366000890732 - f1-score (micro avg)  0.9242
2022-07-27 11:27:09,038 BAD EPOCHS (no improvement): 4
2022-07-27 11:27:09,041 saving best model
2022-07-27 11:27:34,002 ----------------------------------------------------------------------------------------------------
2022-07-27 11:31:26,154 epoch 5 - iter 270/2703 - loss 0.24373920 - samples/sec: 4.65 - lr: 0.000005
2022-07-27 11:35:23,057 epoch 5 - iter 540/2703 - loss 0.25217949 - samples/sec: 4.56 - lr: 0.000005
2022-07-27 11:39:21,171 epoch 5 - iter 810/2703 - loss 0.25087652 - samples/sec: 4.54 - lr: 0.000005
2022-07-27 11:43:18,922 epoch 5 - iter 1080/2703 - loss 0.25267593 - samples/sec: 4.54 - lr: 0.000005
2022-07-27 11:47:11,776 epoch 5 - iter 1350/2703 - loss 0.25391007 - samples/sec: 4.64 - lr: 0.000005
2022-07-27 11:51:04,744 epoch 5 - iter 1620/2703 - loss 0.25320058 - samples/sec: 4.64 - lr: 0.000005
2022-07-27 11:55:10,394 epoch 5 - iter 1890/2703 - loss 0.25414773 - samples/sec: 4.40 - lr: 0.000005
2022-07-27 11:59:00,237 epoch 5 - iter 2160/2703 - loss 0.25383120 - samples/sec: 4.70 - lr: 0.000005
2022-07-27 12:02:56,154 epoch 5 - iter 2430/2703 - loss 0.25288353 - samples/sec: 4.58 - lr: 0.000005
2022-07-27 12:07:00,427 epoch 5 - iter 2700/2703 - loss 0.25037788 - samples/sec: 4.42 - lr: 0.000005
2022-07-27 12:07:02,178 ----------------------------------------------------------------------------------------------------
2022-07-27 12:07:02,178 EPOCH 5 done: loss 0.2504 - lr 0.000005
2022-07-27 12:13:24,279 Evaluating as a multi-label problem: False
2022-07-27 12:13:24,336 DEV : loss 0.03654194995760918 - f1-score (micro avg)  0.9553
2022-07-27 12:13:24,664 BAD EPOCHS (no improvement): 4
2022-07-27 12:13:24,669 saving best model
2022-07-27 12:13:50,526 ----------------------------------------------------------------------------------------------------
2022-07-27 12:17:55,248 epoch 6 - iter 270/2703 - loss 0.24924304 - samples/sec: 4.41 - lr: 0.000005
2022-07-27 12:22:02,191 epoch 6 - iter 540/2703 - loss 0.24751379 - samples/sec: 4.37 - lr: 0.000005
2022-07-27 12:26:01,522 epoch 6 - iter 810/2703 - loss 0.24361964 - samples/sec: 4.51 - lr: 0.000005
2022-07-27 12:30:01,826 epoch 6 - iter 1080/2703 - loss 0.24446714 - samples/sec: 4.49 - lr: 0.000005
2022-07-27 12:33:48,239 epoch 6 - iter 1350/2703 - loss 0.24652161 - samples/sec: 4.77 - lr: 0.000005
2022-07-27 12:37:45,887 epoch 6 - iter 1620/2703 - loss 0.24488057 - samples/sec: 4.54 - lr: 0.000005
2022-07-27 12:41:45,324 epoch 6 - iter 1890/2703 - loss 0.24360529 - samples/sec: 4.51 - lr: 0.000005
2022-07-27 12:45:38,434 epoch 6 - iter 2160/2703 - loss 0.24420797 - samples/sec: 4.63 - lr: 0.000005
2022-07-27 12:49:29,742 epoch 6 - iter 2430/2703 - loss 0.24488013 - samples/sec: 4.67 - lr: 0.000005
2022-07-27 12:53:22,750 epoch 6 - iter 2700/2703 - loss 0.24496874 - samples/sec: 4.64 - lr: 0.000005
2022-07-27 12:53:26,212 ----------------------------------------------------------------------------------------------------
2022-07-27 12:53:26,212 EPOCH 6 done: loss 0.2450 - lr 0.000005
2022-07-27 13:00:06,495 Evaluating as a multi-label problem: False
2022-07-27 13:00:06,564 DEV : loss 0.032471854239702225 - f1-score (micro avg)  0.9629
2022-07-27 13:00:06,935 BAD EPOCHS (no improvement): 4
2022-07-27 13:00:06,941 saving best model
2022-07-27 13:00:32,361 ----------------------------------------------------------------------------------------------------
2022-07-27 13:04:19,194 epoch 7 - iter 270/2703 - loss 0.23739798 - samples/sec: 4.76 - lr: 0.000005
2022-07-27 13:08:18,545 epoch 7 - iter 540/2703 - loss 0.23985544 - samples/sec: 4.51 - lr: 0.000005
2022-07-27 13:12:17,386 epoch 7 - iter 810/2703 - loss 0.24117317 - samples/sec: 4.52 - lr: 0.000005
2022-07-27 13:16:18,225 epoch 7 - iter 1080/2703 - loss 0.24232050 - samples/sec: 4.48 - lr: 0.000005
2022-07-27 13:20:16,427 epoch 7 - iter 1350/2703 - loss 0.24553505 - samples/sec: 4.53 - lr: 0.000005
2022-07-27 13:24:04,836 epoch 7 - iter 1620/2703 - loss 0.24596548 - samples/sec: 4.73 - lr: 0.000005
2022-07-27 13:28:08,747 epoch 7 - iter 1890/2703 - loss 0.24731116 - samples/sec: 4.43 - lr: 0.000005
2022-07-27 13:32:02,592 epoch 7 - iter 2160/2703 - loss 0.24670926 - samples/sec: 4.62 - lr: 0.000005
2022-07-27 13:35:54,068 epoch 7 - iter 2430/2703 - loss 0.24659626 - samples/sec: 4.67 - lr: 0.000005
2022-07-27 13:39:47,026 epoch 7 - iter 2700/2703 - loss 0.24663214 - samples/sec: 4.64 - lr: 0.000005
2022-07-27 13:39:49,498 ----------------------------------------------------------------------------------------------------
2022-07-27 13:39:49,498 EPOCH 7 done: loss 0.2466 - lr 0.000005
2022-07-27 13:46:21,011 Evaluating as a multi-label problem: False
2022-07-27 13:46:21,075 DEV : loss 0.03435579314827919 - f1-score (micro avg)  0.9646
2022-07-27 13:46:21,455 BAD EPOCHS (no improvement): 4
2022-07-27 13:46:21,458 saving best model
2022-07-27 13:46:48,140 ----------------------------------------------------------------------------------------------------
2022-07-27 13:50:38,152 epoch 8 - iter 270/2703 - loss 0.24039420 - samples/sec: 4.70 - lr: 0.000005
2022-07-27 13:54:36,338 epoch 8 - iter 540/2703 - loss 0.24422544 - samples/sec: 4.53 - lr: 0.000005
2022-07-27 13:58:36,928 epoch 8 - iter 810/2703 - loss 0.24048059 - samples/sec: 4.49 - lr: 0.000005
2022-07-27 14:02:30,797 epoch 8 - iter 1080/2703 - loss 0.23888896 - samples/sec: 4.62 - lr: 0.000005
2022-07-27 14:06:32,820 epoch 8 - iter 1350/2703 - loss 0.23739999 - samples/sec: 4.46 - lr: 0.000005
2022-07-27 14:10:23,733 epoch 8 - iter 1620/2703 - loss 0.23936559 - samples/sec: 4.68 - lr: 0.000005
2022-07-27 14:14:23,101 epoch 8 - iter 1890/2703 - loss 0.23985894 - samples/sec: 4.51 - lr: 0.000004
2022-07-27 14:18:18,770 epoch 8 - iter 2160/2703 - loss 0.23999897 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 14:22:11,761 epoch 8 - iter 2430/2703 - loss 0.23871832 - samples/sec: 4.64 - lr: 0.000004
2022-07-27 14:26:03,659 epoch 8 - iter 2700/2703 - loss 0.23766004 - samples/sec: 4.66 - lr: 0.000004
2022-07-27 14:26:06,001 ----------------------------------------------------------------------------------------------------
2022-07-27 14:26:06,001 EPOCH 8 done: loss 0.2377 - lr 0.000004
2022-07-27 14:32:45,808 Evaluating as a multi-label problem: False
2022-07-27 14:32:45,864 DEV : loss 0.03057737462222576 - f1-score (micro avg)  0.9663
2022-07-27 14:32:46,195 BAD EPOCHS (no improvement): 4
2022-07-27 14:32:46,199 saving best model
2022-07-27 14:33:12,243 ----------------------------------------------------------------------------------------------------
2022-07-27 14:37:07,730 epoch 9 - iter 270/2703 - loss 0.23338039 - samples/sec: 4.59 - lr: 0.000004
2022-07-27 14:40:56,175 epoch 9 - iter 540/2703 - loss 0.23938102 - samples/sec: 4.73 - lr: 0.000004
2022-07-27 14:44:49,527 epoch 9 - iter 810/2703 - loss 0.23718625 - samples/sec: 4.63 - lr: 0.000004
2022-07-27 14:48:38,575 epoch 9 - iter 1080/2703 - loss 0.23797094 - samples/sec: 4.72 - lr: 0.000004
2022-07-27 14:52:29,846 epoch 9 - iter 1350/2703 - loss 0.23645083 - samples/sec: 4.67 - lr: 0.000004
2022-07-27 14:56:29,963 epoch 9 - iter 1620/2703 - loss 0.23811081 - samples/sec: 4.50 - lr: 0.000004
2022-07-27 15:00:36,608 epoch 9 - iter 1890/2703 - loss 0.23637784 - samples/sec: 4.38 - lr: 0.000004
2022-07-27 15:04:35,615 epoch 9 - iter 2160/2703 - loss 0.23590706 - samples/sec: 4.52 - lr: 0.000004
2022-07-27 15:08:29,422 epoch 9 - iter 2430/2703 - loss 0.23589283 - samples/sec: 4.62 - lr: 0.000004
2022-07-27 15:12:25,298 epoch 9 - iter 2700/2703 - loss 0.23635972 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 15:12:27,225 ----------------------------------------------------------------------------------------------------
2022-07-27 15:12:27,225 EPOCH 9 done: loss 0.2363 - lr 0.000004
2022-07-27 15:18:57,370 Evaluating as a multi-label problem: False
2022-07-27 15:18:57,423 DEV : loss 0.03514343127608299 - f1-score (micro avg)  0.9667
2022-07-27 15:18:57,752 BAD EPOCHS (no improvement): 4
2022-07-27 15:18:57,758 saving best model
2022-07-27 15:19:24,134 ----------------------------------------------------------------------------------------------------
2022-07-27 15:23:18,116 epoch 10 - iter 270/2703 - loss 0.23864164 - samples/sec: 4.62 - lr: 0.000004
2022-07-27 15:27:12,518 epoch 10 - iter 540/2703 - loss 0.23547876 - samples/sec: 4.61 - lr: 0.000004
2022-07-27 15:31:12,957 epoch 10 - iter 810/2703 - loss 0.23485819 - samples/sec: 4.49 - lr: 0.000004
2022-07-27 15:35:02,569 epoch 10 - iter 1080/2703 - loss 0.23410166 - samples/sec: 4.70 - lr: 0.000004
2022-07-27 15:38:57,127 epoch 10 - iter 1350/2703 - loss 0.23268285 - samples/sec: 4.60 - lr: 0.000004
2022-07-27 15:42:54,372 epoch 10 - iter 1620/2703 - loss 0.23285136 - samples/sec: 4.55 - lr: 0.000004
2022-07-27 15:47:01,292 epoch 10 - iter 1890/2703 - loss 0.23024578 - samples/sec: 4.37 - lr: 0.000004
2022-07-27 15:50:59,177 epoch 10 - iter 2160/2703 - loss 0.23175096 - samples/sec: 4.54 - lr: 0.000004
2022-07-27 15:54:56,768 epoch 10 - iter 2430/2703 - loss 0.23287121 - samples/sec: 4.55 - lr: 0.000004
2022-07-27 15:58:47,043 epoch 10 - iter 2700/2703 - loss 0.23335777 - samples/sec: 4.69 - lr: 0.000004
2022-07-27 15:58:49,132 ----------------------------------------------------------------------------------------------------
2022-07-27 15:58:49,133 EPOCH 10 done: loss 0.2334 - lr 0.000004
2022-07-27 16:05:19,256 Evaluating as a multi-label problem: False
2022-07-27 16:05:19,311 DEV : loss 0.03458813205361366 - f1-score (micro avg)  0.9695
2022-07-27 16:05:19,652 BAD EPOCHS (no improvement): 4
2022-07-27 16:05:19,654 saving best model
2022-07-27 16:05:45,615 ----------------------------------------------------------------------------------------------------
2022-07-27 16:09:39,845 epoch 11 - iter 270/2703 - loss 0.23677161 - samples/sec: 4.61 - lr: 0.000004
2022-07-27 16:13:34,125 epoch 11 - iter 540/2703 - loss 0.23885361 - samples/sec: 4.61 - lr: 0.000004
2022-07-27 16:17:30,243 epoch 11 - iter 810/2703 - loss 0.23600578 - samples/sec: 4.57 - lr: 0.000004
2022-07-27 16:21:18,422 epoch 11 - iter 1080/2703 - loss 0.23575792 - samples/sec: 4.73 - lr: 0.000004
2022-07-27 16:25:27,389 epoch 11 - iter 1350/2703 - loss 0.23581408 - samples/sec: 4.34 - lr: 0.000004
2022-07-27 16:29:13,081 epoch 11 - iter 1620/2703 - loss 0.23661834 - samples/sec: 4.79 - lr: 0.000004
2022-07-27 16:33:08,230 epoch 11 - iter 1890/2703 - loss 0.23561678 - samples/sec: 4.59 - lr: 0.000004
2022-07-27 16:37:05,938 epoch 11 - iter 2160/2703 - loss 0.23461995 - samples/sec: 4.54 - lr: 0.000004
2022-07-27 16:41:07,971 epoch 11 - iter 2430/2703 - loss 0.23327682 - samples/sec: 4.46 - lr: 0.000004
2022-07-27 16:45:12,182 epoch 11 - iter 2700/2703 - loss 0.23373991 - samples/sec: 4.42 - lr: 0.000004
2022-07-27 16:45:14,614 ----------------------------------------------------------------------------------------------------
2022-07-27 16:45:14,614 EPOCH 11 done: loss 0.2337 - lr 0.000004
2022-07-27 16:51:36,827 Evaluating as a multi-label problem: False
2022-07-27 16:51:36,889 DEV : loss 0.03183671832084656 - f1-score (micro avg)  0.9725
2022-07-27 16:51:37,256 BAD EPOCHS (no improvement): 4
2022-07-27 16:51:37,261 saving best model
2022-07-27 16:52:03,442 ----------------------------------------------------------------------------------------------------
2022-07-27 16:55:56,404 epoch 12 - iter 270/2703 - loss 0.23841062 - samples/sec: 4.64 - lr: 0.000004
2022-07-27 16:59:52,418 epoch 12 - iter 540/2703 - loss 0.23133607 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 17:03:49,090 epoch 12 - iter 810/2703 - loss 0.22959985 - samples/sec: 4.56 - lr: 0.000004
2022-07-27 17:07:50,063 epoch 12 - iter 1080/2703 - loss 0.22782622 - samples/sec: 4.48 - lr: 0.000004
2022-07-27 17:11:41,296 epoch 12 - iter 1350/2703 - loss 0.22663837 - samples/sec: 4.67 - lr: 0.000004
2022-07-27 17:15:37,342 epoch 12 - iter 1620/2703 - loss 0.22621365 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 17:19:38,794 epoch 12 - iter 1890/2703 - loss 0.22512817 - samples/sec: 4.47 - lr: 0.000004
2022-07-27 17:23:42,842 epoch 12 - iter 2160/2703 - loss 0.22537515 - samples/sec: 4.43 - lr: 0.000004
2022-07-27 17:27:45,088 epoch 12 - iter 2430/2703 - loss 0.22634675 - samples/sec: 4.46 - lr: 0.000004
2022-07-27 17:31:38,495 epoch 12 - iter 2700/2703 - loss 0.22450905 - samples/sec: 4.63 - lr: 0.000004
2022-07-27 17:31:41,263 ----------------------------------------------------------------------------------------------------
2022-07-27 17:31:41,263 EPOCH 12 done: loss 0.2245 - lr 0.000004
2022-07-27 17:38:13,203 Evaluating as a multi-label problem: False
2022-07-27 17:38:13,257 DEV : loss 0.034839317202568054 - f1-score (micro avg)  0.969
2022-07-27 17:38:13,592 BAD EPOCHS (no improvement): 4
2022-07-27 17:38:13,597 ----------------------------------------------------------------------------------------------------
2022-07-27 17:41:59,262 epoch 13 - iter 270/2703 - loss 0.22966548 - samples/sec: 4.79 - lr: 0.000004
2022-07-27 17:45:53,690 epoch 13 - iter 540/2703 - loss 0.23391397 - samples/sec: 4.61 - lr: 0.000004
2022-07-27 17:49:58,567 epoch 13 - iter 810/2703 - loss 0.22927206 - samples/sec: 4.41 - lr: 0.000004
2022-07-27 17:53:59,254 epoch 13 - iter 1080/2703 - loss 0.22855856 - samples/sec: 4.49 - lr: 0.000004
2022-07-27 17:57:55,719 epoch 13 - iter 1350/2703 - loss 0.22853030 - samples/sec: 4.57 - lr: 0.000004
2022-07-27 18:01:52,237 epoch 13 - iter 1620/2703 - loss 0.22995861 - samples/sec: 4.57 - lr: 0.000004
2022-07-27 18:05:51,106 epoch 13 - iter 1890/2703 - loss 0.23019757 - samples/sec: 4.52 - lr: 0.000004
2022-07-27 18:09:46,903 epoch 13 - iter 2160/2703 - loss 0.23089212 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 18:13:44,557 epoch 13 - iter 2430/2703 - loss 0.22934119 - samples/sec: 4.54 - lr: 0.000004
2022-07-27 18:17:39,610 epoch 13 - iter 2700/2703 - loss 0.22756199 - samples/sec: 4.60 - lr: 0.000004
2022-07-27 18:17:41,595 ----------------------------------------------------------------------------------------------------
2022-07-27 18:17:41,595 EPOCH 13 done: loss 0.2276 - lr 0.000004
2022-07-27 18:24:07,298 Evaluating as a multi-label problem: False
2022-07-27 18:24:07,350 DEV : loss 0.03511950746178627 - f1-score (micro avg)  0.9722
2022-07-27 18:24:07,684 BAD EPOCHS (no improvement): 4
2022-07-27 18:24:07,688 ----------------------------------------------------------------------------------------------------
2022-07-27 18:28:10,901 epoch 14 - iter 270/2703 - loss 0.23353214 - samples/sec: 4.44 - lr: 0.000004
2022-07-27 18:32:08,596 epoch 14 - iter 540/2703 - loss 0.23433888 - samples/sec: 4.54 - lr: 0.000004
2022-07-27 18:35:53,037 epoch 14 - iter 810/2703 - loss 0.23517088 - samples/sec: 4.81 - lr: 0.000004
2022-07-27 18:39:43,004 epoch 14 - iter 1080/2703 - loss 0.23431924 - samples/sec: 4.70 - lr: 0.000004
2022-07-27 18:43:32,352 epoch 14 - iter 1350/2703 - loss 0.23266932 - samples/sec: 4.71 - lr: 0.000004
2022-07-27 18:47:33,473 epoch 14 - iter 1620/2703 - loss 0.23227679 - samples/sec: 4.48 - lr: 0.000004
2022-07-27 18:51:29,172 epoch 14 - iter 1890/2703 - loss 0.23243043 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 18:55:19,405 epoch 14 - iter 2160/2703 - loss 0.23198087 - samples/sec: 4.69 - lr: 0.000004
2022-07-27 18:59:19,272 epoch 14 - iter 2430/2703 - loss 0.23079091 - samples/sec: 4.50 - lr: 0.000004
2022-07-27 19:03:24,516 epoch 14 - iter 2700/2703 - loss 0.23044684 - samples/sec: 4.40 - lr: 0.000004
2022-07-27 19:03:27,463 ----------------------------------------------------------------------------------------------------
2022-07-27 19:03:27,463 EPOCH 14 done: loss 0.2305 - lr 0.000004
2022-07-27 19:09:49,773 Evaluating as a multi-label problem: False
2022-07-27 19:09:49,827 DEV : loss 0.03520858660340309 - f1-score (micro avg)  0.9714
2022-07-27 19:09:50,158 BAD EPOCHS (no improvement): 4
2022-07-27 19:09:50,163 ----------------------------------------------------------------------------------------------------
2022-07-27 19:13:54,892 epoch 15 - iter 270/2703 - loss 0.23548176 - samples/sec: 4.41 - lr: 0.000004
2022-07-27 19:17:44,978 epoch 15 - iter 540/2703 - loss 0.23468507 - samples/sec: 4.69 - lr: 0.000004
2022-07-27 19:21:39,903 epoch 15 - iter 810/2703 - loss 0.23528937 - samples/sec: 4.60 - lr: 0.000004
2022-07-27 19:25:40,717 epoch 15 - iter 1080/2703 - loss 0.23172879 - samples/sec: 4.49 - lr: 0.000004
2022-07-27 19:29:36,330 epoch 15 - iter 1350/2703 - loss 0.22819159 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 19:33:32,224 epoch 15 - iter 1620/2703 - loss 0.22711122 - samples/sec: 4.58 - lr: 0.000004
2022-07-27 19:37:28,763 epoch 15 - iter 1890/2703 - loss 0.22509803 - samples/sec: 4.57 - lr: 0.000004
2022-07-27 19:41:26,688 epoch 15 - iter 2160/2703 - loss 0.22387042 - samples/sec: 4.54 - lr: 0.000004
2022-07-27 19:45:23,624 epoch 15 - iter 2430/2703 - loss 0.22298184 - samples/sec: 4.56 - lr: 0.000003
2022-07-27 19:49:17,853 epoch 15 - iter 2700/2703 - loss 0.22308843 - samples/sec: 4.61 - lr: 0.000003
2022-07-27 19:49:20,152 ----------------------------------------------------------------------------------------------------
2022-07-27 19:49:20,152 EPOCH 15 done: loss 0.2231 - lr 0.000003
2022-07-27 19:55:57,296 Evaluating as a multi-label problem: False
2022-07-27 19:55:57,350 DEV : loss 0.038860250264406204 - f1-score (micro avg)  0.9742
2022-07-27 19:55:57,680 BAD EPOCHS (no improvement): 4
2022-07-27 19:55:57,685 saving best model
2022-07-27 19:56:23,485 ----------------------------------------------------------------------------------------------------
2022-07-27 20:00:16,454 epoch 16 - iter 270/2703 - loss 0.21672059 - samples/sec: 4.64 - lr: 0.000003
2022-07-27 20:04:15,833 epoch 16 - iter 540/2703 - loss 0.22021710 - samples/sec: 4.51 - lr: 0.000003
2022-07-27 20:08:17,085 epoch 16 - iter 810/2703 - loss 0.21802632 - samples/sec: 4.48 - lr: 0.000003
2022-07-27 20:12:21,409 epoch 16 - iter 1080/2703 - loss 0.21685752 - samples/sec: 4.42 - lr: 0.000003
2022-07-27 20:16:20,528 epoch 16 - iter 1350/2703 - loss 0.21905865 - samples/sec: 4.52 - lr: 0.000003
2022-07-27 20:20:10,412 epoch 16 - iter 1620/2703 - loss 0.21885226 - samples/sec: 4.70 - lr: 0.000003
2022-07-27 20:24:03,842 epoch 16 - iter 1890/2703 - loss 0.22073923 - samples/sec: 4.63 - lr: 0.000003
2022-07-27 20:27:57,467 epoch 16 - iter 2160/2703 - loss 0.22171410 - samples/sec: 4.62 - lr: 0.000003
2022-07-27 20:31:58,620 epoch 16 - iter 2430/2703 - loss 0.22278065 - samples/sec: 4.48 - lr: 0.000003
2022-07-27 20:35:50,454 epoch 16 - iter 2700/2703 - loss 0.22145564 - samples/sec: 4.66 - lr: 0.000003
2022-07-27 20:35:52,714 ----------------------------------------------------------------------------------------------------
2022-07-27 20:35:52,714 EPOCH 16 done: loss 0.2214 - lr 0.000003
2022-07-27 20:42:17,577 Evaluating as a multi-label problem: False
2022-07-27 20:42:17,630 DEV : loss 0.034232061356306076 - f1-score (micro avg)  0.9741
2022-07-27 20:42:17,965 BAD EPOCHS (no improvement): 4
2022-07-27 20:42:17,970 ----------------------------------------------------------------------------------------------------
2022-07-27 20:46:16,675 epoch 17 - iter 270/2703 - loss 0.21729383 - samples/sec: 4.52 - lr: 0.000003
2022-07-27 20:50:20,945 epoch 17 - iter 540/2703 - loss 0.22376232 - samples/sec: 4.42 - lr: 0.000003
2022-07-27 20:54:19,850 epoch 17 - iter 810/2703 - loss 0.22393595 - samples/sec: 4.52 - lr: 0.000003
2022-07-27 20:58:14,488 epoch 17 - iter 1080/2703 - loss 0.22240574 - samples/sec: 4.60 - lr: 0.000003
2022-07-27 21:02:22,381 epoch 17 - iter 1350/2703 - loss 0.22205164 - samples/sec: 4.36 - lr: 0.000003
2022-07-27 21:06:12,659 epoch 17 - iter 1620/2703 - loss 0.22260401 - samples/sec: 4.69 - lr: 0.000003
2022-07-27 21:10:10,931 epoch 17 - iter 1890/2703 - loss 0.22291878 - samples/sec: 4.53 - lr: 0.000003
2022-07-27 21:13:54,482 epoch 17 - iter 2160/2703 - loss 0.22237278 - samples/sec: 4.83 - lr: 0.000003
2022-07-27 21:17:51,016 epoch 17 - iter 2430/2703 - loss 0.22078420 - samples/sec: 4.57 - lr: 0.000003
2022-07-27 21:21:49,931 epoch 17 - iter 2700/2703 - loss 0.22132221 - samples/sec: 4.52 - lr: 0.000003
2022-07-27 21:21:51,801 ----------------------------------------------------------------------------------------------------
2022-07-27 21:21:51,801 EPOCH 17 done: loss 0.2213 - lr 0.000003
2022-07-27 21:28:20,164 Evaluating as a multi-label problem: False
2022-07-27 21:28:20,216 DEV : loss 0.037889912724494934 - f1-score (micro avg)  0.9726
2022-07-27 21:28:20,555 BAD EPOCHS (no improvement): 4
2022-07-27 21:28:20,556 ----------------------------------------------------------------------------------------------------
2022-07-27 21:32:21,018 epoch 18 - iter 270/2703 - loss 0.21064136 - samples/sec: 4.49 - lr: 0.000003
2022-07-27 21:36:09,293 epoch 18 - iter 540/2703 - loss 0.21831236 - samples/sec: 4.73 - lr: 0.000003
2022-07-27 21:40:07,373 epoch 18 - iter 810/2703 - loss 0.21518317 - samples/sec: 4.54 - lr: 0.000003
2022-07-27 21:43:59,373 epoch 18 - iter 1080/2703 - loss 0.21691601 - samples/sec: 4.66 - lr: 0.000003
2022-07-27 21:48:03,697 epoch 18 - iter 1350/2703 - loss 0.21685813 - samples/sec: 4.42 - lr: 0.000003
2022-07-27 21:52:01,852 epoch 18 - iter 1620/2703 - loss 0.21657202 - samples/sec: 4.54 - lr: 0.000003
2022-07-27 21:55:59,442 epoch 18 - iter 1890/2703 - loss 0.21568823 - samples/sec: 4.55 - lr: 0.000003
2022-07-27 21:59:55,751 epoch 18 - iter 2160/2703 - loss 0.21612488 - samples/sec: 4.57 - lr: 0.000003
2022-07-27 22:03:49,376 epoch 18 - iter 2430/2703 - loss 0.21741134 - samples/sec: 4.62 - lr: 0.000003
2022-07-27 22:07:37,487 epoch 18 - iter 2700/2703 - loss 0.21720752 - samples/sec: 4.73 - lr: 0.000003
2022-07-27 22:07:39,381 ----------------------------------------------------------------------------------------------------
2022-07-27 22:07:39,381 EPOCH 18 done: loss 0.2172 - lr 0.000003
2022-07-27 22:14:11,299 Evaluating as a multi-label problem: False
2022-07-27 22:14:11,360 DEV : loss 0.03648729994893074 - f1-score (micro avg)  0.9735
2022-07-27 22:14:11,717 BAD EPOCHS (no improvement): 4
2022-07-27 22:14:11,724 ----------------------------------------------------------------------------------------------------
2022-07-27 22:18:17,495 epoch 19 - iter 270/2703 - loss 0.21970345 - samples/sec: 4.39 - lr: 0.000003
2022-07-27 22:22:09,554 epoch 19 - iter 540/2703 - loss 0.22016695 - samples/sec: 4.65 - lr: 0.000003
2022-07-27 22:26:06,268 epoch 19 - iter 810/2703 - loss 0.21901253 - samples/sec: 4.56 - lr: 0.000003
2022-07-27 22:29:54,687 epoch 19 - iter 1080/2703 - loss 0.21967550 - samples/sec: 4.73 - lr: 0.000003
2022-07-27 22:33:51,563 epoch 19 - iter 1350/2703 - loss 0.21807170 - samples/sec: 4.56 - lr: 0.000003
2022-07-27 22:37:55,109 epoch 19 - iter 1620/2703 - loss 0.21754798 - samples/sec: 4.43 - lr: 0.000003
2022-07-27 22:41:47,402 epoch 19 - iter 1890/2703 - loss 0.21703130 - samples/sec: 4.65 - lr: 0.000003
2022-07-27 22:45:43,403 epoch 19 - iter 2160/2703 - loss 0.21719533 - samples/sec: 4.58 - lr: 0.000003
2022-07-27 22:49:42,343 epoch 19 - iter 2430/2703 - loss 0.21697180 - samples/sec: 4.52 - lr: 0.000003
2022-07-27 22:53:43,425 epoch 19 - iter 2700/2703 - loss 0.21757709 - samples/sec: 4.48 - lr: 0.000003
2022-07-27 22:53:45,615 ----------------------------------------------------------------------------------------------------
2022-07-27 22:53:45,615 EPOCH 19 done: loss 0.2176 - lr 0.000003
2022-07-27 23:00:16,368 Evaluating as a multi-label problem: False
2022-07-27 23:00:16,419 DEV : loss 0.03589833155274391 - f1-score (micro avg)  0.9751
2022-07-27 23:00:16,758 BAD EPOCHS (no improvement): 4
2022-07-27 23:00:16,762 saving best model
2022-07-27 23:00:42,493 ----------------------------------------------------------------------------------------------------
2022-07-27 23:04:42,956 epoch 20 - iter 270/2703 - loss 0.22126388 - samples/sec: 4.49 - lr: 0.000003
2022-07-27 23:08:42,363 epoch 20 - iter 540/2703 - loss 0.22544793 - samples/sec: 4.51 - lr: 0.000003
2022-07-27 23:12:35,694 epoch 20 - iter 810/2703 - loss 0.22457110 - samples/sec: 4.63 - lr: 0.000003
2022-07-27 23:16:23,414 epoch 20 - iter 1080/2703 - loss 0.22138674 - samples/sec: 4.74 - lr: 0.000003
2022-07-27 23:20:27,105 epoch 20 - iter 1350/2703 - loss 0.22457497 - samples/sec: 4.43 - lr: 0.000003
2022-07-27 23:24:22,157 epoch 20 - iter 1620/2703 - loss 0.22537640 - samples/sec: 4.60 - lr: 0.000003
2022-07-27 23:28:14,194 epoch 20 - iter 1890/2703 - loss 0.22281352 - samples/sec: 4.65 - lr: 0.000003
2022-07-27 23:32:10,747 epoch 20 - iter 2160/2703 - loss 0.22180269 - samples/sec: 4.57 - lr: 0.000003
2022-07-27 23:36:02,550 epoch 20 - iter 2430/2703 - loss 0.22076646 - samples/sec: 4.66 - lr: 0.000003
2022-07-27 23:39:57,053 epoch 20 - iter 2700/2703 - loss 0.22081061 - samples/sec: 4.61 - lr: 0.000003
2022-07-27 23:39:59,183 ----------------------------------------------------------------------------------------------------
2022-07-27 23:39:59,184 EPOCH 20 done: loss 0.2207 - lr 0.000003
2022-07-27 23:46:39,499 Evaluating as a multi-label problem: False
2022-07-27 23:46:39,554 DEV : loss 0.03709939122200012 - f1-score (micro avg)  0.9749
2022-07-27 23:46:39,886 BAD EPOCHS (no improvement): 4
2022-07-27 23:46:39,888 ----------------------------------------------------------------------------------------------------
2022-07-27 23:50:30,940 epoch 21 - iter 270/2703 - loss 0.22011796 - samples/sec: 4.67 - lr: 0.000003
2022-07-27 23:54:20,232 epoch 21 - iter 540/2703 - loss 0.21966860 - samples/sec: 4.71 - lr: 0.000003
2022-07-27 23:58:14,652 epoch 21 - iter 810/2703 - loss 0.21825237 - samples/sec: 4.61 - lr: 0.000003
2022-07-28 00:02:03,637 epoch 21 - iter 1080/2703 - loss 0.21881859 - samples/sec: 4.72 - lr: 0.000003
2022-07-28 00:05:56,096 epoch 21 - iter 1350/2703 - loss 0.21636180 - samples/sec: 4.65 - lr: 0.000003
2022-07-28 00:09:54,577 epoch 21 - iter 1620/2703 - loss 0.21636564 - samples/sec: 4.53 - lr: 0.000003
2022-07-28 00:13:51,837 epoch 21 - iter 1890/2703 - loss 0.21694407 - samples/sec: 4.55 - lr: 0.000003
2022-07-28 00:17:50,572 epoch 21 - iter 2160/2703 - loss 0.21644061 - samples/sec: 4.52 - lr: 0.000003
2022-07-28 00:21:48,801 epoch 21 - iter 2430/2703 - loss 0.21599264 - samples/sec: 4.53 - lr: 0.000003
2022-07-28 00:25:51,314 epoch 21 - iter 2700/2703 - loss 0.21663384 - samples/sec: 4.45 - lr: 0.000003
2022-07-28 00:25:53,628 ----------------------------------------------------------------------------------------------------
2022-07-28 00:25:53,628 EPOCH 21 done: loss 0.2167 - lr 0.000003
2022-07-28 00:32:28,115 Evaluating as a multi-label problem: False
2022-07-28 00:32:28,172 DEV : loss 0.03551744669675827 - f1-score (micro avg)  0.9762
2022-07-28 00:32:28,502 BAD EPOCHS (no improvement): 4
2022-07-28 00:32:28,507 saving best model
2022-07-28 00:32:54,261 ----------------------------------------------------------------------------------------------------
2022-07-28 00:36:44,894 epoch 22 - iter 270/2703 - loss 0.21120598 - samples/sec: 4.68 - lr: 0.000003
2022-07-28 00:40:46,065 epoch 22 - iter 540/2703 - loss 0.20726941 - samples/sec: 4.48 - lr: 0.000003
2022-07-28 00:44:45,313 epoch 22 - iter 810/2703 - loss 0.21094125 - samples/sec: 4.51 - lr: 0.000003
2022-07-28 00:48:39,132 epoch 22 - iter 1080/2703 - loss 0.21245758 - samples/sec: 4.62 - lr: 0.000003
2022-07-28 00:52:32,926 epoch 22 - iter 1350/2703 - loss 0.21350789 - samples/sec: 4.62 - lr: 0.000003
2022-07-28 00:56:30,997 epoch 22 - iter 1620/2703 - loss 0.21370224 - samples/sec: 4.54 - lr: 0.000003
2022-07-28 01:00:33,541 epoch 22 - iter 1890/2703 - loss 0.21484047 - samples/sec: 4.45 - lr: 0.000003
2022-07-28 01:04:31,001 epoch 22 - iter 2160/2703 - loss 0.21521058 - samples/sec: 4.55 - lr: 0.000003
2022-07-28 01:08:27,265 epoch 22 - iter 2430/2703 - loss 0.21662949 - samples/sec: 4.57 - lr: 0.000003
2022-07-28 01:12:25,929 epoch 22 - iter 2700/2703 - loss 0.21727183 - samples/sec: 4.53 - lr: 0.000003
2022-07-28 01:12:27,775 ----------------------------------------------------------------------------------------------------
2022-07-28 01:12:27,776 EPOCH 22 done: loss 0.2173 - lr 0.000003
2022-07-28 01:18:55,076 Evaluating as a multi-label problem: False
2022-07-28 01:18:55,130 DEV : loss 0.03778297081589699 - f1-score (micro avg)  0.9744
2022-07-28 01:18:55,467 BAD EPOCHS (no improvement): 4
2022-07-28 01:18:55,472 ----------------------------------------------------------------------------------------------------
2022-07-28 01:22:56,824 epoch 23 - iter 270/2703 - loss 0.20703189 - samples/sec: 4.48 - lr: 0.000002
2022-07-28 01:26:48,706 epoch 23 - iter 540/2703 - loss 0.21418720 - samples/sec: 4.66 - lr: 0.000002
2022-07-28 01:30:49,416 epoch 23 - iter 810/2703 - loss 0.21773358 - samples/sec: 4.49 - lr: 0.000002
2022-07-28 01:34:50,486 epoch 23 - iter 1080/2703 - loss 0.21846922 - samples/sec: 4.48 - lr: 0.000002
2022-07-28 01:38:51,841 epoch 23 - iter 1350/2703 - loss 0.21649706 - samples/sec: 4.48 - lr: 0.000002
2022-07-28 01:42:42,760 epoch 23 - iter 1620/2703 - loss 0.21781548 - samples/sec: 4.68 - lr: 0.000002
2022-07-28 01:46:37,154 epoch 23 - iter 1890/2703 - loss 0.21696564 - samples/sec: 4.61 - lr: 0.000002
2022-07-28 01:50:35,435 epoch 23 - iter 2160/2703 - loss 0.21562259 - samples/sec: 4.53 - lr: 0.000002
2022-07-28 01:54:29,722 epoch 23 - iter 2430/2703 - loss 0.21577637 - samples/sec: 4.61 - lr: 0.000002
2022-07-28 01:58:27,609 epoch 23 - iter 2700/2703 - loss 0.21591386 - samples/sec: 4.54 - lr: 0.000002
2022-07-28 01:58:30,393 ----------------------------------------------------------------------------------------------------
2022-07-28 01:58:30,393 EPOCH 23 done: loss 0.2160 - lr 0.000002
2022-07-28 02:04:52,508 Evaluating as a multi-label problem: False
2022-07-28 02:04:52,566 DEV : loss 0.039468228816986084 - f1-score (micro avg)  0.9719
2022-07-28 02:04:52,895 BAD EPOCHS (no improvement): 4
2022-07-28 02:04:52,900 ----------------------------------------------------------------------------------------------------
2022-07-28 02:08:54,801 epoch 24 - iter 270/2703 - loss 0.22832439 - samples/sec: 4.47 - lr: 0.000002
2022-07-28 02:12:40,577 epoch 24 - iter 540/2703 - loss 0.22115440 - samples/sec: 4.78 - lr: 0.000002
2022-07-28 02:16:40,261 epoch 24 - iter 810/2703 - loss 0.21802332 - samples/sec: 4.51 - lr: 0.000002
2022-07-28 02:20:35,320 epoch 24 - iter 1080/2703 - loss 0.21525905 - samples/sec: 4.59 - lr: 0.000002
2022-07-28 02:24:36,284 epoch 24 - iter 1350/2703 - loss 0.21387670 - samples/sec: 4.48 - lr: 0.000002
2022-07-28 02:28:37,989 epoch 24 - iter 1620/2703 - loss 0.21415996 - samples/sec: 4.47 - lr: 0.000002
2022-07-28 02:32:36,370 epoch 24 - iter 1890/2703 - loss 0.21444820 - samples/sec: 4.53 - lr: 0.000002
2022-07-28 02:36:27,253 epoch 24 - iter 2160/2703 - loss 0.21624555 - samples/sec: 4.68 - lr: 0.000002
2022-07-28 02:40:29,092 epoch 24 - iter 2430/2703 - loss 0.21477950 - samples/sec: 4.47 - lr: 0.000002
2022-07-28 02:44:21,861 epoch 24 - iter 2700/2703 - loss 0.21508280 - samples/sec: 4.64 - lr: 0.000002
2022-07-28 02:44:23,677 ----------------------------------------------------------------------------------------------------
2022-07-28 02:44:23,677 EPOCH 24 done: loss 0.2152 - lr 0.000002
2022-07-28 02:50:58,848 Evaluating as a multi-label problem: False
2022-07-28 02:50:58,905 DEV : loss 0.03406090289354324 - f1-score (micro avg)  0.977
2022-07-28 02:50:59,235 BAD EPOCHS (no improvement): 4
2022-07-28 02:50:59,242 saving best model
2022-07-28 02:51:25,597 ----------------------------------------------------------------------------------------------------
2022-07-28 02:55:21,666 epoch 25 - iter 270/2703 - loss 0.20711174 - samples/sec: 4.58 - lr: 0.000002
2022-07-28 02:59:21,228 epoch 25 - iter 540/2703 - loss 0.20681555 - samples/sec: 4.51 - lr: 0.000002
2022-07-28 03:03:15,736 epoch 25 - iter 810/2703 - loss 0.21178570 - samples/sec: 4.61 - lr: 0.000002
2022-07-28 03:07:20,597 epoch 25 - iter 1080/2703 - loss 0.21362782 - samples/sec: 4.41 - lr: 0.000002
2022-07-28 03:11:16,094 epoch 25 - iter 1350/2703 - loss 0.21308259 - samples/sec: 4.59 - lr: 0.000002
2022-07-28 03:15:18,612 epoch 25 - iter 1620/2703 - loss 0.21266688 - samples/sec: 4.45 - lr: 0.000002
2022-07-28 03:19:14,276 epoch 25 - iter 1890/2703 - loss 0.21344448 - samples/sec: 4.58 - lr: 0.000002
2022-07-28 03:23:05,976 epoch 25 - iter 2160/2703 - loss 0.21369667 - samples/sec: 4.66 - lr: 0.000002
2022-07-28 03:27:05,983 epoch 25 - iter 2430/2703 - loss 0.21467981 - samples/sec: 4.50 - lr: 0.000002
2022-07-28 03:31:02,966 epoch 25 - iter 2700/2703 - loss 0.21531650 - samples/sec: 4.56 - lr: 0.000002
2022-07-28 03:31:05,050 ----------------------------------------------------------------------------------------------------
2022-07-28 03:31:05,050 EPOCH 25 done: loss 0.2154 - lr 0.000002
2022-07-28 03:37:32,441 Evaluating as a multi-label problem: False
2022-07-28 03:37:32,496 DEV : loss 0.03626019135117531 - f1-score (micro avg)  0.9738
2022-07-28 03:37:32,828 BAD EPOCHS (no improvement): 4
2022-07-28 03:37:32,835 ----------------------------------------------------------------------------------------------------
2022-07-28 03:41:32,098 epoch 26 - iter 270/2703 - loss 0.21695985 - samples/sec: 4.51 - lr: 0.000002
2022-07-28 03:45:30,149 epoch 26 - iter 540/2703 - loss 0.21845369 - samples/sec: 4.54 - lr: 0.000002
2022-07-28 03:49:32,966 epoch 26 - iter 810/2703 - loss 0.21793573 - samples/sec: 4.45 - lr: 0.000002
2022-07-28 03:53:28,814 epoch 26 - iter 1080/2703 - loss 0.21887643 - samples/sec: 4.58 - lr: 0.000002
2022-07-28 03:57:22,446 epoch 26 - iter 1350/2703 - loss 0.21937756 - samples/sec: 4.62 - lr: 0.000002
2022-07-28 04:01:17,772 epoch 26 - iter 1620/2703 - loss 0.21953216 - samples/sec: 4.59 - lr: 0.000002
2022-07-28 04:05:14,312 epoch 26 - iter 1890/2703 - loss 0.21859476 - samples/sec: 4.57 - lr: 0.000002
2022-07-28 04:09:12,474 epoch 26 - iter 2160/2703 - loss 0.21684315 - samples/sec: 4.54 - lr: 0.000002
2022-07-28 04:13:10,040 epoch 26 - iter 2430/2703 - loss 0.21794074 - samples/sec: 4.55 - lr: 0.000002
2022-07-28 04:17:10,670 epoch 26 - iter 2700/2703 - loss 0.21768951 - samples/sec: 4.49 - lr: 0.000002
2022-07-28 04:17:12,639 ----------------------------------------------------------------------------------------------------
2022-07-28 04:17:12,639 EPOCH 26 done: loss 0.2177 - lr 0.000002
2022-07-28 04:23:44,810 Evaluating as a multi-label problem: False
2022-07-28 04:23:44,863 DEV : loss 0.037918705493211746 - f1-score (micro avg)  0.9747
2022-07-28 04:23:45,200 BAD EPOCHS (no improvement): 4
2022-07-28 04:23:45,203 ----------------------------------------------------------------------------------------------------
2022-07-28 04:27:40,877 epoch 27 - iter 270/2703 - loss 0.21033147 - samples/sec: 4.58 - lr: 0.000002
2022-07-28 04:31:40,222 epoch 27 - iter 540/2703 - loss 0.21064775 - samples/sec: 4.51 - lr: 0.000002
2022-07-28 04:35:39,939 epoch 27 - iter 810/2703 - loss 0.21023318 - samples/sec: 4.51 - lr: 0.000002
2022-07-28 04:39:42,918 epoch 27 - iter 1080/2703 - loss 0.20892600 - samples/sec: 4.45 - lr: 0.000002
2022-07-28 04:43:37,944 epoch 27 - iter 1350/2703 - loss 0.20958222 - samples/sec: 4.60 - lr: 0.000002
2022-07-28 04:47:36,005 epoch 27 - iter 1620/2703 - loss 0.20997566 - samples/sec: 4.54 - lr: 0.000002
2022-07-28 04:51:40,168 epoch 27 - iter 1890/2703 - loss 0.21013299 - samples/sec: 4.42 - lr: 0.000002
2022-07-28 04:55:30,614 epoch 27 - iter 2160/2703 - loss 0.20969500 - samples/sec: 4.69 - lr: 0.000002
2022-07-28 04:59:27,248 epoch 27 - iter 2430/2703 - loss 0.20953844 - samples/sec: 4.56 - lr: 0.000002
2022-07-28 05:03:22,402 epoch 27 - iter 2700/2703 - loss 0.20976570 - samples/sec: 4.59 - lr: 0.000002
2022-07-28 05:03:24,768 ----------------------------------------------------------------------------------------------------
2022-07-28 05:03:24,768 EPOCH 27 done: loss 0.2098 - lr 0.000002
2022-07-28 05:09:59,233 Evaluating as a multi-label problem: False
2022-07-28 05:09:59,286 DEV : loss 0.03742002323269844 - f1-score (micro avg)  0.9765
2022-07-28 05:09:59,621 BAD EPOCHS (no improvement): 4
2022-07-28 05:09:59,626 ----------------------------------------------------------------------------------------------------
2022-07-28 05:13:50,075 epoch 28 - iter 270/2703 - loss 0.22139685 - samples/sec: 4.69 - lr: 0.000002
2022-07-28 05:17:52,179 epoch 28 - iter 540/2703 - loss 0.22113446 - samples/sec: 4.46 - lr: 0.000002
2022-07-28 05:21:54,861 epoch 28 - iter 810/2703 - loss 0.21566952 - samples/sec: 4.45 - lr: 0.000002
2022-07-28 05:25:58,863 epoch 28 - iter 1080/2703 - loss 0.21408468 - samples/sec: 4.43 - lr: 0.000002
2022-07-28 05:30:02,625 epoch 28 - iter 1350/2703 - loss 0.21423491 - samples/sec: 4.43 - lr: 0.000002
2022-07-28 05:33:55,738 epoch 28 - iter 1620/2703 - loss 0.21348074 - samples/sec: 4.63 - lr: 0.000002
2022-07-28 05:37:45,098 epoch 28 - iter 1890/2703 - loss 0.21445643 - samples/sec: 4.71 - lr: 0.000002
2022-07-28 05:41:39,009 epoch 28 - iter 2160/2703 - loss 0.21371918 - samples/sec: 4.62 - lr: 0.000002
2022-07-28 05:45:32,251 epoch 28 - iter 2430/2703 - loss 0.21303404 - samples/sec: 4.63 - lr: 0.000002
2022-07-28 05:49:26,966 epoch 28 - iter 2700/2703 - loss 0.21279893 - samples/sec: 4.60 - lr: 0.000002
2022-07-28 05:49:29,451 ----------------------------------------------------------------------------------------------------
2022-07-28 05:49:29,452 EPOCH 28 done: loss 0.2127 - lr 0.000002
2022-07-28 05:55:55,672 Evaluating as a multi-label problem: False
2022-07-28 05:55:55,722 DEV : loss 0.03936390578746796 - f1-score (micro avg)  0.9753
2022-07-28 05:55:56,055 BAD EPOCHS (no improvement): 4
2022-07-28 05:55:56,059 ----------------------------------------------------------------------------------------------------
2022-07-28 05:59:54,135 epoch 29 - iter 270/2703 - loss 0.20680878 - samples/sec: 4.54 - lr: 0.000002
2022-07-28 06:03:46,195 epoch 29 - iter 540/2703 - loss 0.21529110 - samples/sec: 4.65 - lr: 0.000002
2022-07-28 06:07:37,645 epoch 29 - iter 810/2703 - loss 0.21154688 - samples/sec: 4.67 - lr: 0.000002
2022-07-28 06:11:39,203 epoch 29 - iter 1080/2703 - loss 0.21186099 - samples/sec: 4.47 - lr: 0.000002
2022-07-28 06:15:41,460 epoch 29 - iter 1350/2703 - loss 0.21442948 - samples/sec: 4.46 - lr: 0.000002
2022-07-28 06:19:29,046 epoch 29 - iter 1620/2703 - loss 0.21447730 - samples/sec: 4.75 - lr: 0.000002
2022-07-28 06:23:22,543 epoch 29 - iter 1890/2703 - loss 0.21297983 - samples/sec: 4.63 - lr: 0.000002
2022-07-28 06:27:21,272 epoch 29 - iter 2160/2703 - loss 0.21207121 - samples/sec: 4.52 - lr: 0.000002
2022-07-28 06:31:18,720 epoch 29 - iter 2430/2703 - loss 0.21182958 - samples/sec: 4.55 - lr: 0.000002
2022-07-28 06:35:16,068 epoch 29 - iter 2700/2703 - loss 0.21302639 - samples/sec: 4.55 - lr: 0.000002
2022-07-28 06:35:19,722 ----------------------------------------------------------------------------------------------------
2022-07-28 06:35:19,723 EPOCH 29 done: loss 0.2130 - lr 0.000002
2022-07-28 06:41:54,078 Evaluating as a multi-label problem: False
2022-07-28 06:41:54,131 DEV : loss 0.04156726598739624 - f1-score (micro avg)  0.9742
2022-07-28 06:41:54,469 BAD EPOCHS (no improvement): 4
2022-07-28 06:41:54,470 ----------------------------------------------------------------------------------------------------
2022-07-28 06:45:43,567 epoch 30 - iter 270/2703 - loss 0.21600717 - samples/sec: 4.71 - lr: 0.000002
2022-07-28 06:49:52,182 epoch 30 - iter 540/2703 - loss 0.20963469 - samples/sec: 4.34 - lr: 0.000002
2022-07-28 06:53:47,408 epoch 30 - iter 810/2703 - loss 0.21369731 - samples/sec: 4.59 - lr: 0.000001
2022-07-28 06:57:46,505 epoch 30 - iter 1080/2703 - loss 0.21122053 - samples/sec: 4.52 - lr: 0.000001
2022-07-28 07:01:39,471 epoch 30 - iter 1350/2703 - loss 0.21070652 - samples/sec: 4.64 - lr: 0.000001
2022-07-28 07:05:37,963 epoch 30 - iter 1620/2703 - loss 0.21234041 - samples/sec: 4.53 - lr: 0.000001
2022-07-28 07:09:31,245 epoch 30 - iter 1890/2703 - loss 0.21216091 - samples/sec: 4.63 - lr: 0.000001
2022-07-28 07:13:28,757 epoch 30 - iter 2160/2703 - loss 0.21232481 - samples/sec: 4.55 - lr: 0.000001
2022-07-28 07:17:31,144 epoch 30 - iter 2430/2703 - loss 0.21279437 - samples/sec: 4.46 - lr: 0.000001
2022-07-28 07:21:18,911 epoch 30 - iter 2700/2703 - loss 0.21262752 - samples/sec: 4.74 - lr: 0.000001
2022-07-28 07:21:21,260 ----------------------------------------------------------------------------------------------------
2022-07-28 07:21:21,260 EPOCH 30 done: loss 0.2126 - lr 0.000001
2022-07-28 07:27:50,250 Evaluating as a multi-label problem: False
2022-07-28 07:27:50,307 DEV : loss 0.03897550329566002 - f1-score (micro avg)  0.9757
2022-07-28 07:27:50,637 BAD EPOCHS (no improvement): 4
2022-07-28 07:27:50,642 ----------------------------------------------------------------------------------------------------
2022-07-28 07:31:42,053 epoch 31 - iter 270/2703 - loss 0.21829450 - samples/sec: 4.67 - lr: 0.000001
2022-07-28 07:35:31,107 epoch 31 - iter 540/2703 - loss 0.22086258 - samples/sec: 4.72 - lr: 0.000001
2022-07-28 07:39:37,479 epoch 31 - iter 810/2703 - loss 0.22060162 - samples/sec: 4.38 - lr: 0.000001
2022-07-28 07:43:33,637 epoch 31 - iter 1080/2703 - loss 0.21801900 - samples/sec: 4.57 - lr: 0.000001
2022-07-28 07:47:33,956 epoch 31 - iter 1350/2703 - loss 0.21675641 - samples/sec: 4.49 - lr: 0.000001
2022-07-28 07:51:32,425 epoch 31 - iter 1620/2703 - loss 0.21467836 - samples/sec: 4.53 - lr: 0.000001
2022-07-28 07:55:20,380 epoch 31 - iter 1890/2703 - loss 0.21467538 - samples/sec: 4.74 - lr: 0.000001
2022-07-28 07:59:22,605 epoch 31 - iter 2160/2703 - loss 0.21598259 - samples/sec: 4.46 - lr: 0.000001
2022-07-28 08:03:17,621 epoch 31 - iter 2430/2703 - loss 0.21434804 - samples/sec: 4.60 - lr: 0.000001
2022-07-28 08:07:11,489 epoch 31 - iter 2700/2703 - loss 0.21370518 - samples/sec: 4.62 - lr: 0.000001
2022-07-28 08:07:14,099 ----------------------------------------------------------------------------------------------------
2022-07-28 08:07:14,100 EPOCH 31 done: loss 0.2137 - lr 0.000001
2022-07-28 08:13:49,299 Evaluating as a multi-label problem: False
2022-07-28 08:13:49,355 DEV : loss 0.04019349813461304 - f1-score (micro avg)  0.9752
2022-07-28 08:13:49,693 BAD EPOCHS (no improvement): 4
2022-07-28 08:13:49,697 ----------------------------------------------------------------------------------------------------
2022-07-28 08:17:43,503 epoch 32 - iter 270/2703 - loss 0.21422145 - samples/sec: 4.62 - lr: 0.000001
2022-07-28 08:21:37,571 epoch 32 - iter 540/2703 - loss 0.21891551 - samples/sec: 4.61 - lr: 0.000001
2022-07-28 08:25:29,768 epoch 32 - iter 810/2703 - loss 0.21779290 - samples/sec: 4.65 - lr: 0.000001
2022-07-28 08:29:25,245 epoch 32 - iter 1080/2703 - loss 0.21789041 - samples/sec: 4.59 - lr: 0.000001
2022-07-28 08:33:29,234 epoch 32 - iter 1350/2703 - loss 0.21547089 - samples/sec: 4.43 - lr: 0.000001
2022-07-28 08:37:28,072 epoch 32 - iter 1620/2703 - loss 0.21480144 - samples/sec: 4.52 - lr: 0.000001
2022-07-28 08:41:20,537 epoch 32 - iter 1890/2703 - loss 0.21467526 - samples/sec: 4.65 - lr: 0.000001
2022-07-28 08:45:18,298 epoch 32 - iter 2160/2703 - loss 0.21467838 - samples/sec: 4.54 - lr: 0.000001
2022-07-28 08:49:17,318 epoch 32 - iter 2430/2703 - loss 0.21442324 - samples/sec: 4.52 - lr: 0.000001
2022-07-28 08:53:17,493 epoch 32 - iter 2700/2703 - loss 0.21457739 - samples/sec: 4.50 - lr: 0.000001
2022-07-28 08:53:20,348 ----------------------------------------------------------------------------------------------------
2022-07-28 08:53:20,348 EPOCH 32 done: loss 0.2145 - lr 0.000001
2022-07-28 09:00:01,362 Evaluating as a multi-label problem: False
2022-07-28 09:00:01,418 DEV : loss 0.037762705236673355 - f1-score (micro avg)  0.9757
2022-07-28 09:00:01,755 BAD EPOCHS (no improvement): 4
2022-07-28 09:00:01,757 ----------------------------------------------------------------------------------------------------
2022-07-28 09:03:52,568 epoch 33 - iter 270/2703 - loss 0.20579657 - samples/sec: 4.68 - lr: 0.000001
2022-07-28 09:07:55,724 epoch 33 - iter 540/2703 - loss 0.21050777 - samples/sec: 4.44 - lr: 0.000001
2022-07-28 09:11:53,062 epoch 33 - iter 810/2703 - loss 0.20993242 - samples/sec: 4.55 - lr: 0.000001
2022-07-28 09:15:51,064 epoch 33 - iter 1080/2703 - loss 0.21121319 - samples/sec: 4.54 - lr: 0.000001
2022-07-28 09:19:46,932 epoch 33 - iter 1350/2703 - loss 0.20963236 - samples/sec: 4.58 - lr: 0.000001
2022-07-28 09:23:41,324 epoch 33 - iter 1620/2703 - loss 0.20984826 - samples/sec: 4.61 - lr: 0.000001
2022-07-28 09:27:45,168 epoch 33 - iter 1890/2703 - loss 0.21174823 - samples/sec: 4.43 - lr: 0.000001
2022-07-28 09:31:39,160 epoch 33 - iter 2160/2703 - loss 0.21207326 - samples/sec: 4.62 - lr: 0.000001
2022-07-28 09:35:38,099 epoch 33 - iter 2430/2703 - loss 0.21244766 - samples/sec: 4.52 - lr: 0.000001
2022-07-28 09:39:35,621 epoch 33 - iter 2700/2703 - loss 0.21390189 - samples/sec: 4.55 - lr: 0.000001
2022-07-28 09:39:38,088 ----------------------------------------------------------------------------------------------------
2022-07-28 09:39:38,088 EPOCH 33 done: loss 0.2138 - lr 0.000001
2022-07-28 09:46:10,843 Evaluating as a multi-label problem: False
2022-07-28 09:46:10,902 DEV : loss 0.03868349641561508 - f1-score (micro avg)  0.9756
2022-07-28 09:46:11,235 BAD EPOCHS (no improvement): 4
2022-07-28 09:46:11,240 ----------------------------------------------------------------------------------------------------
2022-07-28 09:50:05,116 epoch 34 - iter 270/2703 - loss 0.21162546 - samples/sec: 4.62 - lr: 0.000001
2022-07-28 09:54:03,790 epoch 34 - iter 540/2703 - loss 0.21106958 - samples/sec: 4.53 - lr: 0.000001
2022-07-28 09:57:54,221 epoch 34 - iter 810/2703 - loss 0.21022269 - samples/sec: 4.69 - lr: 0.000001
2022-07-28 10:01:55,700 epoch 34 - iter 1080/2703 - loss 0.20884765 - samples/sec: 4.47 - lr: 0.000001
2022-07-28 10:05:51,203 epoch 34 - iter 1350/2703 - loss 0.20953377 - samples/sec: 4.59 - lr: 0.000001
2022-07-28 10:09:48,636 epoch 34 - iter 1620/2703 - loss 0.20953247 - samples/sec: 4.55 - lr: 0.000001
2022-07-28 10:13:45,862 epoch 34 - iter 1890/2703 - loss 0.20967854 - samples/sec: 4.55 - lr: 0.000001
2022-07-28 10:17:47,082 epoch 34 - iter 2160/2703 - loss 0.21070240 - samples/sec: 4.48 - lr: 0.000001
2022-07-28 10:21:48,639 epoch 34 - iter 2430/2703 - loss 0.20943925 - samples/sec: 4.47 - lr: 0.000001
2022-07-28 10:25:44,806 epoch 34 - iter 2700/2703 - loss 0.21017902 - samples/sec: 4.57 - lr: 0.000001
2022-07-28 10:25:47,064 ----------------------------------------------------------------------------------------------------
2022-07-28 10:25:47,064 EPOCH 34 done: loss 0.2102 - lr 0.000001
2022-07-28 10:32:18,062 Evaluating as a multi-label problem: False
2022-07-28 10:32:18,117 DEV : loss 0.03908322751522064 - f1-score (micro avg)  0.9756
2022-07-28 10:32:18,452 BAD EPOCHS (no improvement): 4
2022-07-28 10:32:18,457 ----------------------------------------------------------------------------------------------------
2022-07-28 10:36:17,467 epoch 35 - iter 270/2703 - loss 0.20263732 - samples/sec: 4.52 - lr: 0.000001
2022-07-28 10:40:13,351 epoch 35 - iter 540/2703 - loss 0.20871732 - samples/sec: 4.58 - lr: 0.000001
2022-07-28 10:44:07,051 epoch 35 - iter 810/2703 - loss 0.20914042 - samples/sec: 4.62 - lr: 0.000001
2022-07-28 10:47:58,180 epoch 35 - iter 1080/2703 - loss 0.20826899 - samples/sec: 4.67 - lr: 0.000001
2022-07-28 10:51:56,854 epoch 35 - iter 1350/2703 - loss 0.20859259 - samples/sec: 4.53 - lr: 0.000001
2022-07-28 10:55:53,354 epoch 35 - iter 1620/2703 - loss 0.20910642 - samples/sec: 4.57 - lr: 0.000001
2022-07-28 10:59:52,856 epoch 35 - iter 1890/2703 - loss 0.20808251 - samples/sec: 4.51 - lr: 0.000001
2022-07-28 11:03:53,823 epoch 35 - iter 2160/2703 - loss 0.20994307 - samples/sec: 4.48 - lr: 0.000001
2022-07-28 11:07:57,387 epoch 35 - iter 2430/2703 - loss 0.20964012 - samples/sec: 4.43 - lr: 0.000001
2022-07-28 11:11:51,489 epoch 35 - iter 2700/2703 - loss 0.21019401 - samples/sec: 4.61 - lr: 0.000001
2022-07-28 11:11:53,420 ----------------------------------------------------------------------------------------------------
2022-07-28 11:11:53,421 EPOCH 35 done: loss 0.2102 - lr 0.000001
2022-07-28 11:18:19,284 Evaluating as a multi-label problem: False
2022-07-28 11:18:19,341 DEV : loss 0.03976050391793251 - f1-score (micro avg)  0.9756
2022-07-28 11:18:19,671 BAD EPOCHS (no improvement): 4
2022-07-28 11:18:19,676 ----------------------------------------------------------------------------------------------------
2022-07-28 11:22:12,167 epoch 36 - iter 270/2703 - loss 0.21060860 - samples/sec: 4.65 - lr: 0.000001
2022-07-28 11:26:15,762 epoch 36 - iter 540/2703 - loss 0.21150880 - samples/sec: 4.43 - lr: 0.000001
2022-07-28 11:30:15,821 epoch 36 - iter 810/2703 - loss 0.21259973 - samples/sec: 4.50 - lr: 0.000001
2022-07-28 11:34:15,234 epoch 36 - iter 1080/2703 - loss 0.20995511 - samples/sec: 4.51 - lr: 0.000001
2022-07-28 11:38:14,793 epoch 36 - iter 1350/2703 - loss 0.20942653 - samples/sec: 4.51 - lr: 0.000001
2022-07-28 11:42:11,541 epoch 36 - iter 1620/2703 - loss 0.20919316 - samples/sec: 4.56 - lr: 0.000001
2022-07-28 11:46:10,368 epoch 36 - iter 1890/2703 - loss 0.21027054 - samples/sec: 4.52 - lr: 0.000001
2022-07-28 11:50:06,628 epoch 36 - iter 2160/2703 - loss 0.21149432 - samples/sec: 4.57 - lr: 0.000001
2022-07-28 11:54:02,602 epoch 36 - iter 2430/2703 - loss 0.21166546 - samples/sec: 4.58 - lr: 0.000001
2022-07-28 11:58:00,251 epoch 36 - iter 2700/2703 - loss 0.21195322 - samples/sec: 4.54 - lr: 0.000001
2022-07-28 11:58:02,522 ----------------------------------------------------------------------------------------------------
2022-07-28 11:58:02,523 EPOCH 36 done: loss 0.2120 - lr 0.000001
2022-07-28 12:04:34,391 Evaluating as a multi-label problem: False
2022-07-28 12:04:34,444 DEV : loss 0.03978428244590759 - f1-score (micro avg)  0.9752
2022-07-28 12:04:34,770 BAD EPOCHS (no improvement): 4
2022-07-28 12:04:34,777 ----------------------------------------------------------------------------------------------------
2022-07-28 12:08:32,090 epoch 37 - iter 270/2703 - loss 0.21265673 - samples/sec: 4.55 - lr: 0.000001
2022-07-28 12:12:30,013 epoch 37 - iter 540/2703 - loss 0.21693676 - samples/sec: 4.54 - lr: 0.000001
2022-07-28 12:16:26,794 epoch 37 - iter 810/2703 - loss 0.21574289 - samples/sec: 4.56 - lr: 0.000001
2022-07-28 12:20:24,520 epoch 37 - iter 1080/2703 - loss 0.21558931 - samples/sec: 4.54 - lr: 0.000001
2022-07-28 12:24:17,602 epoch 37 - iter 1350/2703 - loss 0.21444611 - samples/sec: 4.63 - lr: 0.000000
2022-07-28 12:28:19,348 epoch 37 - iter 1620/2703 - loss 0.21347445 - samples/sec: 4.47 - lr: 0.000000
2022-07-28 12:32:20,488 epoch 37 - iter 1890/2703 - loss 0.21329414 - samples/sec: 4.48 - lr: 0.000000
2022-07-28 12:36:13,258 epoch 37 - iter 2160/2703 - loss 0.21371371 - samples/sec: 4.64 - lr: 0.000000
2022-07-28 12:40:04,469 epoch 37 - iter 2430/2703 - loss 0.21322800 - samples/sec: 4.67 - lr: 0.000000
2022-07-28 12:44:07,872 epoch 37 - iter 2700/2703 - loss 0.21259017 - samples/sec: 4.44 - lr: 0.000000
2022-07-28 12:44:10,611 ----------------------------------------------------------------------------------------------------
2022-07-28 12:44:10,611 EPOCH 37 done: loss 0.2126 - lr 0.000000
2022-07-28 12:50:41,239 Evaluating as a multi-label problem: False
2022-07-28 12:50:41,294 DEV : loss 0.04001642018556595 - f1-score (micro avg)  0.9751
2022-07-28 12:50:41,624 BAD EPOCHS (no improvement): 4
2022-07-28 12:50:41,628 ----------------------------------------------------------------------------------------------------
2022-07-28 12:54:26,248 epoch 38 - iter 270/2703 - loss 0.21415067 - samples/sec: 4.81 - lr: 0.000000
2022-07-28 12:58:22,210 epoch 38 - iter 540/2703 - loss 0.21621120 - samples/sec: 4.58 - lr: 0.000000
2022-07-28 13:02:19,801 epoch 38 - iter 810/2703 - loss 0.21671834 - samples/sec: 4.55 - lr: 0.000000
2022-07-28 13:06:21,621 epoch 38 - iter 1080/2703 - loss 0.21374297 - samples/sec: 4.47 - lr: 0.000000
2022-07-28 13:10:24,483 epoch 38 - iter 1350/2703 - loss 0.21324162 - samples/sec: 4.45 - lr: 0.000000
2022-07-28 13:14:17,031 epoch 38 - iter 1620/2703 - loss 0.21354774 - samples/sec: 4.64 - lr: 0.000000
2022-07-28 13:18:05,993 epoch 38 - iter 1890/2703 - loss 0.21296500 - samples/sec: 4.72 - lr: 0.000000
2022-07-28 13:22:02,987 epoch 38 - iter 2160/2703 - loss 0.21143863 - samples/sec: 4.56 - lr: 0.000000
2022-07-28 13:25:54,099 epoch 38 - iter 2430/2703 - loss 0.21072816 - samples/sec: 4.67 - lr: 0.000000
2022-07-28 13:30:00,346 epoch 38 - iter 2700/2703 - loss 0.21041699 - samples/sec: 4.39 - lr: 0.000000
2022-07-28 13:30:03,964 ----------------------------------------------------------------------------------------------------
2022-07-28 13:30:03,965 EPOCH 38 done: loss 0.2105 - lr 0.000000
2022-07-28 13:36:31,034 Evaluating as a multi-label problem: False
2022-07-28 13:36:31,089 DEV : loss 0.040138933807611465 - f1-score (micro avg)  0.9746
2022-07-28 13:36:31,420 BAD EPOCHS (no improvement): 4
2022-07-28 13:36:31,427 ----------------------------------------------------------------------------------------------------
2022-07-28 13:40:29,019 epoch 39 - iter 270/2703 - loss 0.21226238 - samples/sec: 4.55 - lr: 0.000000
2022-07-28 13:44:19,494 epoch 39 - iter 540/2703 - loss 0.21246281 - samples/sec: 4.69 - lr: 0.000000
2022-07-28 13:48:17,276 epoch 39 - iter 810/2703 - loss 0.20999285 - samples/sec: 4.54 - lr: 0.000000
2022-07-28 13:52:14,174 epoch 39 - iter 1080/2703 - loss 0.21047489 - samples/sec: 4.56 - lr: 0.000000
2022-07-28 13:55:59,861 epoch 39 - iter 1350/2703 - loss 0.21008716 - samples/sec: 4.79 - lr: 0.000000
2022-07-28 13:59:50,504 epoch 39 - iter 1620/2703 - loss 0.21067462 - samples/sec: 4.68 - lr: 0.000000
2022-07-28 14:03:46,087 epoch 39 - iter 1890/2703 - loss 0.20995993 - samples/sec: 4.58 - lr: 0.000000
2022-07-28 14:07:44,103 epoch 39 - iter 2160/2703 - loss 0.21098484 - samples/sec: 4.54 - lr: 0.000000
2022-07-28 14:11:47,792 epoch 39 - iter 2430/2703 - loss 0.21077354 - samples/sec: 4.43 - lr: 0.000000
2022-07-28 14:15:49,929 epoch 39 - iter 2700/2703 - loss 0.21104942 - samples/sec: 4.46 - lr: 0.000000
2022-07-28 14:15:51,789 ----------------------------------------------------------------------------------------------------
2022-07-28 14:15:51,789 EPOCH 39 done: loss 0.2110 - lr 0.000000
2022-07-28 14:22:24,345 Evaluating as a multi-label problem: False
2022-07-28 14:22:24,405 DEV : loss 0.04056908190250397 - f1-score (micro avg)  0.975
2022-07-28 14:22:24,748 BAD EPOCHS (no improvement): 4
2022-07-28 14:22:24,753 ----------------------------------------------------------------------------------------------------
2022-07-28 14:26:32,494 epoch 40 - iter 270/2703 - loss 0.19794065 - samples/sec: 4.36 - lr: 0.000000
2022-07-28 14:30:27,713 epoch 40 - iter 540/2703 - loss 0.20137167 - samples/sec: 4.59 - lr: 0.000000
2022-07-28 14:34:23,200 epoch 40 - iter 810/2703 - loss 0.20826493 - samples/sec: 4.59 - lr: 0.000000
2022-07-28 14:38:18,134 epoch 40 - iter 1080/2703 - loss 0.20949863 - samples/sec: 4.60 - lr: 0.000000
2022-07-28 14:42:15,020 epoch 40 - iter 1350/2703 - loss 0.21002319 - samples/sec: 4.56 - lr: 0.000000
2022-07-28 14:46:17,097 epoch 40 - iter 1620/2703 - loss 0.20907701 - samples/sec: 4.46 - lr: 0.000000
2022-07-28 14:50:09,707 epoch 40 - iter 1890/2703 - loss 0.20916147 - samples/sec: 4.64 - lr: 0.000000
2022-07-28 14:54:10,586 epoch 40 - iter 2160/2703 - loss 0.20961760 - samples/sec: 4.48 - lr: 0.000000
2022-07-28 14:58:07,652 epoch 40 - iter 2430/2703 - loss 0.21122179 - samples/sec: 4.56 - lr: 0.000000
2022-07-28 15:02:02,845 epoch 40 - iter 2700/2703 - loss 0.21042112 - samples/sec: 4.59 - lr: 0.000000
2022-07-28 15:02:05,258 ----------------------------------------------------------------------------------------------------
2022-07-28 15:02:05,259 EPOCH 40 done: loss 0.2104 - lr 0.000000
2022-07-28 15:08:35,983 Evaluating as a multi-label problem: False
2022-07-28 15:08:36,035 DEV : loss 0.04051869735121727 - f1-score (micro avg)  0.9748
2022-07-28 15:08:36,370 BAD EPOCHS (no improvement): 4
2022-07-28 15:08:40,112 ----------------------------------------------------------------------------------------------------
2022-07-28 15:08:40,114 loading file experiments/corpus_sentence_grid_search_roberta_docstart/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased-context_FT_True_Ly_-1_seed_12_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.1/0/best-model.pt
2022-07-28 15:09:42,463 SequenceTagger predicts: Dictionary with 89 tags: O, S-TERRITORIO, B-TERRITORIO, E-TERRITORIO, I-TERRITORIO, S-FECHAS, B-FECHAS, E-FECHAS, I-FECHAS, S-EDAD_SUJETO_ASISTENCIA, B-EDAD_SUJETO_ASISTENCIA, E-EDAD_SUJETO_ASISTENCIA, I-EDAD_SUJETO_ASISTENCIA, S-NOMBRE_SUJETO_ASISTENCIA, B-NOMBRE_SUJETO_ASISTENCIA, E-NOMBRE_SUJETO_ASISTENCIA, I-NOMBRE_SUJETO_ASISTENCIA, S-NOMBRE_PERSONAL_SANITARIO, B-NOMBRE_PERSONAL_SANITARIO, E-NOMBRE_PERSONAL_SANITARIO, I-NOMBRE_PERSONAL_SANITARIO, S-SEXO_SUJETO_ASISTENCIA, B-SEXO_SUJETO_ASISTENCIA, E-SEXO_SUJETO_ASISTENCIA, I-SEXO_SUJETO_ASISTENCIA, S-CALLE, B-CALLE, E-CALLE, I-CALLE, S-PAIS, B-PAIS, E-PAIS, I-PAIS, S-ID_SUJETO_ASISTENCIA, B-ID_SUJETO_ASISTENCIA, E-ID_SUJETO_ASISTENCIA, I-ID_SUJETO_ASISTENCIA, S-ID_TITULACION_PERSONAL_SANITARIO, B-ID_TITULACION_PERSONAL_SANITARIO, E-ID_TITULACION_PERSONAL_SANITARIO, I-ID_TITULACION_PERSONAL_SANITARIO, S-CORREO_ELECTRONICO, B-CORREO_ELECTRONICO, E-CORREO_ELECTRONICO, I-CORREO_ELECTRONICO, S-ID_ASEGURAMIENTO, B-ID_ASEGURAMIENTO, E-ID_ASEGURAMIENTO, I-ID_ASEGURAMIENTO, S-HOSPITAL
2022-07-28 15:15:53,284 Evaluating as a multi-label problem: False
2022-07-28 15:15:53,334 0.9699	0.9776	0.9737	0.9532
2022-07-28 15:15:53,335 
Results:
- F-score (micro) 0.9737
- F-score (macro) 0.874
- Accuracy 0.9532

By class:
                                  precision    recall  f1-score   support

                      TERRITORIO     0.9853    0.9801    0.9827       956
                          FECHAS     0.9935    0.9951    0.9943       611
          EDAD_SUJETO_ASISTENCIA     0.9717    0.9942    0.9828       518
        NOMBRE_SUJETO_ASISTENCIA     0.9960    1.0000    0.9980       502
       NOMBRE_PERSONAL_SANITARIO     0.9940    0.9960    0.9950       501
          SEXO_SUJETO_ASISTENCIA     0.9892    0.9913    0.9902       461
                           CALLE     0.9403    0.9540    0.9471       413
                            PAIS     0.9837    0.9972    0.9904       363
            ID_SUJETO_ASISTENCIA     0.9722    0.9894    0.9807       283
              CORREO_ELECTRONICO     0.9841    0.9960    0.9900       249
ID_TITULACION_PERSONAL_SANITARIO     0.9957    1.0000    0.9979       234
                ID_ASEGURAMIENTO     0.9949    0.9949    0.9949       198
                        HOSPITAL     0.9219    0.9077    0.9147       130
    FAMILIARES_SUJETO_ASISTENCIA     0.7312    0.8395    0.7816        81
                     INSTITUCION     0.5000    0.4925    0.4962        67
         ID_CONTACTO_ASISTENCIAL     1.0000    0.9744    0.9870        39
                 NUMERO_TELEFONO     0.8276    0.9231    0.8727        26
                       PROFESION     0.4706    0.8889    0.6154         9
                      NUMERO_FAX     0.8750    1.0000    0.9333         7
                    CENTRO_SALUD     1.0000    0.8333    0.9091         6
         OTROS_SUJETO_ASISTENCIA     0.0000    0.0000    0.0000         7

                       micro avg     0.9699    0.9776    0.9737      5661
                       macro avg     0.8632    0.8927    0.8740      5661
                    weighted avg     0.9700    0.9776    0.9736      5661

2022-07-28 15:15:53,335 ----------------------------------------------------------------------------------------------------
