2022-09-28 10:52:44,434 ----------------------------------------------------------------------------------------------------
2022-09-28 10:52:44,437 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=89, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2022-09-28 10:52:44,437 ----------------------------------------------------------------------------------------------------
2022-09-28 10:52:44,438 Corpus: "Corpus: 10311 train + 5268 dev + 5155 test sentences"
2022-09-28 10:52:44,438 ----------------------------------------------------------------------------------------------------
2022-09-28 10:52:44,438 Parameters:
2022-09-28 10:52:44,438  - learning_rate: "0.000005"
2022-09-28 10:52:44,438  - mini_batch_size: "4"
2022-09-28 10:52:44,438  - patience: "3"
2022-09-28 10:52:44,438  - anneal_factor: "0.5"
2022-09-28 10:52:44,438  - max_epochs: "40"
2022-09-28 10:52:44,438  - shuffle: "True"
2022-09-28 10:52:44,438  - train_with_dev: "False"
2022-09-28 10:52:44,438  - batch_growth_annealing: "False"
2022-09-28 10:52:44,438 ----------------------------------------------------------------------------------------------------
2022-09-28 10:52:44,438 Model training base path: "experiments/corpus_sentence_xlmrl_finetune/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased_FT_True_Ly_-1_seed_33_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.05/0"
2022-09-28 10:52:44,438 ----------------------------------------------------------------------------------------------------
2022-09-28 10:52:44,439 Device: cuda:1
2022-09-28 10:52:44,439 ----------------------------------------------------------------------------------------------------
2022-09-28 10:52:44,439 Embeddings storage mode: gpu
2022-09-28 10:52:44,439 ----------------------------------------------------------------------------------------------------
2022-09-28 10:54:36,804 epoch 1 - iter 257/2578 - loss 4.13043707 - samples/sec: 9.15 - lr: 0.000000
2022-09-28 10:56:30,898 epoch 1 - iter 514/2578 - loss 3.75169072 - samples/sec: 9.01 - lr: 0.000000
2022-09-28 10:58:43,544 epoch 1 - iter 771/2578 - loss 2.81231006 - samples/sec: 7.75 - lr: 0.000001
2022-09-28 11:00:54,854 epoch 1 - iter 1028/2578 - loss 2.28310398 - samples/sec: 7.83 - lr: 0.000001
2022-09-28 11:02:51,287 epoch 1 - iter 1285/2578 - loss 2.00818359 - samples/sec: 8.83 - lr: 0.000001
2022-09-28 11:04:58,581 epoch 1 - iter 1542/2578 - loss 1.76946203 - samples/sec: 8.08 - lr: 0.000001
2022-09-28 11:07:00,369 epoch 1 - iter 1799/2578 - loss 1.60667681 - samples/sec: 8.44 - lr: 0.000002
2022-09-28 11:09:07,808 epoch 1 - iter 2056/2578 - loss 1.45351618 - samples/sec: 8.07 - lr: 0.000002
2022-09-28 11:11:15,558 epoch 1 - iter 2313/2578 - loss 1.33150662 - samples/sec: 8.05 - lr: 0.000002
2022-09-28 11:13:11,497 epoch 1 - iter 2570/2578 - loss 1.24881135 - samples/sec: 8.87 - lr: 0.000002
2022-09-28 11:13:15,773 ----------------------------------------------------------------------------------------------------
2022-09-28 11:13:15,773 EPOCH 1 done: loss 1.2453 - lr 0.000002
2022-09-28 11:15:18,036 Evaluating as a multi-label problem: False
2022-09-28 11:15:18,090 DEV : loss 0.12150677293539047 - f1-score (micro avg)  0.7526
2022-09-28 11:15:18,371 BAD EPOCHS (no improvement): 4
2022-09-28 11:15:18,372 saving best model
2022-09-28 11:15:21,906 ----------------------------------------------------------------------------------------------------
2022-09-28 11:17:26,619 epoch 2 - iter 257/2578 - loss 0.39526631 - samples/sec: 8.24 - lr: 0.000003
2022-09-28 11:19:41,169 epoch 2 - iter 514/2578 - loss 0.36622459 - samples/sec: 7.64 - lr: 0.000003
2022-09-28 11:21:50,797 epoch 2 - iter 771/2578 - loss 0.35978900 - samples/sec: 7.93 - lr: 0.000003
2022-09-28 11:24:07,516 epoch 2 - iter 1028/2578 - loss 0.34244095 - samples/sec: 7.52 - lr: 0.000003
2022-09-28 11:26:22,071 epoch 2 - iter 1285/2578 - loss 0.33484205 - samples/sec: 7.64 - lr: 0.000004
2022-09-28 11:28:34,120 epoch 2 - iter 1542/2578 - loss 0.32986784 - samples/sec: 7.79 - lr: 0.000004
2022-09-28 11:30:52,166 epoch 2 - iter 1799/2578 - loss 0.32324865 - samples/sec: 7.45 - lr: 0.000004
2022-09-28 11:33:11,544 epoch 2 - iter 2056/2578 - loss 0.31755945 - samples/sec: 7.38 - lr: 0.000004
2022-09-28 11:35:31,476 epoch 2 - iter 2313/2578 - loss 0.31209604 - samples/sec: 7.35 - lr: 0.000005
2022-09-28 11:37:45,285 epoch 2 - iter 2570/2578 - loss 0.30811317 - samples/sec: 7.68 - lr: 0.000005
2022-09-28 11:37:48,818 ----------------------------------------------------------------------------------------------------
2022-09-28 11:37:48,818 EPOCH 2 done: loss 0.3080 - lr 0.000005
2022-09-28 11:39:50,325 Evaluating as a multi-label problem: False
2022-09-28 11:39:50,374 DEV : loss 0.04957897216081619 - f1-score (micro avg)  0.9148
2022-09-28 11:39:50,653 BAD EPOCHS (no improvement): 4
2022-09-28 11:39:50,670 saving best model
2022-09-28 11:40:05,703 ----------------------------------------------------------------------------------------------------
2022-09-28 11:42:22,675 epoch 3 - iter 257/2578 - loss 0.27957532 - samples/sec: 7.51 - lr: 0.000005
2022-09-28 11:44:41,548 epoch 3 - iter 514/2578 - loss 0.26879299 - samples/sec: 7.40 - lr: 0.000005
2022-09-28 11:46:56,937 epoch 3 - iter 771/2578 - loss 0.26775071 - samples/sec: 7.59 - lr: 0.000005
2022-09-28 11:49:09,426 epoch 3 - iter 1028/2578 - loss 0.26819220 - samples/sec: 7.76 - lr: 0.000005
2022-09-28 11:51:18,877 epoch 3 - iter 1285/2578 - loss 0.26737262 - samples/sec: 7.94 - lr: 0.000005
2022-09-28 11:53:40,072 epoch 3 - iter 1542/2578 - loss 0.26687252 - samples/sec: 7.28 - lr: 0.000005
2022-09-28 11:55:47,028 epoch 3 - iter 1799/2578 - loss 0.26706229 - samples/sec: 8.10 - lr: 0.000005
2022-09-28 11:57:56,573 epoch 3 - iter 2056/2578 - loss 0.26601339 - samples/sec: 7.94 - lr: 0.000005
2022-09-28 12:00:14,969 epoch 3 - iter 2313/2578 - loss 0.26433018 - samples/sec: 7.43 - lr: 0.000005
2022-09-28 12:02:24,788 epoch 3 - iter 2570/2578 - loss 0.26404315 - samples/sec: 7.92 - lr: 0.000005
2022-09-28 12:02:28,764 ----------------------------------------------------------------------------------------------------
2022-09-28 12:02:28,765 EPOCH 3 done: loss 0.2639 - lr 0.000005
2022-09-28 12:04:30,277 Evaluating as a multi-label problem: False
2022-09-28 12:04:30,326 DEV : loss 0.03794371336698532 - f1-score (micro avg)  0.935
2022-09-28 12:04:30,615 BAD EPOCHS (no improvement): 4
2022-09-28 12:04:30,617 saving best model
2022-09-28 12:04:45,667 ----------------------------------------------------------------------------------------------------
2022-09-28 12:07:02,158 epoch 4 - iter 257/2578 - loss 0.25411680 - samples/sec: 7.53 - lr: 0.000005
2022-09-28 12:09:17,656 epoch 4 - iter 514/2578 - loss 0.25492851 - samples/sec: 7.59 - lr: 0.000005
2022-09-28 12:11:27,184 epoch 4 - iter 771/2578 - loss 0.25790532 - samples/sec: 7.94 - lr: 0.000005
2022-09-28 12:13:38,916 epoch 4 - iter 1028/2578 - loss 0.25853191 - samples/sec: 7.80 - lr: 0.000005
2022-09-28 12:15:54,052 epoch 4 - iter 1285/2578 - loss 0.25703126 - samples/sec: 7.61 - lr: 0.000005
2022-09-28 12:18:16,822 epoch 4 - iter 1542/2578 - loss 0.25420920 - samples/sec: 7.20 - lr: 0.000005
2022-09-28 12:20:28,638 epoch 4 - iter 1799/2578 - loss 0.25579158 - samples/sec: 7.80 - lr: 0.000005
2022-09-28 12:22:40,619 epoch 4 - iter 2056/2578 - loss 0.25669289 - samples/sec: 7.79 - lr: 0.000005
2022-09-28 12:24:54,976 epoch 4 - iter 2313/2578 - loss 0.25682375 - samples/sec: 7.65 - lr: 0.000005
2022-09-28 12:27:14,817 epoch 4 - iter 2570/2578 - loss 0.25505690 - samples/sec: 7.35 - lr: 0.000005
2022-09-28 12:27:18,716 ----------------------------------------------------------------------------------------------------
2022-09-28 12:27:18,716 EPOCH 4 done: loss 0.2549 - lr 0.000005
2022-09-28 12:29:21,131 Evaluating as a multi-label problem: False
2022-09-28 12:29:21,183 DEV : loss 0.03845294937491417 - f1-score (micro avg)  0.9477
2022-09-28 12:29:21,464 BAD EPOCHS (no improvement): 4
2022-09-28 12:29:21,465 saving best model
2022-09-28 12:29:35,918 ----------------------------------------------------------------------------------------------------
2022-09-28 12:31:52,795 epoch 5 - iter 257/2578 - loss 0.23851751 - samples/sec: 7.51 - lr: 0.000005
2022-09-28 12:34:01,546 epoch 5 - iter 514/2578 - loss 0.23811530 - samples/sec: 7.99 - lr: 0.000005
2022-09-28 12:36:13,513 epoch 5 - iter 771/2578 - loss 0.24355660 - samples/sec: 7.79 - lr: 0.000005
2022-09-28 12:38:30,803 epoch 5 - iter 1028/2578 - loss 0.24539794 - samples/sec: 7.49 - lr: 0.000005
2022-09-28 12:40:42,894 epoch 5 - iter 1285/2578 - loss 0.24553667 - samples/sec: 7.78 - lr: 0.000005
2022-09-28 12:42:56,173 epoch 5 - iter 1542/2578 - loss 0.24722268 - samples/sec: 7.71 - lr: 0.000005
2022-09-28 12:45:15,557 epoch 5 - iter 1799/2578 - loss 0.24549560 - samples/sec: 7.38 - lr: 0.000005
2022-09-28 12:47:32,582 epoch 5 - iter 2056/2578 - loss 0.24578833 - samples/sec: 7.50 - lr: 0.000005
2022-09-28 12:49:49,472 epoch 5 - iter 2313/2578 - loss 0.24536737 - samples/sec: 7.51 - lr: 0.000005
2022-09-28 12:52:02,152 epoch 5 - iter 2570/2578 - loss 0.24557573 - samples/sec: 7.75 - lr: 0.000005
2022-09-28 12:52:05,766 ----------------------------------------------------------------------------------------------------
2022-09-28 12:52:05,766 EPOCH 5 done: loss 0.2461 - lr 0.000005
2022-09-28 12:54:07,409 Evaluating as a multi-label problem: False
2022-09-28 12:54:07,463 DEV : loss 0.03537509962916374 - f1-score (micro avg)  0.9629
2022-09-28 12:54:07,751 BAD EPOCHS (no improvement): 4
2022-09-28 12:54:07,752 saving best model
2022-09-28 12:54:22,731 ----------------------------------------------------------------------------------------------------
2022-09-28 12:56:33,542 epoch 6 - iter 257/2578 - loss 0.23208340 - samples/sec: 7.86 - lr: 0.000005
2022-09-28 12:58:44,864 epoch 6 - iter 514/2578 - loss 0.23503387 - samples/sec: 7.83 - lr: 0.000005
2022-09-28 13:01:06,517 epoch 6 - iter 771/2578 - loss 0.23984499 - samples/sec: 7.26 - lr: 0.000005
2022-09-28 13:03:20,983 epoch 6 - iter 1028/2578 - loss 0.24030334 - samples/sec: 7.65 - lr: 0.000005
2022-09-28 13:05:40,575 epoch 6 - iter 1285/2578 - loss 0.24060424 - samples/sec: 7.37 - lr: 0.000005
2022-09-28 13:07:58,853 epoch 6 - iter 1542/2578 - loss 0.24070157 - samples/sec: 7.44 - lr: 0.000005
2022-09-28 13:10:19,674 epoch 6 - iter 1799/2578 - loss 0.24029773 - samples/sec: 7.30 - lr: 0.000005
2022-09-28 13:12:28,650 epoch 6 - iter 2056/2578 - loss 0.23988858 - samples/sec: 7.97 - lr: 0.000005
2022-09-28 13:14:41,648 epoch 6 - iter 2313/2578 - loss 0.23981550 - samples/sec: 7.73 - lr: 0.000004
2022-09-28 13:16:49,062 epoch 6 - iter 2570/2578 - loss 0.24161589 - samples/sec: 8.07 - lr: 0.000004
2022-09-28 13:16:53,345 ----------------------------------------------------------------------------------------------------
2022-09-28 13:16:53,345 EPOCH 6 done: loss 0.2415 - lr 0.000004
2022-09-28 13:18:54,387 Evaluating as a multi-label problem: False
2022-09-28 13:18:54,434 DEV : loss 0.029858164489269257 - f1-score (micro avg)  0.9665
2022-09-28 13:18:54,721 BAD EPOCHS (no improvement): 4
2022-09-28 13:18:54,723 saving best model
2022-09-28 13:19:09,781 ----------------------------------------------------------------------------------------------------
2022-09-28 13:21:29,647 epoch 7 - iter 257/2578 - loss 0.22412492 - samples/sec: 7.35 - lr: 0.000004
2022-09-28 13:23:50,607 epoch 7 - iter 514/2578 - loss 0.23295641 - samples/sec: 7.29 - lr: 0.000004
2022-09-28 13:26:02,265 epoch 7 - iter 771/2578 - loss 0.23585677 - samples/sec: 7.81 - lr: 0.000004
2022-09-28 13:28:12,066 epoch 7 - iter 1028/2578 - loss 0.23520280 - samples/sec: 7.92 - lr: 0.000004
2022-09-28 13:30:27,159 epoch 7 - iter 1285/2578 - loss 0.23452685 - samples/sec: 7.61 - lr: 0.000004
2022-09-28 13:32:36,292 epoch 7 - iter 1542/2578 - loss 0.23294454 - samples/sec: 7.96 - lr: 0.000004
2022-09-28 13:34:50,709 epoch 7 - iter 1799/2578 - loss 0.23354223 - samples/sec: 7.65 - lr: 0.000004
2022-09-28 13:37:07,789 epoch 7 - iter 2056/2578 - loss 0.23319385 - samples/sec: 7.50 - lr: 0.000004
2022-09-28 13:39:23,116 epoch 7 - iter 2313/2578 - loss 0.23248527 - samples/sec: 7.60 - lr: 0.000004
2022-09-28 13:41:34,624 epoch 7 - iter 2570/2578 - loss 0.23323912 - samples/sec: 7.82 - lr: 0.000004
2022-09-28 13:41:37,564 ----------------------------------------------------------------------------------------------------
2022-09-28 13:41:37,564 EPOCH 7 done: loss 0.2332 - lr 0.000004
2022-09-28 13:43:38,946 Evaluating as a multi-label problem: False
2022-09-28 13:43:38,993 DEV : loss 0.03175165131688118 - f1-score (micro avg)  0.9667
2022-09-28 13:43:39,280 BAD EPOCHS (no improvement): 4
2022-09-28 13:43:39,281 saving best model
2022-09-28 13:43:54,433 ----------------------------------------------------------------------------------------------------
2022-09-28 13:46:13,489 epoch 8 - iter 257/2578 - loss 0.22393087 - samples/sec: 7.39 - lr: 0.000004
2022-09-28 13:48:31,829 epoch 8 - iter 514/2578 - loss 0.22827448 - samples/sec: 7.43 - lr: 0.000004
2022-09-28 13:50:50,784 epoch 8 - iter 771/2578 - loss 0.22742082 - samples/sec: 7.40 - lr: 0.000004
2022-09-28 13:53:05,454 epoch 8 - iter 1028/2578 - loss 0.22742971 - samples/sec: 7.63 - lr: 0.000004
2022-09-28 13:55:16,543 epoch 8 - iter 1285/2578 - loss 0.22824881 - samples/sec: 7.84 - lr: 0.000004
2022-09-28 13:57:27,615 epoch 8 - iter 1542/2578 - loss 0.23037993 - samples/sec: 7.84 - lr: 0.000004
2022-09-28 13:59:41,432 epoch 8 - iter 1799/2578 - loss 0.23053095 - samples/sec: 7.68 - lr: 0.000004
2022-09-28 14:01:52,482 epoch 8 - iter 2056/2578 - loss 0.23052049 - samples/sec: 7.85 - lr: 0.000004
2022-09-28 14:03:57,452 epoch 8 - iter 2313/2578 - loss 0.23113541 - samples/sec: 8.23 - lr: 0.000004
2022-09-28 14:06:18,351 epoch 8 - iter 2570/2578 - loss 0.23284482 - samples/sec: 7.30 - lr: 0.000004
2022-09-28 14:06:21,668 ----------------------------------------------------------------------------------------------------
2022-09-28 14:06:21,668 EPOCH 8 done: loss 0.2328 - lr 0.000004
2022-09-28 14:08:22,261 Evaluating as a multi-label problem: False
2022-09-28 14:08:22,309 DEV : loss 0.03157655522227287 - f1-score (micro avg)  0.9716
2022-09-28 14:08:22,596 BAD EPOCHS (no improvement): 4
2022-09-28 14:08:22,598 saving best model
2022-09-28 14:08:38,171 ----------------------------------------------------------------------------------------------------
2022-09-28 14:10:47,408 epoch 9 - iter 257/2578 - loss 0.22650514 - samples/sec: 7.96 - lr: 0.000004
2022-09-28 14:12:57,802 epoch 9 - iter 514/2578 - loss 0.22731394 - samples/sec: 7.88 - lr: 0.000004
2022-09-28 14:15:15,000 epoch 9 - iter 771/2578 - loss 0.23050485 - samples/sec: 7.49 - lr: 0.000004
2022-09-28 14:17:29,775 epoch 9 - iter 1028/2578 - loss 0.23128507 - samples/sec: 7.63 - lr: 0.000004
2022-09-28 14:19:47,105 epoch 9 - iter 1285/2578 - loss 0.23146238 - samples/sec: 7.49 - lr: 0.000004
2022-09-28 14:22:01,199 epoch 9 - iter 1542/2578 - loss 0.23215043 - samples/sec: 7.67 - lr: 0.000004
2022-09-28 14:24:16,003 epoch 9 - iter 1799/2578 - loss 0.23092558 - samples/sec: 7.63 - lr: 0.000004
2022-09-28 14:26:29,572 epoch 9 - iter 2056/2578 - loss 0.23137160 - samples/sec: 7.70 - lr: 0.000004
2022-09-28 14:28:42,288 epoch 9 - iter 2313/2578 - loss 0.23027062 - samples/sec: 7.75 - lr: 0.000004
2022-09-28 14:30:55,485 epoch 9 - iter 2570/2578 - loss 0.23029721 - samples/sec: 7.72 - lr: 0.000004
2022-09-28 14:31:00,601 ----------------------------------------------------------------------------------------------------
2022-09-28 14:31:00,601 EPOCH 9 done: loss 0.2302 - lr 0.000004
2022-09-28 14:33:01,971 Evaluating as a multi-label problem: False
2022-09-28 14:33:02,019 DEV : loss 0.03523799031972885 - f1-score (micro avg)  0.9688
2022-09-28 14:33:02,304 BAD EPOCHS (no improvement): 4
2022-09-28 14:33:02,306 ----------------------------------------------------------------------------------------------------
2022-09-28 14:35:13,450 epoch 10 - iter 257/2578 - loss 0.22347121 - samples/sec: 7.84 - lr: 0.000004
2022-09-28 14:37:29,356 epoch 10 - iter 514/2578 - loss 0.22485207 - samples/sec: 7.56 - lr: 0.000004
2022-09-28 14:39:43,820 epoch 10 - iter 771/2578 - loss 0.22594427 - samples/sec: 7.65 - lr: 0.000004
2022-09-28 14:41:59,090 epoch 10 - iter 1028/2578 - loss 0.22545214 - samples/sec: 7.60 - lr: 0.000004
2022-09-28 14:44:09,746 epoch 10 - iter 1285/2578 - loss 0.22651370 - samples/sec: 7.87 - lr: 0.000004
2022-09-28 14:46:21,909 epoch 10 - iter 1542/2578 - loss 0.22696782 - samples/sec: 7.78 - lr: 0.000004
2022-09-28 14:48:40,171 epoch 10 - iter 1799/2578 - loss 0.22820028 - samples/sec: 7.44 - lr: 0.000004
2022-09-28 14:50:57,974 epoch 10 - iter 2056/2578 - loss 0.22814961 - samples/sec: 7.46 - lr: 0.000004
2022-09-28 14:53:06,802 epoch 10 - iter 2313/2578 - loss 0.22883599 - samples/sec: 7.98 - lr: 0.000004
2022-09-28 14:55:23,751 epoch 10 - iter 2570/2578 - loss 0.22853754 - samples/sec: 7.51 - lr: 0.000004
2022-09-28 14:55:28,562 ----------------------------------------------------------------------------------------------------
2022-09-28 14:55:28,563 EPOCH 10 done: loss 0.2285 - lr 0.000004
2022-09-28 14:57:29,959 Evaluating as a multi-label problem: False
2022-09-28 14:57:30,011 DEV : loss 0.0337832011282444 - f1-score (micro avg)  0.9716
2022-09-28 14:57:30,303 BAD EPOCHS (no improvement): 4
2022-09-28 14:57:30,305 ----------------------------------------------------------------------------------------------------
2022-09-28 14:59:42,944 epoch 11 - iter 257/2578 - loss 0.24516582 - samples/sec: 7.75 - lr: 0.000004
2022-09-28 15:01:59,806 epoch 11 - iter 514/2578 - loss 0.23414175 - samples/sec: 7.51 - lr: 0.000004
2022-09-28 15:04:12,815 epoch 11 - iter 771/2578 - loss 0.23135370 - samples/sec: 7.73 - lr: 0.000004
2022-09-28 15:06:29,085 epoch 11 - iter 1028/2578 - loss 0.23232332 - samples/sec: 7.54 - lr: 0.000004
2022-09-28 15:08:36,218 epoch 11 - iter 1285/2578 - loss 0.23197356 - samples/sec: 8.09 - lr: 0.000004
2022-09-28 15:10:49,260 epoch 11 - iter 1542/2578 - loss 0.23136046 - samples/sec: 7.73 - lr: 0.000004
2022-09-28 15:13:05,626 epoch 11 - iter 1799/2578 - loss 0.23123874 - samples/sec: 7.54 - lr: 0.000004
2022-09-28 15:15:18,745 epoch 11 - iter 2056/2578 - loss 0.23018355 - samples/sec: 7.72 - lr: 0.000004
2022-09-28 15:17:40,837 epoch 11 - iter 2313/2578 - loss 0.23040594 - samples/sec: 7.24 - lr: 0.000004
2022-09-28 15:19:56,127 epoch 11 - iter 2570/2578 - loss 0.22957420 - samples/sec: 7.60 - lr: 0.000004
2022-09-28 15:19:59,646 ----------------------------------------------------------------------------------------------------
2022-09-28 15:19:59,646 EPOCH 11 done: loss 0.2296 - lr 0.000004
2022-09-28 15:22:00,528 Evaluating as a multi-label problem: False
2022-09-28 15:22:00,581 DEV : loss 0.031558964401483536 - f1-score (micro avg)  0.973
2022-09-28 15:22:00,885 BAD EPOCHS (no improvement): 4
2022-09-28 15:22:00,887 saving best model
2022-09-28 15:22:15,986 ----------------------------------------------------------------------------------------------------
2022-09-28 15:24:24,696 epoch 12 - iter 257/2578 - loss 0.22380408 - samples/sec: 7.99 - lr: 0.000004
2022-09-28 15:26:45,686 epoch 12 - iter 514/2578 - loss 0.23350198 - samples/sec: 7.29 - lr: 0.000004
2022-09-28 15:29:04,375 epoch 12 - iter 771/2578 - loss 0.23060498 - samples/sec: 7.41 - lr: 0.000004
2022-09-28 15:31:17,593 epoch 12 - iter 1028/2578 - loss 0.22778448 - samples/sec: 7.72 - lr: 0.000004
2022-09-28 15:33:27,467 epoch 12 - iter 1285/2578 - loss 0.22785064 - samples/sec: 7.92 - lr: 0.000004
2022-09-28 15:35:36,708 epoch 12 - iter 1542/2578 - loss 0.22653530 - samples/sec: 7.96 - lr: 0.000004
2022-09-28 15:37:47,820 epoch 12 - iter 1799/2578 - loss 0.22626584 - samples/sec: 7.84 - lr: 0.000004
2022-09-28 15:40:02,994 epoch 12 - iter 2056/2578 - loss 0.22626808 - samples/sec: 7.61 - lr: 0.000004
2022-09-28 15:42:17,368 epoch 12 - iter 2313/2578 - loss 0.22514672 - samples/sec: 7.65 - lr: 0.000004
2022-09-28 15:44:37,936 epoch 12 - iter 2570/2578 - loss 0.22428497 - samples/sec: 7.31 - lr: 0.000004
2022-09-28 15:44:41,497 ----------------------------------------------------------------------------------------------------
2022-09-28 15:44:41,497 EPOCH 12 done: loss 0.2243 - lr 0.000004
2022-09-28 15:46:42,788 Evaluating as a multi-label problem: False
2022-09-28 15:46:42,836 DEV : loss 0.03669183328747749 - f1-score (micro avg)  0.9716
2022-09-28 15:46:43,124 BAD EPOCHS (no improvement): 4
2022-09-28 15:46:43,126 ----------------------------------------------------------------------------------------------------
2022-09-28 15:48:46,570 epoch 13 - iter 257/2578 - loss 0.22256984 - samples/sec: 8.33 - lr: 0.000004
2022-09-28 15:51:01,043 epoch 13 - iter 514/2578 - loss 0.22058498 - samples/sec: 7.65 - lr: 0.000004
2022-09-28 15:53:20,334 epoch 13 - iter 771/2578 - loss 0.21799737 - samples/sec: 7.38 - lr: 0.000004
2022-09-28 15:55:39,859 epoch 13 - iter 1028/2578 - loss 0.21907999 - samples/sec: 7.37 - lr: 0.000004
2022-09-28 15:57:53,681 epoch 13 - iter 1285/2578 - loss 0.22120155 - samples/sec: 7.68 - lr: 0.000004
2022-09-28 16:00:06,158 epoch 13 - iter 1542/2578 - loss 0.22426032 - samples/sec: 7.76 - lr: 0.000004
2022-09-28 16:02:20,937 epoch 13 - iter 1799/2578 - loss 0.22392000 - samples/sec: 7.63 - lr: 0.000004
2022-09-28 16:04:32,310 epoch 13 - iter 2056/2578 - loss 0.22411394 - samples/sec: 7.83 - lr: 0.000004
2022-09-28 16:06:49,640 epoch 13 - iter 2313/2578 - loss 0.22344118 - samples/sec: 7.49 - lr: 0.000004
2022-09-28 16:09:06,713 epoch 13 - iter 2570/2578 - loss 0.22294996 - samples/sec: 7.50 - lr: 0.000004
2022-09-28 16:09:11,272 ----------------------------------------------------------------------------------------------------
2022-09-28 16:09:11,272 EPOCH 13 done: loss 0.2230 - lr 0.000004
2022-09-28 16:11:12,717 Evaluating as a multi-label problem: False
2022-09-28 16:11:12,769 DEV : loss 0.03831896930932999 - f1-score (micro avg)  0.9715
2022-09-28 16:11:13,060 BAD EPOCHS (no improvement): 4
2022-09-28 16:11:13,062 ----------------------------------------------------------------------------------------------------
2022-09-28 16:13:26,218 epoch 14 - iter 257/2578 - loss 0.22403506 - samples/sec: 7.72 - lr: 0.000004
2022-09-28 16:15:44,006 epoch 14 - iter 514/2578 - loss 0.22155469 - samples/sec: 7.46 - lr: 0.000004
2022-09-28 16:17:55,029 epoch 14 - iter 771/2578 - loss 0.22075816 - samples/sec: 7.85 - lr: 0.000004
2022-09-28 16:20:07,950 epoch 14 - iter 1028/2578 - loss 0.21970483 - samples/sec: 7.73 - lr: 0.000004
2022-09-28 16:22:24,332 epoch 14 - iter 1285/2578 - loss 0.21950557 - samples/sec: 7.54 - lr: 0.000003
2022-09-28 16:24:38,544 epoch 14 - iter 1542/2578 - loss 0.21999594 - samples/sec: 7.66 - lr: 0.000003
2022-09-28 16:26:55,242 epoch 14 - iter 1799/2578 - loss 0.21925681 - samples/sec: 7.52 - lr: 0.000003
2022-09-28 16:29:13,624 epoch 14 - iter 2056/2578 - loss 0.22017549 - samples/sec: 7.43 - lr: 0.000003
2022-09-28 16:31:24,479 epoch 14 - iter 2313/2578 - loss 0.21964547 - samples/sec: 7.86 - lr: 0.000003
2022-09-28 16:33:37,321 epoch 14 - iter 2570/2578 - loss 0.21958842 - samples/sec: 7.74 - lr: 0.000003
2022-09-28 16:33:41,819 ----------------------------------------------------------------------------------------------------
2022-09-28 16:33:41,820 EPOCH 14 done: loss 0.2196 - lr 0.000003
2022-09-28 16:35:45,453 Evaluating as a multi-label problem: False
2022-09-28 16:35:45,606 DEV : loss 0.034654535353183746 - f1-score (micro avg)  0.9728
2022-09-28 16:35:45,949 BAD EPOCHS (no improvement): 4
2022-09-28 16:35:45,951 ----------------------------------------------------------------------------------------------------
2022-09-28 16:38:01,792 epoch 15 - iter 257/2578 - loss 0.21904993 - samples/sec: 7.57 - lr: 0.000003
2022-09-28 16:40:10,057 epoch 15 - iter 514/2578 - loss 0.22268133 - samples/sec: 8.02 - lr: 0.000003
2022-09-28 16:42:33,771 epoch 15 - iter 771/2578 - loss 0.22147824 - samples/sec: 7.15 - lr: 0.000003
2022-09-28 16:44:47,249 epoch 15 - iter 1028/2578 - loss 0.22248105 - samples/sec: 7.70 - lr: 0.000003
2022-09-28 16:47:03,061 epoch 15 - iter 1285/2578 - loss 0.22187153 - samples/sec: 7.57 - lr: 0.000003
2022-09-28 16:49:21,215 epoch 15 - iter 1542/2578 - loss 0.22349792 - samples/sec: 7.44 - lr: 0.000003
2022-09-28 16:51:32,594 epoch 15 - iter 1799/2578 - loss 0.22165905 - samples/sec: 7.83 - lr: 0.000003
2022-09-28 16:53:49,638 epoch 15 - iter 2056/2578 - loss 0.22038473 - samples/sec: 7.50 - lr: 0.000003
2022-09-28 16:56:00,017 epoch 15 - iter 2313/2578 - loss 0.22015021 - samples/sec: 7.89 - lr: 0.000003
2022-09-28 16:58:08,952 epoch 15 - iter 2570/2578 - loss 0.22113980 - samples/sec: 7.97 - lr: 0.000003
2022-09-28 16:58:13,176 ----------------------------------------------------------------------------------------------------
2022-09-28 16:58:13,176 EPOCH 15 done: loss 0.2212 - lr 0.000003
2022-09-28 17:00:14,100 Evaluating as a multi-label problem: False
2022-09-28 17:00:14,150 DEV : loss 0.03484518453478813 - f1-score (micro avg)  0.9747
2022-09-28 17:00:14,435 BAD EPOCHS (no improvement): 4
2022-09-28 17:00:14,437 saving best model
2022-09-28 17:00:28,997 ----------------------------------------------------------------------------------------------------
2022-09-28 17:02:37,573 epoch 16 - iter 257/2578 - loss 0.22750059 - samples/sec: 8.00 - lr: 0.000003
2022-09-28 17:04:51,519 epoch 16 - iter 514/2578 - loss 0.22739885 - samples/sec: 7.68 - lr: 0.000003
2022-09-28 17:07:04,152 epoch 16 - iter 771/2578 - loss 0.22222290 - samples/sec: 7.75 - lr: 0.000003
2022-09-28 17:09:15,883 epoch 16 - iter 1028/2578 - loss 0.22203913 - samples/sec: 7.80 - lr: 0.000003
2022-09-28 17:11:34,324 epoch 16 - iter 1285/2578 - loss 0.22139423 - samples/sec: 7.43 - lr: 0.000003
2022-09-28 17:13:45,905 epoch 16 - iter 1542/2578 - loss 0.22085017 - samples/sec: 7.81 - lr: 0.000003
2022-09-28 17:16:02,555 epoch 16 - iter 1799/2578 - loss 0.22114328 - samples/sec: 7.52 - lr: 0.000003
2022-09-28 17:18:21,735 epoch 16 - iter 2056/2578 - loss 0.22062159 - samples/sec: 7.39 - lr: 0.000003
2022-09-28 17:20:38,856 epoch 16 - iter 2313/2578 - loss 0.21975437 - samples/sec: 7.50 - lr: 0.000003
2022-09-28 17:22:48,676 epoch 16 - iter 2570/2578 - loss 0.21990528 - samples/sec: 7.92 - lr: 0.000003
2022-09-28 17:22:51,676 ----------------------------------------------------------------------------------------------------
2022-09-28 17:22:51,677 EPOCH 16 done: loss 0.2198 - lr 0.000003
2022-09-28 17:24:53,091 Evaluating as a multi-label problem: False
2022-09-28 17:24:53,139 DEV : loss 0.03746265172958374 - f1-score (micro avg)  0.9734
2022-09-28 17:24:53,426 BAD EPOCHS (no improvement): 4
2022-09-28 17:24:53,428 ----------------------------------------------------------------------------------------------------
2022-09-28 17:27:04,285 epoch 17 - iter 257/2578 - loss 0.21913373 - samples/sec: 7.86 - lr: 0.000003
2022-09-28 17:29:19,216 epoch 17 - iter 514/2578 - loss 0.21762788 - samples/sec: 7.62 - lr: 0.000003
2022-09-28 17:31:30,075 epoch 17 - iter 771/2578 - loss 0.21822131 - samples/sec: 7.86 - lr: 0.000003
2022-09-28 17:33:41,848 epoch 17 - iter 1028/2578 - loss 0.21934000 - samples/sec: 7.80 - lr: 0.000003
2022-09-28 17:35:49,311 epoch 17 - iter 1285/2578 - loss 0.21802068 - samples/sec: 8.07 - lr: 0.000003
2022-09-28 17:38:08,634 epoch 17 - iter 1542/2578 - loss 0.21957067 - samples/sec: 7.38 - lr: 0.000003
2022-09-28 17:40:22,638 epoch 17 - iter 1799/2578 - loss 0.22026277 - samples/sec: 7.67 - lr: 0.000003
2022-09-28 17:42:43,165 epoch 17 - iter 2056/2578 - loss 0.21900997 - samples/sec: 7.32 - lr: 0.000003
2022-09-28 17:44:57,432 epoch 17 - iter 2313/2578 - loss 0.21928339 - samples/sec: 7.66 - lr: 0.000003
2022-09-28 17:47:14,962 epoch 17 - iter 2570/2578 - loss 0.21987681 - samples/sec: 7.48 - lr: 0.000003
2022-09-28 17:47:18,359 ----------------------------------------------------------------------------------------------------
2022-09-28 17:47:18,359 EPOCH 17 done: loss 0.2198 - lr 0.000003
2022-09-28 17:49:19,584 Evaluating as a multi-label problem: False
2022-09-28 17:49:19,631 DEV : loss 0.035356540232896805 - f1-score (micro avg)  0.9759
2022-09-28 17:49:19,922 BAD EPOCHS (no improvement): 4
2022-09-28 17:49:19,924 saving best model
2022-09-28 17:49:34,913 ----------------------------------------------------------------------------------------------------
2022-09-28 17:51:49,315 epoch 18 - iter 257/2578 - loss 0.21424616 - samples/sec: 7.65 - lr: 0.000003
2022-09-28 17:54:05,846 epoch 18 - iter 514/2578 - loss 0.21818941 - samples/sec: 7.53 - lr: 0.000003
2022-09-28 17:56:21,479 epoch 18 - iter 771/2578 - loss 0.21368805 - samples/sec: 7.58 - lr: 0.000003
2022-09-28 17:58:32,001 epoch 18 - iter 1028/2578 - loss 0.21416018 - samples/sec: 7.88 - lr: 0.000003
2022-09-28 18:00:50,192 epoch 18 - iter 1285/2578 - loss 0.21523233 - samples/sec: 7.44 - lr: 0.000003
2022-09-28 18:02:59,342 epoch 18 - iter 1542/2578 - loss 0.21552742 - samples/sec: 7.96 - lr: 0.000003
2022-09-28 18:05:14,961 epoch 18 - iter 1799/2578 - loss 0.21533975 - samples/sec: 7.58 - lr: 0.000003
2022-09-28 18:07:23,541 epoch 18 - iter 2056/2578 - loss 0.21547837 - samples/sec: 8.00 - lr: 0.000003
2022-09-28 18:09:39,751 epoch 18 - iter 2313/2578 - loss 0.21522820 - samples/sec: 7.55 - lr: 0.000003
2022-09-28 18:11:52,158 epoch 18 - iter 2570/2578 - loss 0.21594571 - samples/sec: 7.76 - lr: 0.000003
2022-09-28 18:11:55,473 ----------------------------------------------------------------------------------------------------
2022-09-28 18:11:55,473 EPOCH 18 done: loss 0.2161 - lr 0.000003
2022-09-28 18:13:56,755 Evaluating as a multi-label problem: False
2022-09-28 18:13:56,803 DEV : loss 0.039238568395376205 - f1-score (micro avg)  0.9725
2022-09-28 18:13:57,093 BAD EPOCHS (no improvement): 4
2022-09-28 18:13:57,095 ----------------------------------------------------------------------------------------------------
2022-09-28 18:16:11,930 epoch 19 - iter 257/2578 - loss 0.21001413 - samples/sec: 7.63 - lr: 0.000003
2022-09-28 18:18:22,831 epoch 19 - iter 514/2578 - loss 0.21271268 - samples/sec: 7.85 - lr: 0.000003
2022-09-28 18:20:36,840 epoch 19 - iter 771/2578 - loss 0.21218264 - samples/sec: 7.67 - lr: 0.000003
2022-09-28 18:22:49,451 epoch 19 - iter 1028/2578 - loss 0.21610153 - samples/sec: 7.75 - lr: 0.000003
2022-09-28 18:25:03,521 epoch 19 - iter 1285/2578 - loss 0.21643578 - samples/sec: 7.67 - lr: 0.000003
2022-09-28 18:27:07,062 epoch 19 - iter 1542/2578 - loss 0.21621841 - samples/sec: 8.32 - lr: 0.000003
2022-09-28 18:29:25,089 epoch 19 - iter 1799/2578 - loss 0.21847628 - samples/sec: 7.45 - lr: 0.000003
2022-09-28 18:31:44,213 epoch 19 - iter 2056/2578 - loss 0.21764891 - samples/sec: 7.39 - lr: 0.000003
2022-09-28 18:33:57,704 epoch 19 - iter 2313/2578 - loss 0.21657777 - samples/sec: 7.70 - lr: 0.000003
2022-09-28 18:36:13,480 epoch 19 - iter 2570/2578 - loss 0.21668837 - samples/sec: 7.57 - lr: 0.000003
2022-09-28 18:36:16,627 ----------------------------------------------------------------------------------------------------
2022-09-28 18:36:16,627 EPOCH 19 done: loss 0.2165 - lr 0.000003
2022-09-28 18:38:17,273 Evaluating as a multi-label problem: False
2022-09-28 18:38:17,323 DEV : loss 0.03863511607050896 - f1-score (micro avg)  0.9739
2022-09-28 18:38:17,612 BAD EPOCHS (no improvement): 4
2022-09-28 18:38:17,614 ----------------------------------------------------------------------------------------------------
2022-09-28 18:40:32,131 epoch 20 - iter 257/2578 - loss 0.21008883 - samples/sec: 7.64 - lr: 0.000003
2022-09-28 18:42:44,752 epoch 20 - iter 514/2578 - loss 0.21308090 - samples/sec: 7.75 - lr: 0.000003
2022-09-28 18:45:04,364 epoch 20 - iter 771/2578 - loss 0.21270366 - samples/sec: 7.36 - lr: 0.000003
2022-09-28 18:47:14,114 epoch 20 - iter 1028/2578 - loss 0.21208530 - samples/sec: 7.92 - lr: 0.000003
2022-09-28 18:49:26,445 epoch 20 - iter 1285/2578 - loss 0.21388776 - samples/sec: 7.77 - lr: 0.000003
2022-09-28 18:51:36,733 epoch 20 - iter 1542/2578 - loss 0.21450460 - samples/sec: 7.89 - lr: 0.000003
2022-09-28 18:53:46,445 epoch 20 - iter 1799/2578 - loss 0.21474937 - samples/sec: 7.93 - lr: 0.000003
2022-09-28 18:56:00,830 epoch 20 - iter 2056/2578 - loss 0.21517893 - samples/sec: 7.65 - lr: 0.000003
2022-09-28 18:58:22,727 epoch 20 - iter 2313/2578 - loss 0.21573047 - samples/sec: 7.25 - lr: 0.000003
2022-09-28 19:00:37,785 epoch 20 - iter 2570/2578 - loss 0.21486844 - samples/sec: 7.61 - lr: 0.000003
2022-09-28 19:00:42,727 ----------------------------------------------------------------------------------------------------
2022-09-28 19:00:42,728 EPOCH 20 done: loss 0.2150 - lr 0.000003
2022-09-28 19:02:43,597 Evaluating as a multi-label problem: False
2022-09-28 19:02:43,644 DEV : loss 0.03831285238265991 - f1-score (micro avg)  0.974
2022-09-28 19:02:43,932 BAD EPOCHS (no improvement): 4
2022-09-28 19:02:43,934 ----------------------------------------------------------------------------------------------------
2022-09-28 19:04:59,695 epoch 21 - iter 257/2578 - loss 0.22297362 - samples/sec: 7.57 - lr: 0.000003
2022-09-28 19:07:12,523 epoch 21 - iter 514/2578 - loss 0.21984029 - samples/sec: 7.74 - lr: 0.000003
2022-09-28 19:09:27,422 epoch 21 - iter 771/2578 - loss 0.21795894 - samples/sec: 7.62 - lr: 0.000003
2022-09-28 19:11:37,090 epoch 21 - iter 1028/2578 - loss 0.21529225 - samples/sec: 7.93 - lr: 0.000003
2022-09-28 19:13:52,920 epoch 21 - iter 1285/2578 - loss 0.21618067 - samples/sec: 7.57 - lr: 0.000003
2022-09-28 19:16:04,667 epoch 21 - iter 1542/2578 - loss 0.21632180 - samples/sec: 7.80 - lr: 0.000003
2022-09-28 19:18:20,662 epoch 21 - iter 1799/2578 - loss 0.21730921 - samples/sec: 7.56 - lr: 0.000003
2022-09-28 19:20:41,316 epoch 21 - iter 2056/2578 - loss 0.21680435 - samples/sec: 7.31 - lr: 0.000003
2022-09-28 19:22:55,742 epoch 21 - iter 2313/2578 - loss 0.21674454 - samples/sec: 7.65 - lr: 0.000003
2022-09-28 19:25:13,607 epoch 21 - iter 2570/2578 - loss 0.21613402 - samples/sec: 7.46 - lr: 0.000003
2022-09-28 19:25:17,123 ----------------------------------------------------------------------------------------------------
2022-09-28 19:25:17,123 EPOCH 21 done: loss 0.2160 - lr 0.000003
2022-09-28 19:27:18,248 Evaluating as a multi-label problem: False
2022-09-28 19:27:18,295 DEV : loss 0.03802865371108055 - f1-score (micro avg)  0.9756
2022-09-28 19:27:18,584 BAD EPOCHS (no improvement): 4
2022-09-28 19:27:18,585 ----------------------------------------------------------------------------------------------------
2022-09-28 19:29:31,691 epoch 22 - iter 257/2578 - loss 0.21136430 - samples/sec: 7.72 - lr: 0.000002
2022-09-28 19:31:41,965 epoch 22 - iter 514/2578 - loss 0.21499004 - samples/sec: 7.89 - lr: 0.000002
2022-09-28 19:33:53,527 epoch 22 - iter 771/2578 - loss 0.21272553 - samples/sec: 7.81 - lr: 0.000002
2022-09-28 19:36:05,340 epoch 22 - iter 1028/2578 - loss 0.21488672 - samples/sec: 7.80 - lr: 0.000002
2022-09-28 19:38:21,537 epoch 22 - iter 1285/2578 - loss 0.21545596 - samples/sec: 7.55 - lr: 0.000002
2022-09-28 19:40:38,746 epoch 22 - iter 1542/2578 - loss 0.21615078 - samples/sec: 7.49 - lr: 0.000002
2022-09-28 19:42:44,264 epoch 22 - iter 1799/2578 - loss 0.21681783 - samples/sec: 8.19 - lr: 0.000002
2022-09-28 19:45:04,866 epoch 22 - iter 2056/2578 - loss 0.21599027 - samples/sec: 7.31 - lr: 0.000002
2022-09-28 19:47:20,978 epoch 22 - iter 2313/2578 - loss 0.21685628 - samples/sec: 7.55 - lr: 0.000002
2022-09-28 19:49:39,938 epoch 22 - iter 2570/2578 - loss 0.21728873 - samples/sec: 7.40 - lr: 0.000002
2022-09-28 19:49:44,964 ----------------------------------------------------------------------------------------------------
2022-09-28 19:49:44,964 EPOCH 22 done: loss 0.2172 - lr 0.000002
2022-09-28 19:51:45,536 Evaluating as a multi-label problem: False
2022-09-28 19:51:45,584 DEV : loss 0.03865654021501541 - f1-score (micro avg)  0.9758
2022-09-28 19:51:45,874 BAD EPOCHS (no improvement): 4
2022-09-28 19:51:45,876 ----------------------------------------------------------------------------------------------------
2022-09-28 19:54:02,666 epoch 23 - iter 257/2578 - loss 0.21275340 - samples/sec: 7.52 - lr: 0.000002
2022-09-28 19:56:20,548 epoch 23 - iter 514/2578 - loss 0.20877212 - samples/sec: 7.46 - lr: 0.000002
2022-09-28 19:58:26,843 epoch 23 - iter 771/2578 - loss 0.21220979 - samples/sec: 8.14 - lr: 0.000002
2022-09-28 20:00:43,283 epoch 23 - iter 1028/2578 - loss 0.21455257 - samples/sec: 7.54 - lr: 0.000002
2022-09-28 20:02:58,801 epoch 23 - iter 1285/2578 - loss 0.21469433 - samples/sec: 7.59 - lr: 0.000002
2022-09-28 20:05:11,244 epoch 23 - iter 1542/2578 - loss 0.21684702 - samples/sec: 7.76 - lr: 0.000002
2022-09-28 20:07:25,192 epoch 23 - iter 1799/2578 - loss 0.21635060 - samples/sec: 7.68 - lr: 0.000002
2022-09-28 20:09:40,564 epoch 23 - iter 2056/2578 - loss 0.21634446 - samples/sec: 7.59 - lr: 0.000002
2022-09-28 20:11:55,936 epoch 23 - iter 2313/2578 - loss 0.21597674 - samples/sec: 7.59 - lr: 0.000002
2022-09-28 20:14:08,523 epoch 23 - iter 2570/2578 - loss 0.21597858 - samples/sec: 7.75 - lr: 0.000002
2022-09-28 20:14:15,844 ----------------------------------------------------------------------------------------------------
2022-09-28 20:14:15,844 EPOCH 23 done: loss 0.2161 - lr 0.000002
2022-09-28 20:16:17,060 Evaluating as a multi-label problem: False
2022-09-28 20:16:17,107 DEV : loss 0.03820624202489853 - f1-score (micro avg)  0.9752
2022-09-28 20:16:17,400 BAD EPOCHS (no improvement): 4
2022-09-28 20:16:17,401 ----------------------------------------------------------------------------------------------------
2022-09-28 20:18:32,760 epoch 24 - iter 257/2578 - loss 0.21860519 - samples/sec: 7.60 - lr: 0.000002
2022-09-28 20:20:46,121 epoch 24 - iter 514/2578 - loss 0.21497900 - samples/sec: 7.71 - lr: 0.000002
2022-09-28 20:22:59,543 epoch 24 - iter 771/2578 - loss 0.21281870 - samples/sec: 7.71 - lr: 0.000002
2022-09-28 20:25:10,871 epoch 24 - iter 1028/2578 - loss 0.21252771 - samples/sec: 7.83 - lr: 0.000002
2022-09-28 20:27:28,398 epoch 24 - iter 1285/2578 - loss 0.21329888 - samples/sec: 7.48 - lr: 0.000002
2022-09-28 20:29:41,098 epoch 24 - iter 1542/2578 - loss 0.21436197 - samples/sec: 7.75 - lr: 0.000002
2022-09-28 20:32:01,826 epoch 24 - iter 1799/2578 - loss 0.21507188 - samples/sec: 7.31 - lr: 0.000002
2022-09-28 20:34:14,410 epoch 24 - iter 2056/2578 - loss 0.21472606 - samples/sec: 7.75 - lr: 0.000002
2022-09-28 20:36:25,904 epoch 24 - iter 2313/2578 - loss 0.21379157 - samples/sec: 7.82 - lr: 0.000002
2022-09-28 20:38:40,702 epoch 24 - iter 2570/2578 - loss 0.21384087 - samples/sec: 7.63 - lr: 0.000002
2022-09-28 20:38:44,947 ----------------------------------------------------------------------------------------------------
2022-09-28 20:38:44,947 EPOCH 24 done: loss 0.2139 - lr 0.000002
2022-09-28 20:40:45,252 Evaluating as a multi-label problem: False
2022-09-28 20:40:45,299 DEV : loss 0.041703831404447556 - f1-score (micro avg)  0.9739
2022-09-28 20:40:45,585 BAD EPOCHS (no improvement): 4
2022-09-28 20:40:45,586 ----------------------------------------------------------------------------------------------------
2022-09-28 20:43:00,031 epoch 25 - iter 257/2578 - loss 0.22435915 - samples/sec: 7.65 - lr: 0.000002
2022-09-28 20:45:20,363 epoch 25 - iter 514/2578 - loss 0.21941376 - samples/sec: 7.33 - lr: 0.000002
2022-09-28 20:47:33,957 epoch 25 - iter 771/2578 - loss 0.21487429 - samples/sec: 7.70 - lr: 0.000002
2022-09-28 20:49:49,948 epoch 25 - iter 1028/2578 - loss 0.21350087 - samples/sec: 7.56 - lr: 0.000002
2022-09-28 20:52:05,519 epoch 25 - iter 1285/2578 - loss 0.21283800 - samples/sec: 7.58 - lr: 0.000002
2022-09-28 20:54:14,593 epoch 25 - iter 1542/2578 - loss 0.21185302 - samples/sec: 7.97 - lr: 0.000002
2022-09-28 20:56:29,714 epoch 25 - iter 1799/2578 - loss 0.21257946 - samples/sec: 7.61 - lr: 0.000002
2022-09-28 20:58:45,560 epoch 25 - iter 2056/2578 - loss 0.21217307 - samples/sec: 7.57 - lr: 0.000002
2022-09-28 21:00:57,510 epoch 25 - iter 2313/2578 - loss 0.21223619 - samples/sec: 7.79 - lr: 0.000002
2022-09-28 21:03:06,093 epoch 25 - iter 2570/2578 - loss 0.21252349 - samples/sec: 8.00 - lr: 0.000002
2022-09-28 21:03:10,859 ----------------------------------------------------------------------------------------------------
2022-09-28 21:03:10,860 EPOCH 25 done: loss 0.2126 - lr 0.000002
2022-09-28 21:05:11,801 Evaluating as a multi-label problem: False
2022-09-28 21:05:11,848 DEV : loss 0.03916212171316147 - f1-score (micro avg)  0.9749
2022-09-28 21:05:12,134 BAD EPOCHS (no improvement): 4
2022-09-28 21:05:12,136 ----------------------------------------------------------------------------------------------------
2022-09-28 21:07:19,227 epoch 26 - iter 257/2578 - loss 0.21301763 - samples/sec: 8.09 - lr: 0.000002
2022-09-28 21:09:41,256 epoch 26 - iter 514/2578 - loss 0.21172104 - samples/sec: 7.24 - lr: 0.000002
2022-09-28 21:11:55,056 epoch 26 - iter 771/2578 - loss 0.21264189 - samples/sec: 7.68 - lr: 0.000002
2022-09-28 21:14:06,888 epoch 26 - iter 1028/2578 - loss 0.21255567 - samples/sec: 7.80 - lr: 0.000002
2022-09-28 21:16:18,591 epoch 26 - iter 1285/2578 - loss 0.21381792 - samples/sec: 7.81 - lr: 0.000002
2022-09-28 21:18:36,648 epoch 26 - iter 1542/2578 - loss 0.21423645 - samples/sec: 7.45 - lr: 0.000002
2022-09-28 21:20:52,683 epoch 26 - iter 1799/2578 - loss 0.21375563 - samples/sec: 7.56 - lr: 0.000002
2022-09-28 21:23:09,361 epoch 26 - iter 2056/2578 - loss 0.21331455 - samples/sec: 7.52 - lr: 0.000002
2022-09-28 21:25:21,059 epoch 26 - iter 2313/2578 - loss 0.21349826 - samples/sec: 7.81 - lr: 0.000002
2022-09-28 21:27:33,391 epoch 26 - iter 2570/2578 - loss 0.21311790 - samples/sec: 7.77 - lr: 0.000002
2022-09-28 21:27:37,628 ----------------------------------------------------------------------------------------------------
2022-09-28 21:27:37,628 EPOCH 26 done: loss 0.2130 - lr 0.000002
2022-09-28 21:29:37,910 Evaluating as a multi-label problem: False
2022-09-28 21:29:37,957 DEV : loss 0.03943811357021332 - f1-score (micro avg)  0.9763
2022-09-28 21:29:38,246 BAD EPOCHS (no improvement): 4
2022-09-28 21:29:38,247 saving best model
2022-09-28 21:29:53,959 ----------------------------------------------------------------------------------------------------
2022-09-28 21:31:58,724 epoch 27 - iter 257/2578 - loss 0.21015076 - samples/sec: 8.24 - lr: 0.000002
2022-09-28 21:34:12,908 epoch 27 - iter 514/2578 - loss 0.20928686 - samples/sec: 7.66 - lr: 0.000002
2022-09-28 21:36:34,158 epoch 27 - iter 771/2578 - loss 0.21121951 - samples/sec: 7.28 - lr: 0.000002
2022-09-28 21:38:50,668 epoch 27 - iter 1028/2578 - loss 0.21393824 - samples/sec: 7.53 - lr: 0.000002
2022-09-28 21:41:03,167 epoch 27 - iter 1285/2578 - loss 0.21128641 - samples/sec: 7.76 - lr: 0.000002
2022-09-28 21:43:14,423 epoch 27 - iter 1542/2578 - loss 0.21179037 - samples/sec: 7.83 - lr: 0.000002
2022-09-28 21:45:26,224 epoch 27 - iter 1799/2578 - loss 0.21250147 - samples/sec: 7.80 - lr: 0.000002
2022-09-28 21:47:47,910 epoch 27 - iter 2056/2578 - loss 0.21240811 - samples/sec: 7.26 - lr: 0.000002
2022-09-28 21:49:54,130 epoch 27 - iter 2313/2578 - loss 0.21197010 - samples/sec: 8.15 - lr: 0.000002
2022-09-28 21:52:14,144 epoch 27 - iter 2570/2578 - loss 0.21127338 - samples/sec: 7.34 - lr: 0.000002
2022-09-28 21:52:19,618 ----------------------------------------------------------------------------------------------------
2022-09-28 21:52:19,618 EPOCH 27 done: loss 0.2112 - lr 0.000002
2022-09-28 21:54:20,591 Evaluating as a multi-label problem: False
2022-09-28 21:54:20,638 DEV : loss 0.04159776121377945 - f1-score (micro avg)  0.9747
2022-09-28 21:54:20,898 BAD EPOCHS (no improvement): 4
2022-09-28 21:54:20,928 ----------------------------------------------------------------------------------------------------
2022-09-28 21:56:30,980 epoch 28 - iter 257/2578 - loss 0.20671199 - samples/sec: 7.91 - lr: 0.000002
2022-09-28 21:58:44,005 epoch 28 - iter 514/2578 - loss 0.21491208 - samples/sec: 7.73 - lr: 0.000002
2022-09-28 22:01:02,203 epoch 28 - iter 771/2578 - loss 0.21635958 - samples/sec: 7.44 - lr: 0.000002
2022-09-28 22:03:24,114 epoch 28 - iter 1028/2578 - loss 0.21495359 - samples/sec: 7.24 - lr: 0.000002
2022-09-28 22:05:36,764 epoch 28 - iter 1285/2578 - loss 0.21456944 - samples/sec: 7.75 - lr: 0.000002
2022-09-28 22:07:56,944 epoch 28 - iter 1542/2578 - loss 0.21327482 - samples/sec: 7.33 - lr: 0.000002
2022-09-28 22:10:04,690 epoch 28 - iter 1799/2578 - loss 0.21306161 - samples/sec: 8.05 - lr: 0.000002
2022-09-28 22:12:11,093 epoch 28 - iter 2056/2578 - loss 0.21464583 - samples/sec: 8.13 - lr: 0.000002
2022-09-28 22:14:25,082 epoch 28 - iter 2313/2578 - loss 0.21542489 - samples/sec: 7.67 - lr: 0.000002
2022-09-28 22:16:41,021 epoch 28 - iter 2570/2578 - loss 0.21499269 - samples/sec: 7.56 - lr: 0.000002
2022-09-28 22:16:45,471 ----------------------------------------------------------------------------------------------------
2022-09-28 22:16:45,472 EPOCH 28 done: loss 0.2151 - lr 0.000002
2022-09-28 22:18:46,475 Evaluating as a multi-label problem: False
2022-09-28 22:18:46,523 DEV : loss 0.04089448228478432 - f1-score (micro avg)  0.9758
2022-09-28 22:18:46,814 BAD EPOCHS (no improvement): 4
2022-09-28 22:18:46,815 ----------------------------------------------------------------------------------------------------
2022-09-28 22:21:02,633 epoch 29 - iter 257/2578 - loss 0.20939731 - samples/sec: 7.57 - lr: 0.000002
2022-09-28 22:23:15,728 epoch 29 - iter 514/2578 - loss 0.21101174 - samples/sec: 7.72 - lr: 0.000002
2022-09-28 22:25:32,706 epoch 29 - iter 771/2578 - loss 0.21457330 - samples/sec: 7.51 - lr: 0.000002
2022-09-28 22:27:47,143 epoch 29 - iter 1028/2578 - loss 0.21272473 - samples/sec: 7.65 - lr: 0.000002
2022-09-28 22:30:01,858 epoch 29 - iter 1285/2578 - loss 0.21239994 - samples/sec: 7.63 - lr: 0.000002
2022-09-28 22:32:18,306 epoch 29 - iter 1542/2578 - loss 0.21307508 - samples/sec: 7.53 - lr: 0.000002
2022-09-28 22:34:32,664 epoch 29 - iter 1799/2578 - loss 0.21383664 - samples/sec: 7.65 - lr: 0.000001
2022-09-28 22:36:43,027 epoch 29 - iter 2056/2578 - loss 0.21407633 - samples/sec: 7.89 - lr: 0.000001
2022-09-28 22:38:51,327 epoch 29 - iter 2313/2578 - loss 0.21406808 - samples/sec: 8.01 - lr: 0.000001
2022-09-28 22:41:06,445 epoch 29 - iter 2570/2578 - loss 0.21356886 - samples/sec: 7.61 - lr: 0.000001
2022-09-28 22:41:10,649 ----------------------------------------------------------------------------------------------------
2022-09-28 22:41:10,650 EPOCH 29 done: loss 0.2135 - lr 0.000001
2022-09-28 22:43:10,949 Evaluating as a multi-label problem: False
2022-09-28 22:43:10,996 DEV : loss 0.04022705927491188 - f1-score (micro avg)  0.9746
2022-09-28 22:43:11,284 BAD EPOCHS (no improvement): 4
2022-09-28 22:43:11,286 ----------------------------------------------------------------------------------------------------
2022-09-28 22:45:16,275 epoch 30 - iter 257/2578 - loss 0.20649919 - samples/sec: 8.23 - lr: 0.000001
2022-09-28 22:47:25,754 epoch 30 - iter 514/2578 - loss 0.20815993 - samples/sec: 7.94 - lr: 0.000001
2022-09-28 22:49:49,152 epoch 30 - iter 771/2578 - loss 0.20562469 - samples/sec: 7.17 - lr: 0.000001
2022-09-28 22:52:01,947 epoch 30 - iter 1028/2578 - loss 0.20737844 - samples/sec: 7.74 - lr: 0.000001
2022-09-28 22:54:13,775 epoch 30 - iter 1285/2578 - loss 0.20793395 - samples/sec: 7.80 - lr: 0.000001
2022-09-28 22:56:27,836 epoch 30 - iter 1542/2578 - loss 0.20849109 - samples/sec: 7.67 - lr: 0.000001
2022-09-28 22:58:42,683 epoch 30 - iter 1799/2578 - loss 0.20877261 - samples/sec: 7.62 - lr: 0.000001
2022-09-28 23:00:53,420 epoch 30 - iter 2056/2578 - loss 0.20914366 - samples/sec: 7.86 - lr: 0.000001
2022-09-28 23:03:11,913 epoch 30 - iter 2313/2578 - loss 0.20922684 - samples/sec: 7.42 - lr: 0.000001
2022-09-28 23:05:28,692 epoch 30 - iter 2570/2578 - loss 0.20972141 - samples/sec: 7.52 - lr: 0.000001
2022-09-28 23:05:32,244 ----------------------------------------------------------------------------------------------------
2022-09-28 23:05:32,244 EPOCH 30 done: loss 0.2097 - lr 0.000001
2022-09-28 23:07:33,279 Evaluating as a multi-label problem: False
2022-09-28 23:07:33,326 DEV : loss 0.03980273753404617 - f1-score (micro avg)  0.976
2022-09-28 23:07:33,613 BAD EPOCHS (no improvement): 4
2022-09-28 23:07:33,615 ----------------------------------------------------------------------------------------------------
2022-09-28 23:09:33,430 epoch 31 - iter 257/2578 - loss 0.20974372 - samples/sec: 8.58 - lr: 0.000001
2022-09-28 23:11:46,937 epoch 31 - iter 514/2578 - loss 0.21764200 - samples/sec: 7.70 - lr: 0.000001
2022-09-28 23:13:54,369 epoch 31 - iter 771/2578 - loss 0.21563287 - samples/sec: 8.07 - lr: 0.000001
2022-09-28 23:16:16,171 epoch 31 - iter 1028/2578 - loss 0.21901389 - samples/sec: 7.25 - lr: 0.000001
2022-09-28 23:18:32,717 epoch 31 - iter 1285/2578 - loss 0.21646866 - samples/sec: 7.53 - lr: 0.000001
2022-09-28 23:20:49,854 epoch 31 - iter 1542/2578 - loss 0.21519322 - samples/sec: 7.50 - lr: 0.000001
2022-09-28 23:23:13,029 epoch 31 - iter 1799/2578 - loss 0.21579447 - samples/sec: 7.18 - lr: 0.000001
2022-09-28 23:25:33,446 epoch 31 - iter 2056/2578 - loss 0.21402670 - samples/sec: 7.32 - lr: 0.000001
2022-09-28 23:27:40,128 epoch 31 - iter 2313/2578 - loss 0.21337056 - samples/sec: 8.12 - lr: 0.000001
2022-09-28 23:29:52,231 epoch 31 - iter 2570/2578 - loss 0.21348632 - samples/sec: 7.78 - lr: 0.000001
2022-09-28 23:29:56,159 ----------------------------------------------------------------------------------------------------
2022-09-28 23:29:56,159 EPOCH 31 done: loss 0.2134 - lr 0.000001
2022-09-28 23:31:57,130 Evaluating as a multi-label problem: False
2022-09-28 23:31:57,177 DEV : loss 0.040859024971723557 - f1-score (micro avg)  0.9755
2022-09-28 23:31:57,466 BAD EPOCHS (no improvement): 4
2022-09-28 23:31:57,467 ----------------------------------------------------------------------------------------------------
2022-09-28 23:34:14,135 epoch 32 - iter 257/2578 - loss 0.21083872 - samples/sec: 7.52 - lr: 0.000001
2022-09-28 23:36:22,528 epoch 32 - iter 514/2578 - loss 0.21040650 - samples/sec: 8.01 - lr: 0.000001
2022-09-28 23:38:34,554 epoch 32 - iter 771/2578 - loss 0.21145261 - samples/sec: 7.79 - lr: 0.000001
2022-09-28 23:40:44,561 epoch 32 - iter 1028/2578 - loss 0.21127004 - samples/sec: 7.91 - lr: 0.000001
2022-09-28 23:42:59,486 epoch 32 - iter 1285/2578 - loss 0.21293353 - samples/sec: 7.62 - lr: 0.000001
2022-09-28 23:45:15,502 epoch 32 - iter 1542/2578 - loss 0.21507484 - samples/sec: 7.56 - lr: 0.000001
2022-09-28 23:47:25,146 epoch 32 - iter 1799/2578 - loss 0.21574823 - samples/sec: 7.93 - lr: 0.000001
2022-09-28 23:49:41,746 epoch 32 - iter 2056/2578 - loss 0.21442183 - samples/sec: 7.53 - lr: 0.000001
2022-09-28 23:51:59,289 epoch 32 - iter 2313/2578 - loss 0.21279723 - samples/sec: 7.47 - lr: 0.000001
2022-09-28 23:54:16,155 epoch 32 - iter 2570/2578 - loss 0.21198457 - samples/sec: 7.51 - lr: 0.000001
2022-09-28 23:54:19,863 ----------------------------------------------------------------------------------------------------
2022-09-28 23:54:19,863 EPOCH 32 done: loss 0.2121 - lr 0.000001
2022-09-28 23:56:20,084 Evaluating as a multi-label problem: False
2022-09-28 23:56:20,131 DEV : loss 0.04090125858783722 - f1-score (micro avg)  0.976
2022-09-28 23:56:20,419 BAD EPOCHS (no improvement): 4
2022-09-28 23:56:20,420 ----------------------------------------------------------------------------------------------------
2022-09-28 23:58:36,512 epoch 33 - iter 257/2578 - loss 0.21358547 - samples/sec: 7.55 - lr: 0.000001
2022-09-29 00:00:48,356 epoch 33 - iter 514/2578 - loss 0.21127060 - samples/sec: 7.80 - lr: 0.000001
2022-09-29 00:03:01,832 epoch 33 - iter 771/2578 - loss 0.21063775 - samples/sec: 7.70 - lr: 0.000001
2022-09-29 00:05:13,414 epoch 33 - iter 1028/2578 - loss 0.21226919 - samples/sec: 7.81 - lr: 0.000001
2022-09-29 00:07:23,453 epoch 33 - iter 1285/2578 - loss 0.21350285 - samples/sec: 7.91 - lr: 0.000001
2022-09-29 00:09:44,991 epoch 33 - iter 1542/2578 - loss 0.21327367 - samples/sec: 7.26 - lr: 0.000001
2022-09-29 00:11:58,332 epoch 33 - iter 1799/2578 - loss 0.21399781 - samples/sec: 7.71 - lr: 0.000001
2022-09-29 00:14:14,355 epoch 33 - iter 2056/2578 - loss 0.21405407 - samples/sec: 7.56 - lr: 0.000001
2022-09-29 00:16:25,024 epoch 33 - iter 2313/2578 - loss 0.21402250 - samples/sec: 7.87 - lr: 0.000001
2022-09-29 00:18:42,098 epoch 33 - iter 2570/2578 - loss 0.21277255 - samples/sec: 7.50 - lr: 0.000001
2022-09-29 00:18:46,879 ----------------------------------------------------------------------------------------------------
2022-09-29 00:18:46,879 EPOCH 33 done: loss 0.2127 - lr 0.000001
2022-09-29 00:20:47,914 Evaluating as a multi-label problem: False
2022-09-29 00:20:47,961 DEV : loss 0.03889509662985802 - f1-score (micro avg)  0.9761
2022-09-29 00:20:48,254 BAD EPOCHS (no improvement): 4
2022-09-29 00:20:48,255 ----------------------------------------------------------------------------------------------------
2022-09-29 00:22:54,797 epoch 34 - iter 257/2578 - loss 0.20985187 - samples/sec: 8.12 - lr: 0.000001
2022-09-29 00:25:05,724 epoch 34 - iter 514/2578 - loss 0.20782667 - samples/sec: 7.85 - lr: 0.000001
2022-09-29 00:27:16,816 epoch 34 - iter 771/2578 - loss 0.20734043 - samples/sec: 7.84 - lr: 0.000001
2022-09-29 00:29:45,282 epoch 34 - iter 1028/2578 - loss 0.20907243 - samples/sec: 6.92 - lr: 0.000001
2022-09-29 00:31:52,465 epoch 34 - iter 1285/2578 - loss 0.21051588 - samples/sec: 8.08 - lr: 0.000001
2022-09-29 00:34:05,637 epoch 34 - iter 1542/2578 - loss 0.21123530 - samples/sec: 7.72 - lr: 0.000001
2022-09-29 00:36:25,520 epoch 34 - iter 1799/2578 - loss 0.21043861 - samples/sec: 7.35 - lr: 0.000001
2022-09-29 00:38:37,910 epoch 34 - iter 2056/2578 - loss 0.21092090 - samples/sec: 7.77 - lr: 0.000001
2022-09-29 00:40:52,765 epoch 34 - iter 2313/2578 - loss 0.21005358 - samples/sec: 7.62 - lr: 0.000001
2022-09-29 00:43:06,933 epoch 34 - iter 2570/2578 - loss 0.21008882 - samples/sec: 7.66 - lr: 0.000001
2022-09-29 00:43:10,256 ----------------------------------------------------------------------------------------------------
2022-09-29 00:43:10,256 EPOCH 34 done: loss 0.2101 - lr 0.000001
2022-09-29 00:45:10,580 Evaluating as a multi-label problem: False
2022-09-29 00:45:10,627 DEV : loss 0.04011530056595802 - f1-score (micro avg)  0.9757
2022-09-29 00:45:10,914 BAD EPOCHS (no improvement): 4
2022-09-29 00:45:10,915 ----------------------------------------------------------------------------------------------------
2022-09-29 00:47:24,697 epoch 35 - iter 257/2578 - loss 0.20902098 - samples/sec: 7.69 - lr: 0.000001
2022-09-29 00:49:38,056 epoch 35 - iter 514/2578 - loss 0.21364679 - samples/sec: 7.71 - lr: 0.000001
2022-09-29 00:51:56,121 epoch 35 - iter 771/2578 - loss 0.21405937 - samples/sec: 7.45 - lr: 0.000001
2022-09-29 00:54:10,947 epoch 35 - iter 1028/2578 - loss 0.21449950 - samples/sec: 7.63 - lr: 0.000001
2022-09-29 00:56:26,048 epoch 35 - iter 1285/2578 - loss 0.21523314 - samples/sec: 7.61 - lr: 0.000001
2022-09-29 00:58:45,639 epoch 35 - iter 1542/2578 - loss 0.21486642 - samples/sec: 7.37 - lr: 0.000001
2022-09-29 01:00:59,867 epoch 35 - iter 1799/2578 - loss 0.21451314 - samples/sec: 7.66 - lr: 0.000001
2022-09-29 01:03:09,426 epoch 35 - iter 2056/2578 - loss 0.21370836 - samples/sec: 7.94 - lr: 0.000001
2022-09-29 01:05:20,270 epoch 35 - iter 2313/2578 - loss 0.21397137 - samples/sec: 7.86 - lr: 0.000001
2022-09-29 01:07:31,639 epoch 35 - iter 2570/2578 - loss 0.21421860 - samples/sec: 7.83 - lr: 0.000001
2022-09-29 01:07:35,185 ----------------------------------------------------------------------------------------------------
2022-09-29 01:07:35,185 EPOCH 35 done: loss 0.2141 - lr 0.000001
2022-09-29 01:09:36,176 Evaluating as a multi-label problem: False
2022-09-29 01:09:36,223 DEV : loss 0.04062245786190033 - f1-score (micro avg)  0.9762
2022-09-29 01:09:36,509 BAD EPOCHS (no improvement): 4
2022-09-29 01:09:36,510 ----------------------------------------------------------------------------------------------------
2022-09-29 01:11:49,090 epoch 36 - iter 257/2578 - loss 0.21258645 - samples/sec: 7.75 - lr: 0.000001
2022-09-29 01:14:06,711 epoch 36 - iter 514/2578 - loss 0.21766666 - samples/sec: 7.47 - lr: 0.000001
2022-09-29 01:16:25,405 epoch 36 - iter 771/2578 - loss 0.21408118 - samples/sec: 7.41 - lr: 0.000001
2022-09-29 01:18:37,833 epoch 36 - iter 1028/2578 - loss 0.21473278 - samples/sec: 7.76 - lr: 0.000001
2022-09-29 01:20:47,616 epoch 36 - iter 1285/2578 - loss 0.21201448 - samples/sec: 7.92 - lr: 0.000001
2022-09-29 01:23:00,424 epoch 36 - iter 1542/2578 - loss 0.21088434 - samples/sec: 7.74 - lr: 0.000001
2022-09-29 01:25:13,927 epoch 36 - iter 1799/2578 - loss 0.21176487 - samples/sec: 7.70 - lr: 0.000001
2022-09-29 01:27:27,354 epoch 36 - iter 2056/2578 - loss 0.21250576 - samples/sec: 7.71 - lr: 0.000001
2022-09-29 01:29:40,689 epoch 36 - iter 2313/2578 - loss 0.21198193 - samples/sec: 7.71 - lr: 0.000001
2022-09-29 01:31:57,576 epoch 36 - iter 2570/2578 - loss 0.21181412 - samples/sec: 7.51 - lr: 0.000001
2022-09-29 01:32:02,438 ----------------------------------------------------------------------------------------------------
2022-09-29 01:32:02,438 EPOCH 36 done: loss 0.2119 - lr 0.000001
2022-09-29 01:34:03,493 Evaluating as a multi-label problem: False
2022-09-29 01:34:03,540 DEV : loss 0.04088783264160156 - f1-score (micro avg)  0.9761
2022-09-29 01:34:03,828 BAD EPOCHS (no improvement): 4
2022-09-29 01:34:03,829 ----------------------------------------------------------------------------------------------------
2022-09-29 01:36:24,126 epoch 37 - iter 257/2578 - loss 0.21220813 - samples/sec: 7.33 - lr: 0.000001
2022-09-29 01:38:40,801 epoch 37 - iter 514/2578 - loss 0.21330027 - samples/sec: 7.52 - lr: 0.000001
2022-09-29 01:41:00,568 epoch 37 - iter 771/2578 - loss 0.21097113 - samples/sec: 7.36 - lr: 0.000000
2022-09-29 01:43:10,376 epoch 37 - iter 1028/2578 - loss 0.21094519 - samples/sec: 7.92 - lr: 0.000000
2022-09-29 01:45:20,453 epoch 37 - iter 1285/2578 - loss 0.21071344 - samples/sec: 7.90 - lr: 0.000000
2022-09-29 01:47:32,143 epoch 37 - iter 1542/2578 - loss 0.21164422 - samples/sec: 7.81 - lr: 0.000000
2022-09-29 01:49:42,002 epoch 37 - iter 1799/2578 - loss 0.21313993 - samples/sec: 7.92 - lr: 0.000000
2022-09-29 01:51:59,349 epoch 37 - iter 2056/2578 - loss 0.21338767 - samples/sec: 7.49 - lr: 0.000000
2022-09-29 01:54:13,894 epoch 37 - iter 2313/2578 - loss 0.21294069 - samples/sec: 7.64 - lr: 0.000000
2022-09-29 01:56:27,021 epoch 37 - iter 2570/2578 - loss 0.21291874 - samples/sec: 7.72 - lr: 0.000000
2022-09-29 01:56:30,608 ----------------------------------------------------------------------------------------------------
2022-09-29 01:56:30,608 EPOCH 37 done: loss 0.2129 - lr 0.000000
2022-09-29 01:58:30,792 Evaluating as a multi-label problem: False
2022-09-29 01:58:30,839 DEV : loss 0.041478414088487625 - f1-score (micro avg)  0.9752
2022-09-29 01:58:31,127 BAD EPOCHS (no improvement): 4
2022-09-29 01:58:31,128 ----------------------------------------------------------------------------------------------------
2022-09-29 02:00:43,317 epoch 38 - iter 257/2578 - loss 0.20667139 - samples/sec: 7.78 - lr: 0.000000
2022-09-29 02:02:54,428 epoch 38 - iter 514/2578 - loss 0.21344325 - samples/sec: 7.84 - lr: 0.000000
2022-09-29 02:05:11,449 epoch 38 - iter 771/2578 - loss 0.21195358 - samples/sec: 7.50 - lr: 0.000000
2022-09-29 02:07:29,152 epoch 38 - iter 1028/2578 - loss 0.21167597 - samples/sec: 7.47 - lr: 0.000000
2022-09-29 02:09:44,216 epoch 38 - iter 1285/2578 - loss 0.20918238 - samples/sec: 7.61 - lr: 0.000000
2022-09-29 02:12:00,786 epoch 38 - iter 1542/2578 - loss 0.20857070 - samples/sec: 7.53 - lr: 0.000000
2022-09-29 02:14:13,897 epoch 38 - iter 1799/2578 - loss 0.20915733 - samples/sec: 7.72 - lr: 0.000000
2022-09-29 02:16:26,378 epoch 38 - iter 2056/2578 - loss 0.21052216 - samples/sec: 7.76 - lr: 0.000000
2022-09-29 02:18:39,048 epoch 38 - iter 2313/2578 - loss 0.21167039 - samples/sec: 7.75 - lr: 0.000000
2022-09-29 02:20:55,675 epoch 38 - iter 2570/2578 - loss 0.21121633 - samples/sec: 7.52 - lr: 0.000000
2022-09-29 02:20:59,217 ----------------------------------------------------------------------------------------------------
2022-09-29 02:20:59,217 EPOCH 38 done: loss 0.2112 - lr 0.000000
2022-09-29 02:23:00,248 Evaluating as a multi-label problem: False
2022-09-29 02:23:00,295 DEV : loss 0.04086737334728241 - f1-score (micro avg)  0.9751
2022-09-29 02:23:00,585 BAD EPOCHS (no improvement): 4
2022-09-29 02:23:00,587 ----------------------------------------------------------------------------------------------------
2022-09-29 02:25:08,590 epoch 39 - iter 257/2578 - loss 0.21061392 - samples/sec: 8.03 - lr: 0.000000
2022-09-29 02:27:17,243 epoch 39 - iter 514/2578 - loss 0.21305363 - samples/sec: 7.99 - lr: 0.000000
2022-09-29 02:29:34,537 epoch 39 - iter 771/2578 - loss 0.21101398 - samples/sec: 7.49 - lr: 0.000000
2022-09-29 02:31:48,840 epoch 39 - iter 1028/2578 - loss 0.21161795 - samples/sec: 7.66 - lr: 0.000000
2022-09-29 02:34:03,232 epoch 39 - iter 1285/2578 - loss 0.21119329 - samples/sec: 7.65 - lr: 0.000000
2022-09-29 02:36:25,274 epoch 39 - iter 1542/2578 - loss 0.21086018 - samples/sec: 7.24 - lr: 0.000000
2022-09-29 02:38:38,786 epoch 39 - iter 1799/2578 - loss 0.20948712 - samples/sec: 7.70 - lr: 0.000000
2022-09-29 02:40:45,476 epoch 39 - iter 2056/2578 - loss 0.20956152 - samples/sec: 8.12 - lr: 0.000000
2022-09-29 02:42:58,383 epoch 39 - iter 2313/2578 - loss 0.21014686 - samples/sec: 7.74 - lr: 0.000000
2022-09-29 02:45:15,686 epoch 39 - iter 2570/2578 - loss 0.20984287 - samples/sec: 7.49 - lr: 0.000000
2022-09-29 02:45:19,678 ----------------------------------------------------------------------------------------------------
2022-09-29 02:45:19,678 EPOCH 39 done: loss 0.2098 - lr 0.000000
2022-09-29 02:47:19,975 Evaluating as a multi-label problem: False
2022-09-29 02:47:20,022 DEV : loss 0.041431158781051636 - f1-score (micro avg)  0.9755
2022-09-29 02:47:20,312 BAD EPOCHS (no improvement): 4
2022-09-29 02:47:20,313 ----------------------------------------------------------------------------------------------------
2022-09-29 02:49:39,900 epoch 40 - iter 257/2578 - loss 0.20900598 - samples/sec: 7.37 - lr: 0.000000
2022-09-29 02:51:47,378 epoch 40 - iter 514/2578 - loss 0.21476639 - samples/sec: 8.06 - lr: 0.000000
2022-09-29 02:54:02,443 epoch 40 - iter 771/2578 - loss 0.21382667 - samples/sec: 7.61 - lr: 0.000000
2022-09-29 02:56:17,773 epoch 40 - iter 1028/2578 - loss 0.21152883 - samples/sec: 7.60 - lr: 0.000000
2022-09-29 02:58:34,995 epoch 40 - iter 1285/2578 - loss 0.21130218 - samples/sec: 7.49 - lr: 0.000000
2022-09-29 03:00:50,895 epoch 40 - iter 1542/2578 - loss 0.21103929 - samples/sec: 7.57 - lr: 0.000000
2022-09-29 03:03:06,783 epoch 40 - iter 1799/2578 - loss 0.21059252 - samples/sec: 7.57 - lr: 0.000000
2022-09-29 03:05:17,854 epoch 40 - iter 2056/2578 - loss 0.21001128 - samples/sec: 7.84 - lr: 0.000000
2022-09-29 03:07:27,228 epoch 40 - iter 2313/2578 - loss 0.20988688 - samples/sec: 7.95 - lr: 0.000000
2022-09-29 03:09:39,998 epoch 40 - iter 2570/2578 - loss 0.21004394 - samples/sec: 7.74 - lr: 0.000000
2022-09-29 03:09:43,293 ----------------------------------------------------------------------------------------------------
2022-09-29 03:09:43,293 EPOCH 40 done: loss 0.2101 - lr 0.000000
2022-09-29 03:11:44,324 Evaluating as a multi-label problem: False
2022-09-29 03:11:44,371 DEV : loss 0.041387155652046204 - f1-score (micro avg)  0.9752
2022-09-29 03:11:44,659 BAD EPOCHS (no improvement): 4
2022-09-29 03:11:48,285 ----------------------------------------------------------------------------------------------------
2022-09-29 03:11:48,322 loading file experiments/corpus_sentence_xlmrl_finetune/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased_FT_True_Ly_-1_seed_33_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.05/0/best-model.pt
2022-09-29 03:12:01,210 SequenceTagger predicts: Dictionary with 89 tags: O, S-TERRITORIO, B-TERRITORIO, E-TERRITORIO, I-TERRITORIO, S-FECHAS, B-FECHAS, E-FECHAS, I-FECHAS, S-EDAD_SUJETO_ASISTENCIA, B-EDAD_SUJETO_ASISTENCIA, E-EDAD_SUJETO_ASISTENCIA, I-EDAD_SUJETO_ASISTENCIA, S-NOMBRE_SUJETO_ASISTENCIA, B-NOMBRE_SUJETO_ASISTENCIA, E-NOMBRE_SUJETO_ASISTENCIA, I-NOMBRE_SUJETO_ASISTENCIA, S-NOMBRE_PERSONAL_SANITARIO, B-NOMBRE_PERSONAL_SANITARIO, E-NOMBRE_PERSONAL_SANITARIO, I-NOMBRE_PERSONAL_SANITARIO, S-SEXO_SUJETO_ASISTENCIA, B-SEXO_SUJETO_ASISTENCIA, E-SEXO_SUJETO_ASISTENCIA, I-SEXO_SUJETO_ASISTENCIA, S-CALLE, B-CALLE, E-CALLE, I-CALLE, S-PAIS, B-PAIS, E-PAIS, I-PAIS, S-ID_SUJETO_ASISTENCIA, B-ID_SUJETO_ASISTENCIA, E-ID_SUJETO_ASISTENCIA, I-ID_SUJETO_ASISTENCIA, S-ID_TITULACION_PERSONAL_SANITARIO, B-ID_TITULACION_PERSONAL_SANITARIO, E-ID_TITULACION_PERSONAL_SANITARIO, I-ID_TITULACION_PERSONAL_SANITARIO, S-CORREO_ELECTRONICO, B-CORREO_ELECTRONICO, E-CORREO_ELECTRONICO, I-CORREO_ELECTRONICO, S-ID_ASEGURAMIENTO, B-ID_ASEGURAMIENTO, E-ID_ASEGURAMIENTO, I-ID_ASEGURAMIENTO, S-HOSPITAL
2022-09-29 03:13:56,761 Evaluating as a multi-label problem: False
2022-09-29 03:13:56,807 0.9692	0.9783	0.9737	0.9537
2022-09-29 03:13:56,807 
Results:
- F-score (micro) 0.9737
- F-score (macro) 0.8705
- Accuracy 0.9537

By class:
                                  precision    recall  f1-score   support

                      TERRITORIO     0.9884    0.9801    0.9842       956
                          FECHAS     0.9902    0.9951    0.9927       611
          EDAD_SUJETO_ASISTENCIA     0.9735    0.9942    0.9838       518
        NOMBRE_SUJETO_ASISTENCIA     0.9980    1.0000    0.9990       502
       NOMBRE_PERSONAL_SANITARIO     0.9901    0.9940    0.9920       501
          SEXO_SUJETO_ASISTENCIA     0.9828    0.9935    0.9881       461
                           CALLE     0.9498    0.9613    0.9555       413
                            PAIS     0.9837    0.9972    0.9904       363
            ID_SUJETO_ASISTENCIA     0.9655    0.9894    0.9773       283
              CORREO_ELECTRONICO     0.9920    0.9960    0.9940       249
ID_TITULACION_PERSONAL_SANITARIO     0.9957    1.0000    0.9979       234
                ID_ASEGURAMIENTO     0.9949    0.9949    0.9949       198
                        HOSPITAL     0.9606    0.9385    0.9494       130
    FAMILIARES_SUJETO_ASISTENCIA     0.6813    0.7654    0.7209        81
                     INSTITUCION     0.5072    0.5224    0.5147        67
         ID_CONTACTO_ASISTENCIAL     1.0000    0.9744    0.9870        39
                 NUMERO_TELEFONO     0.8889    0.9231    0.9057        26
                       PROFESION     0.5000    1.0000    0.6667         9
                      NUMERO_FAX     0.6364    1.0000    0.7778         7
         OTROS_SUJETO_ASISTENCIA     0.0000    0.0000    0.0000         7
                    CENTRO_SALUD     1.0000    0.8333    0.9091         6

                       micro avg     0.9692    0.9783    0.9737      5661
                       macro avg     0.8562    0.8978    0.8705      5661
                    weighted avg     0.9707    0.9783    0.9742      5661

2022-09-29 03:13:56,807 ----------------------------------------------------------------------------------------------------
