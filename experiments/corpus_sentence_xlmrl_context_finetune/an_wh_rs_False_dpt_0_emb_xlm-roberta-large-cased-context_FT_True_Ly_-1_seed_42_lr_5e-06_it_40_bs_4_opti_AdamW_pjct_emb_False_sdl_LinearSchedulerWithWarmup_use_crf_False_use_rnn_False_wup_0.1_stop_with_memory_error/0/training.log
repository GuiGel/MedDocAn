2022-07-28 15:52:02,507 ----------------------------------------------------------------------------------------------------
2022-07-28 15:52:02,511 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=89, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2022-07-28 15:52:02,512 ----------------------------------------------------------------------------------------------------
2022-07-28 15:52:02,512 Corpus: "Corpus: 10811 train + 5518 dev + 5405 test sentences"
2022-07-28 15:52:02,512 ----------------------------------------------------------------------------------------------------
2022-07-28 15:52:02,512 Parameters:
2022-07-28 15:52:02,513  - learning_rate: "0.000005"
2022-07-28 15:52:02,513  - mini_batch_size: "4"
2022-07-28 15:52:02,513  - patience: "3"
2022-07-28 15:52:02,513  - anneal_factor: "0.5"
2022-07-28 15:52:02,513  - max_epochs: "40"
2022-07-28 15:52:02,513  - shuffle: "True"
2022-07-28 15:52:02,513  - train_with_dev: "False"
2022-07-28 15:52:02,513  - batch_growth_annealing: "False"
2022-07-28 15:52:02,513 ----------------------------------------------------------------------------------------------------
2022-07-28 15:52:02,513 Model training base path: "experiments/corpus_sentence_grid_search_roberta_docstart/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased-context_FT_True_Ly_-1_seed_42_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.1/0"
2022-07-28 15:52:02,513 ----------------------------------------------------------------------------------------------------
2022-07-28 15:52:02,513 Device: cuda:1
2022-07-28 15:52:02,513 ----------------------------------------------------------------------------------------------------
2022-07-28 15:52:02,514 Embeddings storage mode: gpu
2022-07-28 15:52:02,514 ----------------------------------------------------------------------------------------------------
2022-07-28 15:55:33,811 epoch 1 - iter 270/2703 - loss 5.07943675 - samples/sec: 5.11 - lr: 0.000000
2022-07-28 15:59:15,278 epoch 1 - iter 540/2703 - loss 4.97629067 - samples/sec: 4.88 - lr: 0.000000
2022-07-28 16:03:09,065 epoch 1 - iter 810/2703 - loss 4.48778996 - samples/sec: 4.62 - lr: 0.000000
2022-07-28 16:06:59,700 epoch 1 - iter 1080/2703 - loss 3.65350360 - samples/sec: 4.68 - lr: 0.000000
2022-07-28 16:10:35,017 epoch 1 - iter 1350/2703 - loss 3.13724631 - samples/sec: 5.02 - lr: 0.000001
2022-07-28 16:14:22,533 epoch 1 - iter 1620/2703 - loss 2.72664822 - samples/sec: 4.75 - lr: 0.000001
2022-07-28 16:18:06,648 epoch 1 - iter 1890/2703 - loss 2.44951777 - samples/sec: 4.82 - lr: 0.000001
2022-07-28 16:21:58,082 epoch 1 - iter 2160/2703 - loss 2.19706460 - samples/sec: 4.67 - lr: 0.000001
2022-07-28 16:25:43,207 epoch 1 - iter 2430/2703 - loss 2.00289011 - samples/sec: 4.80 - lr: 0.000001
2022-07-28 16:29:19,704 epoch 1 - iter 2700/2703 - loss 1.87005847 - samples/sec: 4.99 - lr: 0.000001
2022-07-28 16:29:22,724 ----------------------------------------------------------------------------------------------------
2022-07-28 16:29:22,724 EPOCH 1 done: loss 1.8673 - lr 0.000001
2022-07-28 16:35:44,877 Evaluating as a multi-label problem: False
2022-07-28 16:35:44,943 DEV : loss 0.21512486040592194 - f1-score (micro avg)  0.6157
2022-07-28 16:35:45,265 BAD EPOCHS (no improvement): 4
2022-07-28 16:35:45,270 saving best model
2022-07-28 16:35:48,823 ----------------------------------------------------------------------------------------------------
2022-07-28 16:39:36,751 epoch 2 - iter 270/2703 - loss 0.49259165 - samples/sec: 4.74 - lr: 0.000001
2022-07-28 16:43:41,269 epoch 2 - iter 540/2703 - loss 0.45843746 - samples/sec: 4.42 - lr: 0.000001
2022-07-28 16:47:29,089 epoch 2 - iter 810/2703 - loss 0.45182906 - samples/sec: 4.74 - lr: 0.000002
2022-07-28 16:51:19,480 epoch 2 - iter 1080/2703 - loss 0.43378480 - samples/sec: 4.69 - lr: 0.000002
2022-07-28 16:55:19,269 epoch 2 - iter 1350/2703 - loss 0.41859217 - samples/sec: 4.50 - lr: 0.000002
2022-07-28 16:59:16,713 epoch 2 - iter 1620/2703 - loss 0.40861725 - samples/sec: 4.55 - lr: 0.000002
2022-07-28 17:03:10,062 epoch 2 - iter 1890/2703 - loss 0.40052184 - samples/sec: 4.63 - lr: 0.000002
2022-07-28 17:07:10,825 epoch 2 - iter 2160/2703 - loss 0.39263685 - samples/sec: 4.49 - lr: 0.000002
2022-07-28 17:11:09,149 epoch 2 - iter 2430/2703 - loss 0.38084868 - samples/sec: 4.53 - lr: 0.000002
2022-07-28 17:15:07,320 epoch 2 - iter 2700/2703 - loss 0.37277977 - samples/sec: 4.53 - lr: 0.000002
2022-07-28 17:15:10,776 ----------------------------------------------------------------------------------------------------
2022-07-28 17:15:10,776 EPOCH 2 done: loss 0.3723 - lr 0.000002
2022-07-28 17:21:29,634 Evaluating as a multi-label problem: False
2022-07-28 17:21:29,693 DEV : loss 0.07423406094312668 - f1-score (micro avg)  0.8282
2022-07-28 17:21:30,013 BAD EPOCHS (no improvement): 4
2022-07-28 17:21:30,019 saving best model
2022-07-28 17:21:50,499 ----------------------------------------------------------------------------------------------------
2022-07-28 17:25:45,439 epoch 3 - iter 270/2703 - loss 0.29579583 - samples/sec: 4.60 - lr: 0.000003
2022-07-28 17:29:33,523 epoch 3 - iter 540/2703 - loss 0.30404887 - samples/sec: 4.74 - lr: 0.000003
2022-07-28 17:33:26,974 epoch 3 - iter 810/2703 - loss 0.30168523 - samples/sec: 4.63 - lr: 0.000003
2022-07-28 17:37:16,212 epoch 3 - iter 1080/2703 - loss 0.30008991 - samples/sec: 4.71 - lr: 0.000003
2022-07-28 17:41:09,081 epoch 3 - iter 1350/2703 - loss 0.29820252 - samples/sec: 4.64 - lr: 0.000003
2022-07-28 17:45:01,599 epoch 3 - iter 1620/2703 - loss 0.29613842 - samples/sec: 4.65 - lr: 0.000003
2022-07-28 17:49:06,595 epoch 3 - iter 1890/2703 - loss 0.29099120 - samples/sec: 4.41 - lr: 0.000003
2022-07-28 17:53:04,434 epoch 3 - iter 2160/2703 - loss 0.29132100 - samples/sec: 4.54 - lr: 0.000003
2022-07-28 17:56:56,520 epoch 3 - iter 2430/2703 - loss 0.29069532 - samples/sec: 4.65 - lr: 0.000004
2022-07-28 18:00:49,686 epoch 3 - iter 2700/2703 - loss 0.28880776 - samples/sec: 4.63 - lr: 0.000004
2022-07-28 18:00:51,775 ----------------------------------------------------------------------------------------------------
2022-07-28 18:00:51,775 EPOCH 3 done: loss 0.2888 - lr 0.000004
2022-07-28 18:07:16,306 Evaluating as a multi-label problem: False
2022-07-28 18:07:16,363 DEV : loss 0.05243473872542381 - f1-score (micro avg)  0.8876
2022-07-28 18:07:16,684 BAD EPOCHS (no improvement): 4
2022-07-28 18:07:16,690 saving best model
2022-07-28 18:07:37,211 ----------------------------------------------------------------------------------------------------
2022-07-28 18:11:32,256 epoch 4 - iter 270/2703 - loss 0.27476052 - samples/sec: 4.60 - lr: 0.000004
2022-07-28 18:15:35,590 epoch 4 - iter 540/2703 - loss 0.27909128 - samples/sec: 4.44 - lr: 0.000004
2022-07-28 18:19:20,179 epoch 4 - iter 810/2703 - loss 0.27437196 - samples/sec: 4.81 - lr: 0.000004
2022-07-28 18:23:15,239 epoch 4 - iter 1080/2703 - loss 0.27110430 - samples/sec: 4.59 - lr: 0.000004
2022-07-28 18:27:13,161 epoch 4 - iter 1350/2703 - loss 0.27020665 - samples/sec: 4.54 - lr: 0.000004
2022-07-28 18:31:04,574 epoch 4 - iter 1620/2703 - loss 0.26934911 - samples/sec: 4.67 - lr: 0.000004
2022-07-28 18:34:58,471 epoch 4 - iter 1890/2703 - loss 0.26918124 - samples/sec: 4.62 - lr: 0.000005
2022-07-28 18:38:53,644 epoch 4 - iter 2160/2703 - loss 0.26906500 - samples/sec: 4.59 - lr: 0.000005
2022-07-28 18:42:46,444 epoch 4 - iter 2430/2703 - loss 0.26907794 - samples/sec: 4.64 - lr: 0.000005
2022-07-28 18:46:46,827 epoch 4 - iter 2700/2703 - loss 0.26730267 - samples/sec: 4.49 - lr: 0.000005
2022-07-28 18:46:49,268 ----------------------------------------------------------------------------------------------------
2022-07-28 18:46:49,268 EPOCH 4 done: loss 0.2672 - lr 0.000005
2022-07-28 18:53:18,773 Evaluating as a multi-label problem: False
2022-07-28 18:53:18,827 DEV : loss 0.05054688826203346 - f1-score (micro avg)  0.9067
2022-07-28 18:53:19,159 BAD EPOCHS (no improvement): 4
2022-07-28 18:53:19,164 saving best model
2022-07-28 18:53:39,958 ----------------------------------------------------------------------------------------------------
2022-07-28 18:57:20,448 epoch 5 - iter 270/2703 - loss 0.25155942 - samples/sec: 4.90 - lr: 0.000005
2022-07-28 19:01:14,801 epoch 5 - iter 540/2703 - loss 0.25770611 - samples/sec: 4.61 - lr: 0.000005
2022-07-28 19:05:18,052 epoch 5 - iter 810/2703 - loss 0.25448489 - samples/sec: 4.44 - lr: 0.000005
2022-07-28 19:09:17,224 epoch 5 - iter 1080/2703 - loss 0.25188484 - samples/sec: 4.52 - lr: 0.000005
2022-07-28 19:13:13,509 epoch 5 - iter 1350/2703 - loss 0.25203341 - samples/sec: 4.57 - lr: 0.000005
2022-07-28 19:17:09,579 epoch 5 - iter 1620/2703 - loss 0.25360331 - samples/sec: 4.58 - lr: 0.000005
2022-07-28 19:21:07,150 epoch 5 - iter 1890/2703 - loss 0.25608048 - samples/sec: 4.55 - lr: 0.000005
2022-07-28 19:25:03,070 epoch 5 - iter 2160/2703 - loss 0.25445628 - samples/sec: 4.58 - lr: 0.000005
2022-07-28 19:28:53,587 epoch 5 - iter 2430/2703 - loss 0.25343001 - samples/sec: 4.69 - lr: 0.000005
2022-07-28 19:32:53,500 epoch 5 - iter 2700/2703 - loss 0.25220821 - samples/sec: 4.50 - lr: 0.000005
2022-07-28 19:32:56,063 ----------------------------------------------------------------------------------------------------
2022-07-28 19:32:56,063 EPOCH 5 done: loss 0.2522 - lr 0.000005
2022-07-28 19:39:21,347 Evaluating as a multi-label problem: False
2022-07-28 19:39:21,402 DEV : loss 0.03398900851607323 - f1-score (micro avg)  0.9455
2022-07-28 19:39:21,728 BAD EPOCHS (no improvement): 4
2022-07-28 19:39:21,731 saving best model
2022-07-28 19:39:42,628 ----------------------------------------------------------------------------------------------------
2022-07-28 19:43:40,105 epoch 6 - iter 270/2703 - loss 0.24245415 - samples/sec: 4.55 - lr: 0.000005
2022-07-28 19:47:31,870 epoch 6 - iter 540/2703 - loss 0.23582474 - samples/sec: 4.66 - lr: 0.000005
2022-07-28 19:51:27,886 epoch 6 - iter 810/2703 - loss 0.23902602 - samples/sec: 4.58 - lr: 0.000005
2022-07-28 19:55:26,985 epoch 6 - iter 1080/2703 - loss 0.24095305 - samples/sec: 4.52 - lr: 0.000005
2022-07-28 19:59:29,353 epoch 6 - iter 1350/2703 - loss 0.24382583 - samples/sec: 4.46 - lr: 0.000005
2022-07-28 20:03:25,514 epoch 6 - iter 1620/2703 - loss 0.24488162 - samples/sec: 4.57 - lr: 0.000005
2022-07-28 20:07:19,332 epoch 6 - iter 1890/2703 - loss 0.24501918 - samples/sec: 4.62 - lr: 0.000005
2022-07-28 20:11:23,230 epoch 6 - iter 2160/2703 - loss 0.24629199 - samples/sec: 4.43 - lr: 0.000005
2022-07-28 20:15:24,300 epoch 6 - iter 2430/2703 - loss 0.24353056 - samples/sec: 4.48 - lr: 0.000005
2022-07-28 20:19:16,497 epoch 6 - iter 2700/2703 - loss 0.24314020 - samples/sec: 4.65 - lr: 0.000005
2022-07-28 20:19:18,569 ----------------------------------------------------------------------------------------------------
2022-07-28 20:19:18,569 EPOCH 6 done: loss 0.2432 - lr 0.000005
2022-07-28 20:25:47,277 Evaluating as a multi-label problem: False
2022-07-28 20:25:47,333 DEV : loss 0.03416936472058296 - f1-score (micro avg)  0.9616
2022-07-28 20:25:47,658 BAD EPOCHS (no improvement): 4
2022-07-28 20:25:47,664 saving best model
2022-07-28 20:26:09,229 ----------------------------------------------------------------------------------------------------
2022-07-28 20:30:02,526 epoch 7 - iter 270/2703 - loss 0.23389432 - samples/sec: 4.63 - lr: 0.000005
2022-07-28 20:34:07,186 epoch 7 - iter 540/2703 - loss 0.23395078 - samples/sec: 4.41 - lr: 0.000005
2022-07-28 20:37:57,682 epoch 7 - iter 810/2703 - loss 0.23838209 - samples/sec: 4.69 - lr: 0.000005
2022-07-28 20:41:56,062 epoch 7 - iter 1080/2703 - loss 0.23773927 - samples/sec: 4.53 - lr: 0.000005
2022-07-28 20:45:51,363 epoch 7 - iter 1350/2703 - loss 0.23760455 - samples/sec: 4.59 - lr: 0.000005
2022-07-28 20:49:49,544 epoch 7 - iter 1620/2703 - loss 0.23745111 - samples/sec: 4.53 - lr: 0.000005
2022-07-28 20:53:39,222 epoch 7 - iter 1890/2703 - loss 0.23844018 - samples/sec: 4.70 - lr: 0.000005
2022-07-28 20:57:37,767 epoch 7 - iter 2160/2703 - loss 0.23923301 - samples/sec: 4.53 - lr: 0.000005
2022-07-28 21:01:40,765 epoch 7 - iter 2430/2703 - loss 0.23883962 - samples/sec: 4.44 - lr: 0.000005
2022-07-28 21:05:35,866 epoch 7 - iter 2700/2703 - loss 0.24042758 - samples/sec: 4.59 - lr: 0.000005
2022-07-28 21:05:38,982 ----------------------------------------------------------------------------------------------------
2022-07-28 21:05:38,982 EPOCH 7 done: loss 0.2406 - lr 0.000005
2022-07-28 21:12:02,452 Evaluating as a multi-label problem: False
2022-07-28 21:12:02,503 DEV : loss 0.03618079423904419 - f1-score (micro avg)  0.9619
2022-07-28 21:12:02,830 BAD EPOCHS (no improvement): 4
2022-07-28 21:12:02,835 saving best model
2022-07-28 21:12:24,801 ----------------------------------------------------------------------------------------------------
2022-07-28 21:16:13,732 epoch 8 - iter 270/2703 - loss 0.23480011 - samples/sec: 4.72 - lr: 0.000005
2022-07-28 21:20:12,033 epoch 8 - iter 540/2703 - loss 0.23697369 - samples/sec: 4.53 - lr: 0.000005
2022-07-28 21:23:57,705 epoch 8 - iter 810/2703 - loss 0.23558327 - samples/sec: 4.79 - lr: 0.000005
2022-07-28 21:28:04,765 epoch 8 - iter 1080/2703 - loss 0.23309624 - samples/sec: 4.37 - lr: 0.000005
2022-07-28 21:32:05,001 epoch 8 - iter 1350/2703 - loss 0.23676299 - samples/sec: 4.50 - lr: 0.000005
2022-07-28 21:35:58,679 epoch 8 - iter 1620/2703 - loss 0.23420110 - samples/sec: 4.62 - lr: 0.000005
2022-07-28 21:39:55,813 epoch 8 - iter 1890/2703 - loss 0.23530893 - samples/sec: 4.55 - lr: 0.000004
2022-07-28 21:43:47,047 epoch 8 - iter 2160/2703 - loss 0.23555166 - samples/sec: 4.67 - lr: 0.000004
2022-07-28 21:47:38,825 epoch 8 - iter 2430/2703 - loss 0.23574600 - samples/sec: 4.66 - lr: 0.000004
2022-07-28 21:51:39,538 epoch 8 - iter 2700/2703 - loss 0.23504454 - samples/sec: 4.49 - lr: 0.000004
2022-07-28 21:51:42,278 ----------------------------------------------------------------------------------------------------
2022-07-28 21:51:42,278 EPOCH 8 done: loss 0.2350 - lr 0.000004
2022-07-28 21:58:10,175 Evaluating as a multi-label problem: False
2022-07-28 21:58:10,231 DEV : loss 0.034358493983745575 - f1-score (micro avg)  0.9686
2022-07-28 21:58:10,571 BAD EPOCHS (no improvement): 4
2022-07-28 21:58:10,575 saving best model
2022-07-28 21:58:32,503 ----------------------------------------------------------------------------------------------------
2022-07-28 22:02:29,115 epoch 9 - iter 270/2703 - loss 0.23098337 - samples/sec: 4.56 - lr: 0.000004
2022-07-28 22:06:26,220 epoch 9 - iter 540/2703 - loss 0.22904310 - samples/sec: 4.56 - lr: 0.000004
2022-07-28 22:10:16,719 epoch 9 - iter 810/2703 - loss 0.23465789 - samples/sec: 4.69 - lr: 0.000004
2022-07-28 22:14:05,972 epoch 9 - iter 1080/2703 - loss 0.23740832 - samples/sec: 4.71 - lr: 0.000004
2022-07-28 22:18:02,659 epoch 9 - iter 1350/2703 - loss 0.23630361 - samples/sec: 4.56 - lr: 0.000004
2022-07-28 22:21:56,079 epoch 9 - iter 1620/2703 - loss 0.23813496 - samples/sec: 4.63 - lr: 0.000004
2022-07-28 22:25:54,830 epoch 9 - iter 1890/2703 - loss 0.23723044 - samples/sec: 4.52 - lr: 0.000004
2022-07-28 22:30:01,114 epoch 9 - iter 2160/2703 - loss 0.23750406 - samples/sec: 4.39 - lr: 0.000004
2022-07-28 22:33:59,984 epoch 9 - iter 2430/2703 - loss 0.23475533 - samples/sec: 4.52 - lr: 0.000004
2022-07-28 22:37:58,272 epoch 9 - iter 2700/2703 - loss 0.23388356 - samples/sec: 4.53 - lr: 0.000004
2022-07-28 22:38:00,502 ----------------------------------------------------------------------------------------------------
2022-07-28 22:38:00,503 EPOCH 9 done: loss 0.2337 - lr 0.000004
2022-07-28 22:44:24,504 Evaluating as a multi-label problem: False
2022-07-28 22:44:24,557 DEV : loss 0.03194916248321533 - f1-score (micro avg)  0.9695
2022-07-28 22:44:24,881 BAD EPOCHS (no improvement): 4
2022-07-28 22:44:24,887 saving best model
2022-07-28 22:44:47,574 ----------------------------------------------------------------------------------------------------
2022-07-28 22:48:46,422 epoch 10 - iter 270/2703 - loss 0.22791384 - samples/sec: 4.52 - lr: 0.000004
2022-07-28 22:52:42,674 epoch 10 - iter 540/2703 - loss 0.22636588 - samples/sec: 4.57 - lr: 0.000004
2022-07-28 22:56:39,759 epoch 10 - iter 810/2703 - loss 0.22786628 - samples/sec: 4.56 - lr: 0.000004
2022-07-28 23:00:42,299 epoch 10 - iter 1080/2703 - loss 0.22797262 - samples/sec: 4.45 - lr: 0.000004
2022-07-28 23:04:33,466 epoch 10 - iter 1350/2703 - loss 0.22781795 - samples/sec: 4.67 - lr: 0.000004
2022-07-28 23:08:27,790 epoch 10 - iter 1620/2703 - loss 0.23037118 - samples/sec: 4.61 - lr: 0.000004
2022-07-28 23:12:25,259 epoch 10 - iter 1890/2703 - loss 0.23083238 - samples/sec: 4.55 - lr: 0.000004
2022-07-28 23:16:28,147 epoch 10 - iter 2160/2703 - loss 0.22997714 - samples/sec: 4.45 - lr: 0.000004
2022-07-28 23:20:24,543 epoch 10 - iter 2430/2703 - loss 0.23027745 - samples/sec: 4.57 - lr: 0.000004
2022-07-28 23:24:18,147 epoch 10 - iter 2700/2703 - loss 0.23052392 - samples/sec: 4.62 - lr: 0.000004
2022-07-28 23:24:20,300 ----------------------------------------------------------------------------------------------------
2022-07-28 23:24:20,300 EPOCH 10 done: loss 0.2304 - lr 0.000004
2022-07-28 23:30:48,929 Evaluating as a multi-label problem: False
2022-07-28 23:30:48,980 DEV : loss 0.03612106293439865 - f1-score (micro avg)  0.9733
2022-07-28 23:30:49,306 BAD EPOCHS (no improvement): 4
2022-07-28 23:30:49,316 saving best model
2022-07-28 23:31:12,505 ----------------------------------------------------------------------------------------------------
