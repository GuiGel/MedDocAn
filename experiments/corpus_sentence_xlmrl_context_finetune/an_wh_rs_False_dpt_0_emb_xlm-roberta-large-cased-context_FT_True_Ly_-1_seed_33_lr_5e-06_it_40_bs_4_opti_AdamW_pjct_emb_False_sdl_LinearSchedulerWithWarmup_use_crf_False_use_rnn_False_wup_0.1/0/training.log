2022-07-29 15:57:33,584 ----------------------------------------------------------------------------------------------------
2022-07-29 15:57:33,588 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=89, bias=True)
  (loss_function): CrossEntropyLoss()
)"
2022-07-29 15:57:33,588 ----------------------------------------------------------------------------------------------------
2022-07-29 15:57:33,588 Corpus: "Corpus: 10811 train + 5518 dev + 5405 test sentences"
2022-07-29 15:57:33,589 ----------------------------------------------------------------------------------------------------
2022-07-29 15:57:33,589 Parameters:
2022-07-29 15:57:33,589  - learning_rate: "0.000005"
2022-07-29 15:57:33,589  - mini_batch_size: "4"
2022-07-29 15:57:33,589  - patience: "3"
2022-07-29 15:57:33,589  - anneal_factor: "0.5"
2022-07-29 15:57:33,589  - max_epochs: "40"
2022-07-29 15:57:33,589  - shuffle: "True"
2022-07-29 15:57:33,589  - train_with_dev: "False"
2022-07-29 15:57:33,589  - batch_growth_annealing: "False"
2022-07-29 15:57:33,589 ----------------------------------------------------------------------------------------------------
2022-07-29 15:57:33,589 Model training base path: "experiments/corpus_sentence_grid_search_roberta_docstart/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased-context_FT_True_Ly_-1_seed_33_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.1/0"
2022-07-29 15:57:33,589 ----------------------------------------------------------------------------------------------------
2022-07-29 15:57:33,589 Device: cuda:1
2022-07-29 15:57:33,589 ----------------------------------------------------------------------------------------------------
2022-07-29 15:57:33,589 Embeddings storage mode: gpu
2022-07-29 15:57:33,590 ----------------------------------------------------------------------------------------------------
2022-07-29 16:01:06,287 epoch 1 - iter 270/2703 - loss 4.09284766 - samples/sec: 5.08 - lr: 0.000000
2022-07-29 16:04:44,168 epoch 1 - iter 540/2703 - loss 3.93422460 - samples/sec: 4.96 - lr: 0.000000
2022-07-29 16:08:38,419 epoch 1 - iter 810/2703 - loss 3.35401706 - samples/sec: 4.61 - lr: 0.000000
2022-07-29 16:12:27,958 epoch 1 - iter 1080/2703 - loss 2.75454090 - samples/sec: 4.71 - lr: 0.000000
2022-07-29 16:16:09,250 epoch 1 - iter 1350/2703 - loss 2.40546107 - samples/sec: 4.88 - lr: 0.000001
2022-07-29 16:19:58,926 epoch 1 - iter 1620/2703 - loss 2.12481564 - samples/sec: 4.70 - lr: 0.000001
2022-07-29 16:23:42,351 epoch 1 - iter 1890/2703 - loss 1.93513357 - samples/sec: 4.83 - lr: 0.000001
2022-07-29 16:27:28,141 epoch 1 - iter 2160/2703 - loss 1.75685418 - samples/sec: 4.78 - lr: 0.000001
2022-07-29 16:31:14,709 epoch 1 - iter 2430/2703 - loss 1.61601803 - samples/sec: 4.77 - lr: 0.000001
2022-07-29 16:34:49,573 epoch 1 - iter 2700/2703 - loss 1.52185280 - samples/sec: 5.03 - lr: 0.000001
2022-07-29 16:34:52,129 ----------------------------------------------------------------------------------------------------
2022-07-29 16:34:52,129 EPOCH 1 done: loss 1.5198 - lr 0.000001
2022-07-29 16:41:18,858 Evaluating as a multi-label problem: False
2022-07-29 16:41:18,925 DEV : loss 0.23461110889911652 - f1-score (micro avg)  0.5396
2022-07-29 16:41:19,247 BAD EPOCHS (no improvement): 4
2022-07-29 16:41:19,251 saving best model
2022-07-29 16:41:22,824 ----------------------------------------------------------------------------------------------------
2022-07-29 16:45:24,144 epoch 2 - iter 270/2703 - loss 0.45245736 - samples/sec: 4.48 - lr: 0.000001
2022-07-29 16:49:20,925 epoch 2 - iter 540/2703 - loss 0.45992663 - samples/sec: 4.56 - lr: 0.000001
2022-07-29 16:53:17,211 epoch 2 - iter 810/2703 - loss 0.44908859 - samples/sec: 4.57 - lr: 0.000002
2022-07-29 16:57:06,993 epoch 2 - iter 1080/2703 - loss 0.44612884 - samples/sec: 4.70 - lr: 0.000002
2022-07-29 17:00:57,452 epoch 2 - iter 1350/2703 - loss 0.43702752 - samples/sec: 4.69 - lr: 0.000002
2022-07-29 17:04:47,423 epoch 2 - iter 1620/2703 - loss 0.42635895 - samples/sec: 4.70 - lr: 0.000002
2022-07-29 17:08:44,241 epoch 2 - iter 1890/2703 - loss 0.41361492 - samples/sec: 4.56 - lr: 0.000002
2022-07-29 17:12:38,166 epoch 2 - iter 2160/2703 - loss 0.40335732 - samples/sec: 4.62 - lr: 0.000002
2022-07-29 17:16:30,819 epoch 2 - iter 2430/2703 - loss 0.39453326 - samples/sec: 4.64 - lr: 0.000002
2022-07-29 17:20:35,829 epoch 2 - iter 2700/2703 - loss 0.38551481 - samples/sec: 4.41 - lr: 0.000002
2022-07-29 17:20:38,200 ----------------------------------------------------------------------------------------------------
2022-07-29 17:20:38,200 EPOCH 2 done: loss 0.3856 - lr 0.000002
2022-07-29 17:27:10,160 Evaluating as a multi-label problem: False
2022-07-29 17:27:10,217 DEV : loss 0.06775662302970886 - f1-score (micro avg)  0.8404
2022-07-29 17:27:10,536 BAD EPOCHS (no improvement): 4
2022-07-29 17:27:10,540 saving best model
2022-07-29 17:27:33,999 ----------------------------------------------------------------------------------------------------
2022-07-29 17:31:21,175 epoch 3 - iter 270/2703 - loss 0.29021401 - samples/sec: 4.75 - lr: 0.000003
2022-07-29 17:35:16,183 epoch 3 - iter 540/2703 - loss 0.29651071 - samples/sec: 4.60 - lr: 0.000003
2022-07-29 17:39:22,746 epoch 3 - iter 810/2703 - loss 0.28886272 - samples/sec: 4.38 - lr: 0.000003
2022-07-29 17:43:15,249 epoch 3 - iter 1080/2703 - loss 0.29091212 - samples/sec: 4.65 - lr: 0.000003
2022-07-29 17:47:17,907 epoch 3 - iter 1350/2703 - loss 0.28899766 - samples/sec: 4.45 - lr: 0.000003
2022-07-29 17:51:17,297 epoch 3 - iter 1620/2703 - loss 0.28821046 - samples/sec: 4.51 - lr: 0.000003
2022-07-29 17:55:16,568 epoch 3 - iter 1890/2703 - loss 0.28771977 - samples/sec: 4.51 - lr: 0.000003
2022-07-29 17:59:15,430 epoch 3 - iter 2160/2703 - loss 0.28590800 - samples/sec: 4.52 - lr: 0.000003
2022-07-29 18:03:20,133 epoch 3 - iter 2430/2703 - loss 0.28683874 - samples/sec: 4.41 - lr: 0.000004
2022-07-29 18:07:18,019 epoch 3 - iter 2700/2703 - loss 0.28731782 - samples/sec: 4.54 - lr: 0.000004
2022-07-29 18:07:20,136 ----------------------------------------------------------------------------------------------------
2022-07-29 18:07:20,136 EPOCH 3 done: loss 0.2875 - lr 0.000004
2022-07-29 18:13:50,042 Evaluating as a multi-label problem: False
2022-07-29 18:13:50,098 DEV : loss 0.05036364868283272 - f1-score (micro avg)  0.9095
2022-07-29 18:13:50,420 BAD EPOCHS (no improvement): 4
2022-07-29 18:13:50,426 saving best model
2022-07-29 18:14:14,047 ----------------------------------------------------------------------------------------------------
2022-07-29 18:18:08,478 epoch 4 - iter 270/2703 - loss 0.27107946 - samples/sec: 4.61 - lr: 0.000004
2022-07-29 18:22:07,864 epoch 4 - iter 540/2703 - loss 0.27017280 - samples/sec: 4.51 - lr: 0.000004
2022-07-29 18:26:04,659 epoch 4 - iter 810/2703 - loss 0.27105596 - samples/sec: 4.56 - lr: 0.000004
2022-07-29 18:30:04,068 epoch 4 - iter 1080/2703 - loss 0.26777049 - samples/sec: 4.51 - lr: 0.000004
2022-07-29 18:33:59,580 epoch 4 - iter 1350/2703 - loss 0.26696694 - samples/sec: 4.59 - lr: 0.000004
2022-07-29 18:37:59,869 epoch 4 - iter 1620/2703 - loss 0.26626808 - samples/sec: 4.49 - lr: 0.000004
2022-07-29 18:41:56,877 epoch 4 - iter 1890/2703 - loss 0.26671467 - samples/sec: 4.56 - lr: 0.000005
2022-07-29 18:45:53,866 epoch 4 - iter 2160/2703 - loss 0.26522346 - samples/sec: 4.56 - lr: 0.000005
2022-07-29 18:49:53,285 epoch 4 - iter 2430/2703 - loss 0.26388091 - samples/sec: 4.51 - lr: 0.000005
2022-07-29 18:53:53,323 epoch 4 - iter 2700/2703 - loss 0.26488210 - samples/sec: 4.50 - lr: 0.000005
2022-07-29 18:53:55,341 ----------------------------------------------------------------------------------------------------
2022-07-29 18:53:55,341 EPOCH 4 done: loss 0.2650 - lr 0.000005
2022-07-29 19:00:27,547 Evaluating as a multi-label problem: False
2022-07-29 19:00:27,602 DEV : loss 0.04370090365409851 - f1-score (micro avg)  0.9245
2022-07-29 19:00:27,932 BAD EPOCHS (no improvement): 4
2022-07-29 19:00:27,935 saving best model
2022-07-29 19:00:51,821 ----------------------------------------------------------------------------------------------------
2022-07-29 19:04:53,887 epoch 5 - iter 270/2703 - loss 0.24578831 - samples/sec: 4.46 - lr: 0.000005
2022-07-29 19:09:05,107 epoch 5 - iter 540/2703 - loss 0.24952602 - samples/sec: 4.30 - lr: 0.000005
2022-07-29 19:12:55,750 epoch 5 - iter 810/2703 - loss 0.25332876 - samples/sec: 4.68 - lr: 0.000005
2022-07-29 19:16:56,928 epoch 5 - iter 1080/2703 - loss 0.25539353 - samples/sec: 4.48 - lr: 0.000005
2022-07-29 19:20:58,003 epoch 5 - iter 1350/2703 - loss 0.25540674 - samples/sec: 4.48 - lr: 0.000005
2022-07-29 19:25:01,324 epoch 5 - iter 1620/2703 - loss 0.25566551 - samples/sec: 4.44 - lr: 0.000005
2022-07-29 19:29:00,083 epoch 5 - iter 1890/2703 - loss 0.25534919 - samples/sec: 4.52 - lr: 0.000005
2022-07-29 19:32:47,154 epoch 5 - iter 2160/2703 - loss 0.25597190 - samples/sec: 4.76 - lr: 0.000005
2022-07-29 19:36:45,222 epoch 5 - iter 2430/2703 - loss 0.25600220 - samples/sec: 4.54 - lr: 0.000005
2022-07-29 19:40:35,688 epoch 5 - iter 2700/2703 - loss 0.25638611 - samples/sec: 4.69 - lr: 0.000005
2022-07-29 19:40:38,129 ----------------------------------------------------------------------------------------------------
2022-07-29 19:40:38,130 EPOCH 5 done: loss 0.2564 - lr 0.000005
2022-07-29 19:47:17,289 Evaluating as a multi-label problem: False
2022-07-29 19:47:17,348 DEV : loss 0.04196430370211601 - f1-score (micro avg)  0.9391
2022-07-29 19:47:17,664 BAD EPOCHS (no improvement): 4
2022-07-29 19:47:17,668 saving best model
2022-07-29 19:47:41,380 ----------------------------------------------------------------------------------------------------
2022-07-29 19:51:33,643 epoch 6 - iter 270/2703 - loss 0.23727278 - samples/sec: 4.65 - lr: 0.000005
2022-07-29 19:55:26,100 epoch 6 - iter 540/2703 - loss 0.24341795 - samples/sec: 4.65 - lr: 0.000005
2022-07-29 19:59:22,078 epoch 6 - iter 810/2703 - loss 0.25124770 - samples/sec: 4.58 - lr: 0.000005
2022-07-29 20:03:20,588 epoch 6 - iter 1080/2703 - loss 0.24903730 - samples/sec: 4.53 - lr: 0.000005
2022-07-29 20:07:15,621 epoch 6 - iter 1350/2703 - loss 0.24788054 - samples/sec: 4.60 - lr: 0.000005
2022-07-29 20:11:12,184 epoch 6 - iter 1620/2703 - loss 0.24676598 - samples/sec: 4.57 - lr: 0.000005
2022-07-29 20:15:09,363 epoch 6 - iter 1890/2703 - loss 0.24579463 - samples/sec: 4.55 - lr: 0.000005
2022-07-29 20:19:11,941 epoch 6 - iter 2160/2703 - loss 0.24480892 - samples/sec: 4.45 - lr: 0.000005
2022-07-29 20:23:08,854 epoch 6 - iter 2430/2703 - loss 0.24585646 - samples/sec: 4.56 - lr: 0.000005
2022-07-29 20:27:06,182 epoch 6 - iter 2700/2703 - loss 0.24587121 - samples/sec: 4.55 - lr: 0.000005
2022-07-29 20:27:08,839 ----------------------------------------------------------------------------------------------------
2022-07-29 20:27:08,839 EPOCH 6 done: loss 0.2457 - lr 0.000005
2022-07-29 20:33:41,812 Evaluating as a multi-label problem: False
2022-07-29 20:33:41,864 DEV : loss 0.03768535330891609 - f1-score (micro avg)  0.9585
2022-07-29 20:33:42,184 BAD EPOCHS (no improvement): 4
2022-07-29 20:33:42,191 saving best model
2022-07-29 20:34:06,234 ----------------------------------------------------------------------------------------------------
2022-07-29 20:37:59,475 epoch 7 - iter 270/2703 - loss 0.23480020 - samples/sec: 4.63 - lr: 0.000005
2022-07-29 20:41:54,491 epoch 7 - iter 540/2703 - loss 0.23742675 - samples/sec: 4.60 - lr: 0.000005
2022-07-29 20:45:46,931 epoch 7 - iter 810/2703 - loss 0.23964193 - samples/sec: 4.65 - lr: 0.000005
2022-07-29 20:49:44,730 epoch 7 - iter 1080/2703 - loss 0.24419490 - samples/sec: 4.54 - lr: 0.000005
2022-07-29 20:53:46,352 epoch 7 - iter 1350/2703 - loss 0.24122449 - samples/sec: 4.47 - lr: 0.000005
2022-07-29 20:57:48,559 epoch 7 - iter 1620/2703 - loss 0.23855495 - samples/sec: 4.46 - lr: 0.000005
2022-07-29 21:01:43,759 epoch 7 - iter 1890/2703 - loss 0.24038810 - samples/sec: 4.59 - lr: 0.000005
2022-07-29 21:05:36,320 epoch 7 - iter 2160/2703 - loss 0.23956368 - samples/sec: 4.64 - lr: 0.000005
2022-07-29 21:09:40,795 epoch 7 - iter 2430/2703 - loss 0.24048522 - samples/sec: 4.42 - lr: 0.000005
2022-07-29 21:13:37,017 epoch 7 - iter 2700/2703 - loss 0.24029335 - samples/sec: 4.57 - lr: 0.000005
2022-07-29 21:13:38,878 ----------------------------------------------------------------------------------------------------
2022-07-29 21:13:38,879 EPOCH 7 done: loss 0.2403 - lr 0.000005
2022-07-29 21:20:12,572 Evaluating as a multi-label problem: False
2022-07-29 21:20:12,625 DEV : loss 0.03192580118775368 - f1-score (micro avg)  0.9641
2022-07-29 21:20:12,948 BAD EPOCHS (no improvement): 4
2022-07-29 21:20:12,952 saving best model
2022-07-29 21:20:36,438 ----------------------------------------------------------------------------------------------------
2022-07-29 21:24:38,692 epoch 8 - iter 270/2703 - loss 0.23532479 - samples/sec: 4.46 - lr: 0.000005
2022-07-29 21:28:37,966 epoch 8 - iter 540/2703 - loss 0.23606645 - samples/sec: 4.51 - lr: 0.000005
2022-07-29 21:32:38,585 epoch 8 - iter 810/2703 - loss 0.23698038 - samples/sec: 4.49 - lr: 0.000005
2022-07-29 21:36:23,220 epoch 8 - iter 1080/2703 - loss 0.23506739 - samples/sec: 4.81 - lr: 0.000005
2022-07-29 21:40:18,381 epoch 8 - iter 1350/2703 - loss 0.23545031 - samples/sec: 4.59 - lr: 0.000005
2022-07-29 21:44:11,808 epoch 8 - iter 1620/2703 - loss 0.23594676 - samples/sec: 4.63 - lr: 0.000005
2022-07-29 21:48:13,770 epoch 8 - iter 1890/2703 - loss 0.23540971 - samples/sec: 4.46 - lr: 0.000004
2022-07-29 21:52:11,351 epoch 8 - iter 2160/2703 - loss 0.23673154 - samples/sec: 4.55 - lr: 0.000004
2022-07-29 21:56:12,528 epoch 8 - iter 2430/2703 - loss 0.23671995 - samples/sec: 4.48 - lr: 0.000004
2022-07-29 22:00:06,709 epoch 8 - iter 2700/2703 - loss 0.23595501 - samples/sec: 4.61 - lr: 0.000004
2022-07-29 22:00:08,033 ----------------------------------------------------------------------------------------------------
2022-07-29 22:00:08,034 EPOCH 8 done: loss 0.2360 - lr 0.000004
2022-07-29 22:06:35,639 Evaluating as a multi-label problem: False
2022-07-29 22:06:35,693 DEV : loss 0.0331433042883873 - f1-score (micro avg)  0.9675
2022-07-29 22:06:36,013 BAD EPOCHS (no improvement): 4
2022-07-29 22:06:36,018 saving best model
2022-07-29 22:06:59,324 ----------------------------------------------------------------------------------------------------
2022-07-29 22:10:50,706 epoch 9 - iter 270/2703 - loss 0.23135334 - samples/sec: 4.67 - lr: 0.000004
2022-07-29 22:14:50,493 epoch 9 - iter 540/2703 - loss 0.23696218 - samples/sec: 4.50 - lr: 0.000004
2022-07-29 22:18:43,971 epoch 9 - iter 810/2703 - loss 0.23655612 - samples/sec: 4.63 - lr: 0.000004
2022-07-29 22:22:48,547 epoch 9 - iter 1080/2703 - loss 0.23758775 - samples/sec: 4.42 - lr: 0.000004
2022-07-29 22:26:42,040 epoch 9 - iter 1350/2703 - loss 0.23802304 - samples/sec: 4.63 - lr: 0.000004
2022-07-29 22:30:40,551 epoch 9 - iter 1620/2703 - loss 0.23585919 - samples/sec: 4.53 - lr: 0.000004
2022-07-29 22:34:34,012 epoch 9 - iter 1890/2703 - loss 0.23558650 - samples/sec: 4.63 - lr: 0.000004
2022-07-29 22:38:36,479 epoch 9 - iter 2160/2703 - loss 0.23567046 - samples/sec: 4.45 - lr: 0.000004
2022-07-29 22:42:30,087 epoch 9 - iter 2430/2703 - loss 0.23516793 - samples/sec: 4.62 - lr: 0.000004
2022-07-29 22:46:30,942 epoch 9 - iter 2700/2703 - loss 0.23531547 - samples/sec: 4.48 - lr: 0.000004
2022-07-29 22:46:33,405 ----------------------------------------------------------------------------------------------------
2022-07-29 22:46:33,406 EPOCH 9 done: loss 0.2357 - lr 0.000004
2022-07-29 22:53:05,178 Evaluating as a multi-label problem: False
2022-07-29 22:53:05,233 DEV : loss 0.032372452318668365 - f1-score (micro avg)  0.9719
2022-07-29 22:53:05,558 BAD EPOCHS (no improvement): 4
2022-07-29 22:53:05,564 saving best model
2022-07-29 22:53:29,508 ----------------------------------------------------------------------------------------------------
2022-07-29 22:57:19,982 epoch 10 - iter 270/2703 - loss 0.23933134 - samples/sec: 4.69 - lr: 0.000004
2022-07-29 23:01:24,969 epoch 10 - iter 540/2703 - loss 0.23405803 - samples/sec: 4.41 - lr: 0.000004
2022-07-29 23:05:19,578 epoch 10 - iter 810/2703 - loss 0.23223589 - samples/sec: 4.60 - lr: 0.000004
2022-07-29 23:09:23,168 epoch 10 - iter 1080/2703 - loss 0.23220059 - samples/sec: 4.43 - lr: 0.000004
2022-07-29 23:13:17,962 epoch 10 - iter 1350/2703 - loss 0.23267499 - samples/sec: 4.60 - lr: 0.000004
2022-07-29 23:17:13,150 epoch 10 - iter 1620/2703 - loss 0.23176846 - samples/sec: 4.59 - lr: 0.000004
2022-07-29 23:21:13,909 epoch 10 - iter 1890/2703 - loss 0.23200978 - samples/sec: 4.49 - lr: 0.000004
2022-07-29 23:25:04,100 epoch 10 - iter 2160/2703 - loss 0.23173662 - samples/sec: 4.69 - lr: 0.000004
2022-07-29 23:28:56,161 epoch 10 - iter 2430/2703 - loss 0.23232137 - samples/sec: 4.65 - lr: 0.000004
2022-07-29 23:32:54,709 epoch 10 - iter 2700/2703 - loss 0.23215976 - samples/sec: 4.53 - lr: 0.000004
2022-07-29 23:32:56,830 ----------------------------------------------------------------------------------------------------
2022-07-29 23:32:56,830 EPOCH 10 done: loss 0.2321 - lr 0.000004
2022-07-29 23:39:31,995 Evaluating as a multi-label problem: False
2022-07-29 23:39:32,048 DEV : loss 0.0330529548227787 - f1-score (micro avg)  0.9725
2022-07-29 23:39:32,381 BAD EPOCHS (no improvement): 4
2022-07-29 23:39:32,384 saving best model
2022-07-29 23:39:56,345 ----------------------------------------------------------------------------------------------------
2022-07-29 23:43:56,831 epoch 11 - iter 270/2703 - loss 0.22725722 - samples/sec: 4.49 - lr: 0.000004
2022-07-29 23:47:57,800 epoch 11 - iter 540/2703 - loss 0.23194805 - samples/sec: 4.48 - lr: 0.000004
2022-07-29 23:51:56,102 epoch 11 - iter 810/2703 - loss 0.22843160 - samples/sec: 4.53 - lr: 0.000004
2022-07-29 23:55:55,425 epoch 11 - iter 1080/2703 - loss 0.22984631 - samples/sec: 4.51 - lr: 0.000004
2022-07-29 23:59:45,048 epoch 11 - iter 1350/2703 - loss 0.22873882 - samples/sec: 4.70 - lr: 0.000004
2022-07-30 00:03:45,368 epoch 11 - iter 1620/2703 - loss 0.22950209 - samples/sec: 4.49 - lr: 0.000004
2022-07-30 00:07:35,312 epoch 11 - iter 1890/2703 - loss 0.22836621 - samples/sec: 4.70 - lr: 0.000004
2022-07-30 00:11:27,208 epoch 11 - iter 2160/2703 - loss 0.22902509 - samples/sec: 4.66 - lr: 0.000004
2022-07-30 00:15:19,187 epoch 11 - iter 2430/2703 - loss 0.22997667 - samples/sec: 4.66 - lr: 0.000004
2022-07-30 00:19:19,730 epoch 11 - iter 2700/2703 - loss 0.22970465 - samples/sec: 4.49 - lr: 0.000004
2022-07-30 00:19:22,075 ----------------------------------------------------------------------------------------------------
2022-07-30 00:19:22,075 EPOCH 11 done: loss 0.2297 - lr 0.000004
2022-07-30 00:25:48,782 Evaluating as a multi-label problem: False
2022-07-30 00:25:48,839 DEV : loss 0.03301270306110382 - f1-score (micro avg)  0.9692
2022-07-30 00:25:49,154 BAD EPOCHS (no improvement): 4
2022-07-30 00:25:49,159 ----------------------------------------------------------------------------------------------------
2022-07-30 00:29:40,122 epoch 12 - iter 270/2703 - loss 0.22275375 - samples/sec: 4.68 - lr: 0.000004
2022-07-30 00:33:50,424 epoch 12 - iter 540/2703 - loss 0.22074639 - samples/sec: 4.32 - lr: 0.000004
2022-07-30 00:37:41,437 epoch 12 - iter 810/2703 - loss 0.22763286 - samples/sec: 4.68 - lr: 0.000004
2022-07-30 00:41:32,083 epoch 12 - iter 1080/2703 - loss 0.22620147 - samples/sec: 4.68 - lr: 0.000004
2022-07-30 00:45:27,844 epoch 12 - iter 1350/2703 - loss 0.22386199 - samples/sec: 4.58 - lr: 0.000004
2022-07-30 00:49:24,668 epoch 12 - iter 1620/2703 - loss 0.22530837 - samples/sec: 4.56 - lr: 0.000004
2022-07-30 00:53:21,394 epoch 12 - iter 1890/2703 - loss 0.22664371 - samples/sec: 4.56 - lr: 0.000004
2022-07-30 00:57:13,820 epoch 12 - iter 2160/2703 - loss 0.22884469 - samples/sec: 4.65 - lr: 0.000004
2022-07-30 01:01:12,849 epoch 12 - iter 2430/2703 - loss 0.22956881 - samples/sec: 4.52 - lr: 0.000004
2022-07-30 01:05:06,202 epoch 12 - iter 2700/2703 - loss 0.22926827 - samples/sec: 4.63 - lr: 0.000004
2022-07-30 01:05:08,041 ----------------------------------------------------------------------------------------------------
2022-07-30 01:05:08,041 EPOCH 12 done: loss 0.2293 - lr 0.000004
2022-07-30 01:11:37,648 Evaluating as a multi-label problem: False
2022-07-30 01:11:37,704 DEV : loss 0.032404083758592606 - f1-score (micro avg)  0.973
2022-07-30 01:11:38,029 BAD EPOCHS (no improvement): 4
2022-07-30 01:11:38,035 saving best model
2022-07-30 01:12:02,001 ----------------------------------------------------------------------------------------------------
2022-07-30 01:15:53,876 epoch 13 - iter 270/2703 - loss 0.22405528 - samples/sec: 4.66 - lr: 0.000004
2022-07-30 01:19:45,707 epoch 13 - iter 540/2703 - loss 0.22426985 - samples/sec: 4.66 - lr: 0.000004
2022-07-30 01:23:40,264 epoch 13 - iter 810/2703 - loss 0.22333595 - samples/sec: 4.60 - lr: 0.000004
2022-07-30 01:27:37,258 epoch 13 - iter 1080/2703 - loss 0.22214534 - samples/sec: 4.56 - lr: 0.000004
2022-07-30 01:31:31,472 epoch 13 - iter 1350/2703 - loss 0.22292586 - samples/sec: 4.61 - lr: 0.000004
2022-07-30 01:35:37,772 epoch 13 - iter 1620/2703 - loss 0.22316124 - samples/sec: 4.39 - lr: 0.000004
2022-07-30 01:39:38,822 epoch 13 - iter 1890/2703 - loss 0.22356418 - samples/sec: 4.48 - lr: 0.000004
2022-07-30 01:43:38,379 epoch 13 - iter 2160/2703 - loss 0.22508762 - samples/sec: 4.51 - lr: 0.000004
2022-07-30 01:47:30,213 epoch 13 - iter 2430/2703 - loss 0.22386091 - samples/sec: 4.66 - lr: 0.000004
2022-07-30 01:51:29,549 epoch 13 - iter 2700/2703 - loss 0.22521299 - samples/sec: 4.51 - lr: 0.000004
2022-07-30 01:51:32,579 ----------------------------------------------------------------------------------------------------
2022-07-30 01:51:32,579 EPOCH 13 done: loss 0.2252 - lr 0.000004
2022-07-30 01:58:00,348 Evaluating as a multi-label problem: False
2022-07-30 01:58:00,400 DEV : loss 0.03793816640973091 - f1-score (micro avg)  0.9728
2022-07-30 01:58:00,730 BAD EPOCHS (no improvement): 4
2022-07-30 01:58:00,736 ----------------------------------------------------------------------------------------------------
2022-07-30 02:01:59,846 epoch 14 - iter 270/2703 - loss 0.22040840 - samples/sec: 4.52 - lr: 0.000004
2022-07-30 02:05:53,051 epoch 14 - iter 540/2703 - loss 0.22149533 - samples/sec: 4.63 - lr: 0.000004
2022-07-30 02:09:54,076 epoch 14 - iter 810/2703 - loss 0.22542878 - samples/sec: 4.48 - lr: 0.000004
2022-07-30 02:13:46,188 epoch 14 - iter 1080/2703 - loss 0.22579412 - samples/sec: 4.65 - lr: 0.000004
2022-07-30 02:17:42,528 epoch 14 - iter 1350/2703 - loss 0.22454741 - samples/sec: 4.57 - lr: 0.000004
2022-07-30 02:21:34,871 epoch 14 - iter 1620/2703 - loss 0.22451612 - samples/sec: 4.65 - lr: 0.000004
2022-07-30 02:25:35,979 epoch 14 - iter 1890/2703 - loss 0.22476821 - samples/sec: 4.48 - lr: 0.000004
2022-07-30 02:29:27,008 epoch 14 - iter 2160/2703 - loss 0.22380377 - samples/sec: 4.68 - lr: 0.000004
2022-07-30 02:33:23,991 epoch 14 - iter 2430/2703 - loss 0.22300473 - samples/sec: 4.56 - lr: 0.000004
2022-07-30 02:37:19,380 epoch 14 - iter 2700/2703 - loss 0.22278342 - samples/sec: 4.59 - lr: 0.000004
2022-07-30 02:37:22,473 ----------------------------------------------------------------------------------------------------
2022-07-30 02:37:22,474 EPOCH 14 done: loss 0.2227 - lr 0.000004
2022-07-30 02:43:46,911 Evaluating as a multi-label problem: False
2022-07-30 02:43:46,968 DEV : loss 0.030461257323622704 - f1-score (micro avg)  0.9763
2022-07-30 02:43:47,291 BAD EPOCHS (no improvement): 4
2022-07-30 02:43:47,296 saving best model
2022-07-30 02:44:11,522 ----------------------------------------------------------------------------------------------------
2022-07-30 02:48:06,310 epoch 15 - iter 270/2703 - loss 0.22375325 - samples/sec: 4.60 - lr: 0.000004
2022-07-30 02:52:01,892 epoch 15 - iter 540/2703 - loss 0.22187827 - samples/sec: 4.58 - lr: 0.000004
2022-07-30 02:55:59,987 epoch 15 - iter 810/2703 - loss 0.22368986 - samples/sec: 4.54 - lr: 0.000004
2022-07-30 03:00:00,012 epoch 15 - iter 1080/2703 - loss 0.22490514 - samples/sec: 4.50 - lr: 0.000004
2022-07-30 03:03:53,088 epoch 15 - iter 1350/2703 - loss 0.22434840 - samples/sec: 4.63 - lr: 0.000004
2022-07-30 03:07:43,473 epoch 15 - iter 1620/2703 - loss 0.22231762 - samples/sec: 4.69 - lr: 0.000004
2022-07-30 03:11:41,567 epoch 15 - iter 1890/2703 - loss 0.22357093 - samples/sec: 4.54 - lr: 0.000004
2022-07-30 03:15:34,510 epoch 15 - iter 2160/2703 - loss 0.22391238 - samples/sec: 4.64 - lr: 0.000004
2022-07-30 03:19:35,563 epoch 15 - iter 2430/2703 - loss 0.22333970 - samples/sec: 4.48 - lr: 0.000003
2022-07-30 03:23:36,554 epoch 15 - iter 2700/2703 - loss 0.22357529 - samples/sec: 4.48 - lr: 0.000003
2022-07-30 03:23:39,132 ----------------------------------------------------------------------------------------------------
2022-07-30 03:23:39,133 EPOCH 15 done: loss 0.2236 - lr 0.000003
2022-07-30 03:30:06,905 Evaluating as a multi-label problem: False
2022-07-30 03:30:06,961 DEV : loss 0.03578616678714752 - f1-score (micro avg)  0.9753
2022-07-30 03:30:07,283 BAD EPOCHS (no improvement): 4
2022-07-30 03:30:07,287 ----------------------------------------------------------------------------------------------------
2022-07-30 03:34:12,102 epoch 16 - iter 270/2703 - loss 0.22876040 - samples/sec: 4.41 - lr: 0.000003
2022-07-30 03:38:08,311 epoch 16 - iter 540/2703 - loss 0.22470660 - samples/sec: 4.57 - lr: 0.000003
2022-07-30 03:41:55,988 epoch 16 - iter 810/2703 - loss 0.22441805 - samples/sec: 4.74 - lr: 0.000003
2022-07-30 03:45:54,361 epoch 16 - iter 1080/2703 - loss 0.22418484 - samples/sec: 4.53 - lr: 0.000003
2022-07-30 03:49:45,068 epoch 16 - iter 1350/2703 - loss 0.22599713 - samples/sec: 4.68 - lr: 0.000003
2022-07-30 03:53:47,006 epoch 16 - iter 1620/2703 - loss 0.22709519 - samples/sec: 4.46 - lr: 0.000003
2022-07-30 03:57:47,336 epoch 16 - iter 1890/2703 - loss 0.22682659 - samples/sec: 4.49 - lr: 0.000003
2022-07-30 04:01:42,264 epoch 16 - iter 2160/2703 - loss 0.22668946 - samples/sec: 4.60 - lr: 0.000003
2022-07-30 04:05:40,175 epoch 16 - iter 2430/2703 - loss 0.22535012 - samples/sec: 4.54 - lr: 0.000003
2022-07-30 04:09:44,928 epoch 16 - iter 2700/2703 - loss 0.22418922 - samples/sec: 4.41 - lr: 0.000003
2022-07-30 04:09:47,431 ----------------------------------------------------------------------------------------------------
2022-07-30 04:09:47,431 EPOCH 16 done: loss 0.2242 - lr 0.000003
2022-07-30 04:16:14,044 Evaluating as a multi-label problem: False
2022-07-30 04:16:14,097 DEV : loss 0.03412263095378876 - f1-score (micro avg)  0.9749
2022-07-30 04:16:14,421 BAD EPOCHS (no improvement): 4
2022-07-30 04:16:14,425 ----------------------------------------------------------------------------------------------------
2022-07-30 04:20:12,898 epoch 17 - iter 270/2703 - loss 0.23292476 - samples/sec: 4.53 - lr: 0.000003
2022-07-30 04:24:06,524 epoch 17 - iter 540/2703 - loss 0.22525544 - samples/sec: 4.62 - lr: 0.000003
2022-07-30 04:27:58,862 epoch 17 - iter 810/2703 - loss 0.22455657 - samples/sec: 4.65 - lr: 0.000003
2022-07-30 04:31:49,176 epoch 17 - iter 1080/2703 - loss 0.22354229 - samples/sec: 4.69 - lr: 0.000003
2022-07-30 04:35:46,713 epoch 17 - iter 1350/2703 - loss 0.22291588 - samples/sec: 4.55 - lr: 0.000003
2022-07-30 04:39:52,264 epoch 17 - iter 1620/2703 - loss 0.22214212 - samples/sec: 4.40 - lr: 0.000003
2022-07-30 04:43:49,926 epoch 17 - iter 1890/2703 - loss 0.22358135 - samples/sec: 4.54 - lr: 0.000003
2022-07-30 04:47:42,792 epoch 17 - iter 2160/2703 - loss 0.22363853 - samples/sec: 4.64 - lr: 0.000003
2022-07-30 04:51:34,901 epoch 17 - iter 2430/2703 - loss 0.22302763 - samples/sec: 4.65 - lr: 0.000003
2022-07-30 04:55:32,484 epoch 17 - iter 2700/2703 - loss 0.22252546 - samples/sec: 4.55 - lr: 0.000003
2022-07-30 04:55:35,419 ----------------------------------------------------------------------------------------------------
2022-07-30 04:55:35,419 EPOCH 17 done: loss 0.2224 - lr 0.000003
2022-07-30 05:02:05,253 Evaluating as a multi-label problem: False
2022-07-30 05:02:05,308 DEV : loss 0.036683522164821625 - f1-score (micro avg)  0.9727
2022-07-30 05:02:05,633 BAD EPOCHS (no improvement): 4
2022-07-30 05:02:05,635 ----------------------------------------------------------------------------------------------------
2022-07-30 05:05:56,549 epoch 18 - iter 270/2703 - loss 0.22668114 - samples/sec: 4.68 - lr: 0.000003
2022-07-30 05:10:01,922 epoch 18 - iter 540/2703 - loss 0.22855789 - samples/sec: 4.40 - lr: 0.000003
2022-07-30 05:14:02,486 epoch 18 - iter 810/2703 - loss 0.22605738 - samples/sec: 4.49 - lr: 0.000003
2022-07-30 05:18:05,778 epoch 18 - iter 1080/2703 - loss 0.22540242 - samples/sec: 4.44 - lr: 0.000003
2022-07-30 05:21:54,487 epoch 18 - iter 1350/2703 - loss 0.22454697 - samples/sec: 4.72 - lr: 0.000003
2022-07-30 05:25:43,209 epoch 18 - iter 1620/2703 - loss 0.22290529 - samples/sec: 4.72 - lr: 0.000003
2022-07-30 05:29:43,506 epoch 18 - iter 1890/2703 - loss 0.22320481 - samples/sec: 4.49 - lr: 0.000003
2022-07-30 05:33:44,250 epoch 18 - iter 2160/2703 - loss 0.22317575 - samples/sec: 4.49 - lr: 0.000003
2022-07-30 05:37:36,305 epoch 18 - iter 2430/2703 - loss 0.22341609 - samples/sec: 4.65 - lr: 0.000003
2022-07-30 05:41:30,471 epoch 18 - iter 2700/2703 - loss 0.22351238 - samples/sec: 4.61 - lr: 0.000003
2022-07-30 05:41:33,047 ----------------------------------------------------------------------------------------------------
2022-07-30 05:41:33,047 EPOCH 18 done: loss 0.2236 - lr 0.000003
2022-07-30 05:48:04,229 Evaluating as a multi-label problem: False
2022-07-30 05:48:04,284 DEV : loss 0.037375420331954956 - f1-score (micro avg)  0.9723
2022-07-30 05:48:04,603 BAD EPOCHS (no improvement): 4
2022-07-30 05:48:04,608 ----------------------------------------------------------------------------------------------------
2022-07-30 05:52:06,742 epoch 19 - iter 270/2703 - loss 0.22167510 - samples/sec: 4.46 - lr: 0.000003
2022-07-30 05:56:03,449 epoch 19 - iter 540/2703 - loss 0.22002020 - samples/sec: 4.56 - lr: 0.000003
2022-07-30 05:59:58,644 epoch 19 - iter 810/2703 - loss 0.21489356 - samples/sec: 4.59 - lr: 0.000003
2022-07-30 06:03:58,532 epoch 19 - iter 1080/2703 - loss 0.21443772 - samples/sec: 4.50 - lr: 0.000003
2022-07-30 06:07:53,613 epoch 19 - iter 1350/2703 - loss 0.21443382 - samples/sec: 4.59 - lr: 0.000003
2022-07-30 06:11:49,246 epoch 19 - iter 1620/2703 - loss 0.21447865 - samples/sec: 4.58 - lr: 0.000003
2022-07-30 06:15:42,161 epoch 19 - iter 1890/2703 - loss 0.21572952 - samples/sec: 4.64 - lr: 0.000003
2022-07-30 06:19:37,416 epoch 19 - iter 2160/2703 - loss 0.21536505 - samples/sec: 4.59 - lr: 0.000003
2022-07-30 06:23:30,624 epoch 19 - iter 2430/2703 - loss 0.21374536 - samples/sec: 4.63 - lr: 0.000003
2022-07-30 06:27:31,417 epoch 19 - iter 2700/2703 - loss 0.21489617 - samples/sec: 4.49 - lr: 0.000003
2022-07-30 06:27:33,546 ----------------------------------------------------------------------------------------------------
2022-07-30 06:27:33,547 EPOCH 19 done: loss 0.2149 - lr 0.000003
2022-07-30 06:34:04,267 Evaluating as a multi-label problem: False
2022-07-30 06:34:04,316 DEV : loss 0.0369076170027256 - f1-score (micro avg)  0.9729
2022-07-30 06:34:04,641 BAD EPOCHS (no improvement): 4
2022-07-30 06:34:04,647 ----------------------------------------------------------------------------------------------------
2022-07-30 06:38:09,310 epoch 20 - iter 270/2703 - loss 0.21162789 - samples/sec: 4.41 - lr: 0.000003
2022-07-30 06:42:03,335 epoch 20 - iter 540/2703 - loss 0.21392496 - samples/sec: 4.62 - lr: 0.000003
2022-07-30 06:46:01,584 epoch 20 - iter 810/2703 - loss 0.21408667 - samples/sec: 4.53 - lr: 0.000003
2022-07-30 06:50:02,899 epoch 20 - iter 1080/2703 - loss 0.21743406 - samples/sec: 4.48 - lr: 0.000003
2022-07-30 06:53:58,493 epoch 20 - iter 1350/2703 - loss 0.21638849 - samples/sec: 4.58 - lr: 0.000003
2022-07-30 06:57:50,833 epoch 20 - iter 1620/2703 - loss 0.21556192 - samples/sec: 4.65 - lr: 0.000003
2022-07-30 07:01:45,355 epoch 20 - iter 1890/2703 - loss 0.21591190 - samples/sec: 4.61 - lr: 0.000003
2022-07-30 07:05:42,137 epoch 20 - iter 2160/2703 - loss 0.21570337 - samples/sec: 4.56 - lr: 0.000003
2022-07-30 07:09:42,679 epoch 20 - iter 2430/2703 - loss 0.21580913 - samples/sec: 4.49 - lr: 0.000003
2022-07-30 07:13:42,735 epoch 20 - iter 2700/2703 - loss 0.21555873 - samples/sec: 4.50 - lr: 0.000003
2022-07-30 07:13:45,068 ----------------------------------------------------------------------------------------------------
2022-07-30 07:13:45,068 EPOCH 20 done: loss 0.2155 - lr 0.000003
2022-07-30 07:20:15,322 Evaluating as a multi-label problem: False
2022-07-30 07:20:15,376 DEV : loss 0.03737499192357063 - f1-score (micro avg)  0.9762
2022-07-30 07:20:15,702 BAD EPOCHS (no improvement): 4
2022-07-30 07:20:15,705 ----------------------------------------------------------------------------------------------------
2022-07-30 07:24:11,899 epoch 21 - iter 270/2703 - loss 0.21910439 - samples/sec: 4.57 - lr: 0.000003
2022-07-30 07:28:05,238 epoch 21 - iter 540/2703 - loss 0.21658186 - samples/sec: 4.63 - lr: 0.000003
2022-07-30 07:32:06,117 epoch 21 - iter 810/2703 - loss 0.21819139 - samples/sec: 4.48 - lr: 0.000003
2022-07-30 07:35:59,750 epoch 21 - iter 1080/2703 - loss 0.21917535 - samples/sec: 4.62 - lr: 0.000003
2022-07-30 07:40:03,313 epoch 21 - iter 1350/2703 - loss 0.21757493 - samples/sec: 4.43 - lr: 0.000003
2022-07-30 07:44:05,693 epoch 21 - iter 1620/2703 - loss 0.21678942 - samples/sec: 4.46 - lr: 0.000003
2022-07-30 07:47:57,571 epoch 21 - iter 1890/2703 - loss 0.21745186 - samples/sec: 4.66 - lr: 0.000003
2022-07-30 07:51:51,036 epoch 21 - iter 2160/2703 - loss 0.21685581 - samples/sec: 4.63 - lr: 0.000003
2022-07-30 07:55:52,900 epoch 21 - iter 2430/2703 - loss 0.21602469 - samples/sec: 4.47 - lr: 0.000003
2022-07-30 07:59:50,700 epoch 21 - iter 2700/2703 - loss 0.21698771 - samples/sec: 4.54 - lr: 0.000003
2022-07-30 07:59:52,726 ----------------------------------------------------------------------------------------------------
2022-07-30 07:59:52,726 EPOCH 21 done: loss 0.2170 - lr 0.000003
2022-07-30 08:06:19,406 Evaluating as a multi-label problem: False
2022-07-30 08:06:19,461 DEV : loss 0.03849078714847565 - f1-score (micro avg)  0.9759
2022-07-30 08:06:19,776 BAD EPOCHS (no improvement): 4
2022-07-30 08:06:19,781 ----------------------------------------------------------------------------------------------------
2022-07-30 08:10:21,569 epoch 22 - iter 270/2703 - loss 0.21217118 - samples/sec: 4.47 - lr: 0.000003
2022-07-30 08:14:21,127 epoch 22 - iter 540/2703 - loss 0.21438040 - samples/sec: 4.51 - lr: 0.000003
2022-07-30 08:18:16,789 epoch 22 - iter 810/2703 - loss 0.21825200 - samples/sec: 4.58 - lr: 0.000003
2022-07-30 08:22:21,454 epoch 22 - iter 1080/2703 - loss 0.22059132 - samples/sec: 4.41 - lr: 0.000003
2022-07-30 08:26:11,198 epoch 22 - iter 1350/2703 - loss 0.21884605 - samples/sec: 4.70 - lr: 0.000003
2022-07-30 08:30:00,836 epoch 22 - iter 1620/2703 - loss 0.21905600 - samples/sec: 4.70 - lr: 0.000003
2022-07-30 08:33:55,623 epoch 22 - iter 1890/2703 - loss 0.21932742 - samples/sec: 4.60 - lr: 0.000003
2022-07-30 08:37:50,416 epoch 22 - iter 2160/2703 - loss 0.21866086 - samples/sec: 4.60 - lr: 0.000003
2022-07-30 08:41:40,850 epoch 22 - iter 2430/2703 - loss 0.21778376 - samples/sec: 4.69 - lr: 0.000003
2022-07-30 08:45:37,930 epoch 22 - iter 2700/2703 - loss 0.21709398 - samples/sec: 4.56 - lr: 0.000003
2022-07-30 08:45:39,895 ----------------------------------------------------------------------------------------------------
2022-07-30 08:45:39,895 EPOCH 22 done: loss 0.2171 - lr 0.000003
2022-07-30 08:52:07,311 Evaluating as a multi-label problem: False
2022-07-30 08:52:07,361 DEV : loss 0.03637803718447685 - f1-score (micro avg)  0.9766
2022-07-30 08:52:07,683 BAD EPOCHS (no improvement): 4
2022-07-30 08:52:07,687 saving best model
2022-07-30 08:52:29,619 ----------------------------------------------------------------------------------------------------
2022-07-30 08:56:28,033 epoch 23 - iter 270/2703 - loss 0.22188713 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 09:00:18,835 epoch 23 - iter 540/2703 - loss 0.21612492 - samples/sec: 4.68 - lr: 0.000002
2022-07-30 09:04:17,687 epoch 23 - iter 810/2703 - loss 0.21830791 - samples/sec: 4.52 - lr: 0.000002
2022-07-30 09:08:21,002 epoch 23 - iter 1080/2703 - loss 0.21760495 - samples/sec: 4.44 - lr: 0.000002
2022-07-30 09:12:14,278 epoch 23 - iter 1350/2703 - loss 0.21704606 - samples/sec: 4.63 - lr: 0.000002
2022-07-30 09:16:14,312 epoch 23 - iter 1620/2703 - loss 0.21803253 - samples/sec: 4.50 - lr: 0.000002
2022-07-30 09:20:16,042 epoch 23 - iter 1890/2703 - loss 0.21627347 - samples/sec: 4.47 - lr: 0.000002
2022-07-30 09:24:05,332 epoch 23 - iter 2160/2703 - loss 0.21570538 - samples/sec: 4.71 - lr: 0.000002
2022-07-30 09:28:02,513 epoch 23 - iter 2430/2703 - loss 0.21622123 - samples/sec: 4.55 - lr: 0.000002
2022-07-30 09:31:55,701 epoch 23 - iter 2700/2703 - loss 0.21605790 - samples/sec: 4.63 - lr: 0.000002
2022-07-30 09:31:57,511 ----------------------------------------------------------------------------------------------------
2022-07-30 09:31:57,511 EPOCH 23 done: loss 0.2161 - lr 0.000002
2022-07-30 09:38:33,466 Evaluating as a multi-label problem: False
2022-07-30 09:38:33,521 DEV : loss 0.03900418058037758 - f1-score (micro avg)  0.9779
2022-07-30 09:38:33,849 BAD EPOCHS (no improvement): 4
2022-07-30 09:38:33,851 saving best model
2022-07-30 09:38:56,116 ----------------------------------------------------------------------------------------------------
2022-07-30 09:42:52,408 epoch 24 - iter 270/2703 - loss 0.21415194 - samples/sec: 4.57 - lr: 0.000002
2022-07-30 09:46:49,869 epoch 24 - iter 540/2703 - loss 0.21702369 - samples/sec: 4.55 - lr: 0.000002
2022-07-30 09:50:49,469 epoch 24 - iter 810/2703 - loss 0.21683803 - samples/sec: 4.51 - lr: 0.000002
2022-07-30 09:54:41,724 epoch 24 - iter 1080/2703 - loss 0.21708383 - samples/sec: 4.65 - lr: 0.000002
2022-07-30 09:58:33,073 epoch 24 - iter 1350/2703 - loss 0.21757010 - samples/sec: 4.67 - lr: 0.000002
2022-07-30 10:02:35,569 epoch 24 - iter 1620/2703 - loss 0.21681316 - samples/sec: 4.45 - lr: 0.000002
2022-07-30 10:06:28,181 epoch 24 - iter 1890/2703 - loss 0.21664058 - samples/sec: 4.64 - lr: 0.000002
2022-07-30 10:10:25,245 epoch 24 - iter 2160/2703 - loss 0.21742728 - samples/sec: 4.56 - lr: 0.000002
2022-07-30 10:14:31,315 epoch 24 - iter 2430/2703 - loss 0.21653046 - samples/sec: 4.39 - lr: 0.000002
2022-07-30 10:18:29,102 epoch 24 - iter 2700/2703 - loss 0.21614116 - samples/sec: 4.54 - lr: 0.000002
2022-07-30 10:18:31,105 ----------------------------------------------------------------------------------------------------
2022-07-30 10:18:31,105 EPOCH 24 done: loss 0.2161 - lr 0.000002
2022-07-30 10:25:00,242 Evaluating as a multi-label problem: False
2022-07-30 10:25:00,298 DEV : loss 0.037795230746269226 - f1-score (micro avg)  0.9754
2022-07-30 10:25:00,619 BAD EPOCHS (no improvement): 4
2022-07-30 10:25:00,623 ----------------------------------------------------------------------------------------------------
2022-07-30 10:29:01,675 epoch 25 - iter 270/2703 - loss 0.21845351 - samples/sec: 4.48 - lr: 0.000002
2022-07-30 10:32:58,544 epoch 25 - iter 540/2703 - loss 0.21479768 - samples/sec: 4.56 - lr: 0.000002
2022-07-30 10:36:58,875 epoch 25 - iter 810/2703 - loss 0.21597713 - samples/sec: 4.49 - lr: 0.000002
2022-07-30 10:40:57,063 epoch 25 - iter 1080/2703 - loss 0.21688462 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 10:44:44,236 epoch 25 - iter 1350/2703 - loss 0.21797953 - samples/sec: 4.75 - lr: 0.000002
2022-07-30 10:48:38,603 epoch 25 - iter 1620/2703 - loss 0.21780567 - samples/sec: 4.61 - lr: 0.000002
2022-07-30 10:52:40,296 epoch 25 - iter 1890/2703 - loss 0.21642458 - samples/sec: 4.47 - lr: 0.000002
2022-07-30 10:56:31,202 epoch 25 - iter 2160/2703 - loss 0.21499342 - samples/sec: 4.68 - lr: 0.000002
2022-07-30 11:00:22,174 epoch 25 - iter 2430/2703 - loss 0.21469389 - samples/sec: 4.68 - lr: 0.000002
2022-07-30 11:04:24,357 epoch 25 - iter 2700/2703 - loss 0.21417629 - samples/sec: 4.46 - lr: 0.000002
2022-07-30 11:04:26,585 ----------------------------------------------------------------------------------------------------
2022-07-30 11:04:26,585 EPOCH 25 done: loss 0.2142 - lr 0.000002
2022-07-30 11:10:54,902 Evaluating as a multi-label problem: False
2022-07-30 11:10:54,953 DEV : loss 0.04127389192581177 - f1-score (micro avg)  0.9752
2022-07-30 11:10:55,278 BAD EPOCHS (no improvement): 4
2022-07-30 11:10:55,282 ----------------------------------------------------------------------------------------------------
2022-07-30 11:14:47,557 epoch 26 - iter 270/2703 - loss 0.20838756 - samples/sec: 4.65 - lr: 0.000002
2022-07-30 11:18:49,740 epoch 26 - iter 540/2703 - loss 0.21349946 - samples/sec: 4.46 - lr: 0.000002
2022-07-30 11:22:48,074 epoch 26 - iter 810/2703 - loss 0.21979272 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 11:26:45,422 epoch 26 - iter 1080/2703 - loss 0.21773751 - samples/sec: 4.55 - lr: 0.000002
2022-07-30 11:30:39,967 epoch 26 - iter 1350/2703 - loss 0.21798859 - samples/sec: 4.60 - lr: 0.000002
2022-07-30 11:34:33,254 epoch 26 - iter 1620/2703 - loss 0.21724899 - samples/sec: 4.63 - lr: 0.000002
2022-07-30 11:38:25,821 epoch 26 - iter 1890/2703 - loss 0.21684115 - samples/sec: 4.64 - lr: 0.000002
2022-07-30 11:42:24,079 epoch 26 - iter 2160/2703 - loss 0.21555797 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 11:46:25,635 epoch 26 - iter 2430/2703 - loss 0.21596271 - samples/sec: 4.47 - lr: 0.000002
2022-07-30 11:50:27,490 epoch 26 - iter 2700/2703 - loss 0.21642170 - samples/sec: 4.47 - lr: 0.000002
2022-07-30 11:50:30,008 ----------------------------------------------------------------------------------------------------
2022-07-30 11:50:30,009 EPOCH 26 done: loss 0.2165 - lr 0.000002
2022-07-30 11:57:02,340 Evaluating as a multi-label problem: False
2022-07-30 11:57:02,395 DEV : loss 0.037837568670511246 - f1-score (micro avg)  0.9759
2022-07-30 11:57:02,718 BAD EPOCHS (no improvement): 4
2022-07-30 11:57:02,721 ----------------------------------------------------------------------------------------------------
2022-07-30 12:01:05,096 epoch 27 - iter 270/2703 - loss 0.22183533 - samples/sec: 4.46 - lr: 0.000002
2022-07-30 12:05:03,398 epoch 27 - iter 540/2703 - loss 0.21795482 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 12:09:04,081 epoch 27 - iter 810/2703 - loss 0.21268106 - samples/sec: 4.49 - lr: 0.000002
2022-07-30 12:12:48,822 epoch 27 - iter 1080/2703 - loss 0.21291381 - samples/sec: 4.81 - lr: 0.000002
2022-07-30 12:16:49,420 epoch 27 - iter 1350/2703 - loss 0.21032396 - samples/sec: 4.49 - lr: 0.000002
2022-07-30 12:20:47,671 epoch 27 - iter 1620/2703 - loss 0.21178632 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 12:24:47,204 epoch 27 - iter 1890/2703 - loss 0.21259912 - samples/sec: 4.51 - lr: 0.000002
2022-07-30 12:28:43,725 epoch 27 - iter 2160/2703 - loss 0.21308477 - samples/sec: 4.57 - lr: 0.000002
2022-07-30 12:32:36,970 epoch 27 - iter 2430/2703 - loss 0.21292055 - samples/sec: 4.63 - lr: 0.000002
2022-07-30 12:36:42,304 epoch 27 - iter 2700/2703 - loss 0.21322793 - samples/sec: 4.40 - lr: 0.000002
2022-07-30 12:36:44,607 ----------------------------------------------------------------------------------------------------
2022-07-30 12:36:44,607 EPOCH 27 done: loss 0.2132 - lr 0.000002
2022-07-30 12:43:12,393 Evaluating as a multi-label problem: False
2022-07-30 12:43:12,445 DEV : loss 0.043128035962581635 - f1-score (micro avg)  0.9757
2022-07-30 12:43:12,761 BAD EPOCHS (no improvement): 4
2022-07-30 12:43:12,766 ----------------------------------------------------------------------------------------------------
2022-07-30 12:47:11,618 epoch 28 - iter 270/2703 - loss 0.21009771 - samples/sec: 4.52 - lr: 0.000002
2022-07-30 12:51:15,568 epoch 28 - iter 540/2703 - loss 0.21322697 - samples/sec: 4.43 - lr: 0.000002
2022-07-30 12:55:14,024 epoch 28 - iter 810/2703 - loss 0.20881223 - samples/sec: 4.53 - lr: 0.000002
2022-07-30 12:59:10,164 epoch 28 - iter 1080/2703 - loss 0.20949901 - samples/sec: 4.57 - lr: 0.000002
2022-07-30 13:03:09,825 epoch 28 - iter 1350/2703 - loss 0.21006188 - samples/sec: 4.51 - lr: 0.000002
2022-07-30 13:07:07,694 epoch 28 - iter 1620/2703 - loss 0.20887364 - samples/sec: 4.54 - lr: 0.000002
2022-07-30 13:11:00,424 epoch 28 - iter 1890/2703 - loss 0.21078979 - samples/sec: 4.64 - lr: 0.000002
2022-07-30 13:14:57,980 epoch 28 - iter 2160/2703 - loss 0.21007522 - samples/sec: 4.55 - lr: 0.000002
2022-07-30 13:18:53,723 epoch 28 - iter 2430/2703 - loss 0.21138082 - samples/sec: 4.58 - lr: 0.000002
2022-07-30 13:22:46,744 epoch 28 - iter 2700/2703 - loss 0.21117607 - samples/sec: 4.64 - lr: 0.000002
2022-07-30 13:22:49,639 ----------------------------------------------------------------------------------------------------
2022-07-30 13:22:49,639 EPOCH 28 done: loss 0.2111 - lr 0.000002
2022-07-30 13:29:16,600 Evaluating as a multi-label problem: False
2022-07-30 13:29:16,650 DEV : loss 0.04205244034528732 - f1-score (micro avg)  0.9767
2022-07-30 13:29:16,972 BAD EPOCHS (no improvement): 4
2022-07-30 13:29:16,983 ----------------------------------------------------------------------------------------------------
2022-07-30 13:33:10,004 epoch 29 - iter 270/2703 - loss 0.21173841 - samples/sec: 4.64 - lr: 0.000002
2022-07-30 13:37:06,628 epoch 29 - iter 540/2703 - loss 0.21056128 - samples/sec: 4.56 - lr: 0.000002
2022-07-30 13:41:01,985 epoch 29 - iter 810/2703 - loss 0.21241191 - samples/sec: 4.59 - lr: 0.000002
2022-07-30 13:45:01,304 epoch 29 - iter 1080/2703 - loss 0.21175904 - samples/sec: 4.51 - lr: 0.000002
2022-07-30 13:49:02,557 epoch 29 - iter 1350/2703 - loss 0.21143308 - samples/sec: 4.48 - lr: 0.000002
2022-07-30 13:52:54,255 epoch 29 - iter 1620/2703 - loss 0.21335824 - samples/sec: 4.66 - lr: 0.000002
2022-07-30 13:56:48,624 epoch 29 - iter 1890/2703 - loss 0.21335095 - samples/sec: 4.61 - lr: 0.000002
2022-07-30 14:00:46,595 epoch 29 - iter 2160/2703 - loss 0.21338671 - samples/sec: 4.54 - lr: 0.000002
2022-07-30 14:04:43,273 epoch 29 - iter 2430/2703 - loss 0.21367470 - samples/sec: 4.56 - lr: 0.000002
2022-07-30 14:08:44,067 epoch 29 - iter 2700/2703 - loss 0.21328976 - samples/sec: 4.49 - lr: 0.000002
2022-07-30 14:08:47,932 ----------------------------------------------------------------------------------------------------
2022-07-30 14:08:47,932 EPOCH 29 done: loss 0.2134 - lr 0.000002
2022-07-30 14:15:21,029 Evaluating as a multi-label problem: False
2022-07-30 14:15:21,084 DEV : loss 0.03699250519275665 - f1-score (micro avg)  0.9774
2022-07-30 14:15:21,412 BAD EPOCHS (no improvement): 4
2022-07-30 14:15:21,415 ----------------------------------------------------------------------------------------------------
2022-07-30 14:19:15,558 epoch 30 - iter 270/2703 - loss 0.20843842 - samples/sec: 4.61 - lr: 0.000002
2022-07-30 14:23:20,077 epoch 30 - iter 540/2703 - loss 0.20715488 - samples/sec: 4.42 - lr: 0.000002
2022-07-30 14:27:21,747 epoch 30 - iter 810/2703 - loss 0.20974964 - samples/sec: 4.47 - lr: 0.000001
2022-07-30 14:31:12,864 epoch 30 - iter 1080/2703 - loss 0.20990087 - samples/sec: 4.67 - lr: 0.000001
2022-07-30 14:35:08,708 epoch 30 - iter 1350/2703 - loss 0.20946676 - samples/sec: 4.58 - lr: 0.000001
2022-07-30 14:39:10,210 epoch 30 - iter 1620/2703 - loss 0.20995187 - samples/sec: 4.47 - lr: 0.000001
2022-07-30 14:43:10,162 epoch 30 - iter 1890/2703 - loss 0.20953863 - samples/sec: 4.50 - lr: 0.000001
2022-07-30 14:46:59,173 epoch 30 - iter 2160/2703 - loss 0.20985845 - samples/sec: 4.72 - lr: 0.000001
2022-07-30 14:50:58,747 epoch 30 - iter 2430/2703 - loss 0.21001641 - samples/sec: 4.51 - lr: 0.000001
2022-07-30 14:54:54,178 epoch 30 - iter 2700/2703 - loss 0.20865755 - samples/sec: 4.59 - lr: 0.000001
2022-07-30 14:54:56,566 ----------------------------------------------------------------------------------------------------
2022-07-30 14:54:56,566 EPOCH 30 done: loss 0.2086 - lr 0.000001
2022-07-30 15:01:27,105 Evaluating as a multi-label problem: False
2022-07-30 15:01:27,157 DEV : loss 0.04088957980275154 - f1-score (micro avg)  0.9765
2022-07-30 15:01:27,474 BAD EPOCHS (no improvement): 4
2022-07-30 15:01:27,480 ----------------------------------------------------------------------------------------------------
2022-07-30 15:05:20,517 epoch 31 - iter 270/2703 - loss 0.20503236 - samples/sec: 4.63 - lr: 0.000001
2022-07-30 15:09:20,824 epoch 31 - iter 540/2703 - loss 0.20524610 - samples/sec: 4.49 - lr: 0.000001
2022-07-30 15:13:23,436 epoch 31 - iter 810/2703 - loss 0.20700548 - samples/sec: 4.45 - lr: 0.000001
2022-07-30 15:17:18,096 epoch 31 - iter 1080/2703 - loss 0.20775541 - samples/sec: 4.60 - lr: 0.000001
2022-07-30 15:21:10,173 epoch 31 - iter 1350/2703 - loss 0.20952340 - samples/sec: 4.65 - lr: 0.000001
2022-07-30 15:25:01,512 epoch 31 - iter 1620/2703 - loss 0.20884779 - samples/sec: 4.67 - lr: 0.000001
2022-07-30 15:28:59,460 epoch 31 - iter 1890/2703 - loss 0.20843836 - samples/sec: 4.54 - lr: 0.000001
2022-07-30 15:33:01,001 epoch 31 - iter 2160/2703 - loss 0.20785981 - samples/sec: 4.47 - lr: 0.000001
2022-07-30 15:36:57,704 epoch 31 - iter 2430/2703 - loss 0.20850453 - samples/sec: 4.56 - lr: 0.000001
2022-07-30 15:41:00,060 epoch 31 - iter 2700/2703 - loss 0.20890301 - samples/sec: 4.46 - lr: 0.000001
2022-07-30 15:41:02,515 ----------------------------------------------------------------------------------------------------
2022-07-30 15:41:02,515 EPOCH 31 done: loss 0.2089 - lr 0.000001
2022-07-30 15:47:38,992 Evaluating as a multi-label problem: False
2022-07-30 15:47:39,044 DEV : loss 0.03810834884643555 - f1-score (micro avg)  0.9771
2022-07-30 15:47:39,369 BAD EPOCHS (no improvement): 4
2022-07-30 15:47:39,376 ----------------------------------------------------------------------------------------------------
2022-07-30 15:51:33,347 epoch 32 - iter 270/2703 - loss 0.20040812 - samples/sec: 4.62 - lr: 0.000001
2022-07-30 15:55:34,402 epoch 32 - iter 540/2703 - loss 0.21078439 - samples/sec: 4.48 - lr: 0.000001
2022-07-30 15:59:26,965 epoch 32 - iter 810/2703 - loss 0.21039663 - samples/sec: 4.64 - lr: 0.000001
2022-07-30 16:03:21,363 epoch 32 - iter 1080/2703 - loss 0.21384415 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 16:07:19,081 epoch 32 - iter 1350/2703 - loss 0.21450720 - samples/sec: 4.54 - lr: 0.000001
2022-07-30 16:11:13,361 epoch 32 - iter 1620/2703 - loss 0.21400831 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 16:15:11,832 epoch 32 - iter 1890/2703 - loss 0.21266932 - samples/sec: 4.53 - lr: 0.000001
2022-07-30 16:19:09,945 epoch 32 - iter 2160/2703 - loss 0.21319284 - samples/sec: 4.54 - lr: 0.000001
2022-07-30 16:23:06,522 epoch 32 - iter 2430/2703 - loss 0.21265886 - samples/sec: 4.57 - lr: 0.000001
2022-07-30 16:27:06,421 epoch 32 - iter 2700/2703 - loss 0.21254952 - samples/sec: 4.50 - lr: 0.000001
2022-07-30 16:27:08,409 ----------------------------------------------------------------------------------------------------
2022-07-30 16:27:08,409 EPOCH 32 done: loss 0.2125 - lr 0.000001
2022-07-30 16:33:34,238 Evaluating as a multi-label problem: False
2022-07-30 16:33:34,291 DEV : loss 0.04142734780907631 - f1-score (micro avg)  0.9753
2022-07-30 16:33:34,612 BAD EPOCHS (no improvement): 4
2022-07-30 16:33:34,615 ----------------------------------------------------------------------------------------------------
2022-07-30 16:37:33,971 epoch 33 - iter 270/2703 - loss 0.20295491 - samples/sec: 4.51 - lr: 0.000001
2022-07-30 16:41:27,306 epoch 33 - iter 540/2703 - loss 0.20532409 - samples/sec: 4.63 - lr: 0.000001
2022-07-30 16:45:28,590 epoch 33 - iter 810/2703 - loss 0.20532577 - samples/sec: 4.48 - lr: 0.000001
2022-07-30 16:49:31,953 epoch 33 - iter 1080/2703 - loss 0.20818835 - samples/sec: 4.44 - lr: 0.000001
2022-07-30 16:53:32,769 epoch 33 - iter 1350/2703 - loss 0.20897042 - samples/sec: 4.49 - lr: 0.000001
2022-07-30 16:57:24,122 epoch 33 - iter 1620/2703 - loss 0.20892733 - samples/sec: 4.67 - lr: 0.000001
2022-07-30 17:01:23,093 epoch 33 - iter 1890/2703 - loss 0.20890339 - samples/sec: 4.52 - lr: 0.000001
2022-07-30 17:05:19,810 epoch 33 - iter 2160/2703 - loss 0.21012518 - samples/sec: 4.56 - lr: 0.000001
2022-07-30 17:09:10,944 epoch 33 - iter 2430/2703 - loss 0.21081037 - samples/sec: 4.67 - lr: 0.000001
2022-07-30 17:13:06,731 epoch 33 - iter 2700/2703 - loss 0.21088880 - samples/sec: 4.58 - lr: 0.000001
2022-07-30 17:13:09,613 ----------------------------------------------------------------------------------------------------
2022-07-30 17:13:09,614 EPOCH 33 done: loss 0.2110 - lr 0.000001
2022-07-30 17:19:44,842 Evaluating as a multi-label problem: False
2022-07-30 17:19:44,892 DEV : loss 0.043718256056308746 - f1-score (micro avg)  0.9749
2022-07-30 17:19:45,211 BAD EPOCHS (no improvement): 4
2022-07-30 17:19:45,218 ----------------------------------------------------------------------------------------------------
2022-07-30 17:23:47,343 epoch 34 - iter 270/2703 - loss 0.21801505 - samples/sec: 4.46 - lr: 0.000001
2022-07-30 17:27:47,396 epoch 34 - iter 540/2703 - loss 0.21313078 - samples/sec: 4.50 - lr: 0.000001
2022-07-30 17:31:41,585 epoch 34 - iter 810/2703 - loss 0.21216720 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 17:35:38,869 epoch 34 - iter 1080/2703 - loss 0.21423700 - samples/sec: 4.55 - lr: 0.000001
2022-07-30 17:39:32,934 epoch 34 - iter 1350/2703 - loss 0.21448644 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 17:43:26,567 epoch 34 - iter 1620/2703 - loss 0.21263116 - samples/sec: 4.62 - lr: 0.000001
2022-07-30 17:47:20,828 epoch 34 - iter 1890/2703 - loss 0.21251743 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 17:51:10,790 epoch 34 - iter 2160/2703 - loss 0.21278083 - samples/sec: 4.70 - lr: 0.000001
2022-07-30 17:55:01,440 epoch 34 - iter 2430/2703 - loss 0.21349239 - samples/sec: 4.68 - lr: 0.000001
2022-07-30 17:58:55,823 epoch 34 - iter 2700/2703 - loss 0.21314144 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 17:58:57,474 ----------------------------------------------------------------------------------------------------
2022-07-30 17:58:57,474 EPOCH 34 done: loss 0.2131 - lr 0.000001
2022-07-30 18:05:30,622 Evaluating as a multi-label problem: False
2022-07-30 18:05:30,673 DEV : loss 0.04247133061289787 - f1-score (micro avg)  0.9754
2022-07-30 18:05:31,002 BAD EPOCHS (no improvement): 4
2022-07-30 18:05:31,006 ----------------------------------------------------------------------------------------------------
2022-07-30 18:09:32,380 epoch 35 - iter 270/2703 - loss 0.20173595 - samples/sec: 4.47 - lr: 0.000001
2022-07-30 18:13:25,668 epoch 35 - iter 540/2703 - loss 0.20659481 - samples/sec: 4.63 - lr: 0.000001
2022-07-30 18:17:20,347 epoch 35 - iter 810/2703 - loss 0.21039001 - samples/sec: 4.60 - lr: 0.000001
2022-07-30 18:21:08,067 epoch 35 - iter 1080/2703 - loss 0.21228061 - samples/sec: 4.74 - lr: 0.000001
2022-07-30 18:25:08,051 epoch 35 - iter 1350/2703 - loss 0.21101706 - samples/sec: 4.50 - lr: 0.000001
2022-07-30 18:29:08,108 epoch 35 - iter 1620/2703 - loss 0.21195508 - samples/sec: 4.50 - lr: 0.000001
2022-07-30 18:33:09,328 epoch 35 - iter 1890/2703 - loss 0.21039749 - samples/sec: 4.48 - lr: 0.000001
2022-07-30 18:37:13,379 epoch 35 - iter 2160/2703 - loss 0.21110933 - samples/sec: 4.43 - lr: 0.000001
2022-07-30 18:41:17,339 epoch 35 - iter 2430/2703 - loss 0.21092591 - samples/sec: 4.43 - lr: 0.000001
2022-07-30 18:45:09,711 epoch 35 - iter 2700/2703 - loss 0.21080717 - samples/sec: 4.65 - lr: 0.000001
2022-07-30 18:45:12,023 ----------------------------------------------------------------------------------------------------
2022-07-30 18:45:12,023 EPOCH 35 done: loss 0.2109 - lr 0.000001
2022-07-30 18:51:42,744 Evaluating as a multi-label problem: False
2022-07-30 18:51:42,796 DEV : loss 0.04272079095244408 - f1-score (micro avg)  0.9765
2022-07-30 18:51:43,114 BAD EPOCHS (no improvement): 4
2022-07-30 18:51:43,121 ----------------------------------------------------------------------------------------------------
2022-07-30 18:55:44,088 epoch 36 - iter 270/2703 - loss 0.21380500 - samples/sec: 4.48 - lr: 0.000001
2022-07-30 18:59:39,358 epoch 36 - iter 540/2703 - loss 0.21182168 - samples/sec: 4.59 - lr: 0.000001
2022-07-30 19:03:34,756 epoch 36 - iter 810/2703 - loss 0.21051072 - samples/sec: 4.59 - lr: 0.000001
2022-07-30 19:07:33,342 epoch 36 - iter 1080/2703 - loss 0.21093559 - samples/sec: 4.53 - lr: 0.000001
2022-07-30 19:11:32,242 epoch 36 - iter 1350/2703 - loss 0.21096331 - samples/sec: 4.52 - lr: 0.000001
2022-07-30 19:15:14,403 epoch 36 - iter 1620/2703 - loss 0.21179404 - samples/sec: 4.86 - lr: 0.000001
2022-07-30 19:19:11,295 epoch 36 - iter 1890/2703 - loss 0.21349164 - samples/sec: 4.56 - lr: 0.000001
2022-07-30 19:23:06,890 epoch 36 - iter 2160/2703 - loss 0.21276134 - samples/sec: 4.58 - lr: 0.000001
2022-07-30 19:27:05,422 epoch 36 - iter 2430/2703 - loss 0.21308370 - samples/sec: 4.53 - lr: 0.000001
2022-07-30 19:31:02,944 epoch 36 - iter 2700/2703 - loss 0.21247675 - samples/sec: 4.55 - lr: 0.000001
2022-07-30 19:31:05,129 ----------------------------------------------------------------------------------------------------
2022-07-30 19:31:05,129 EPOCH 36 done: loss 0.2125 - lr 0.000001
2022-07-30 19:37:43,058 Evaluating as a multi-label problem: False
2022-07-30 19:37:43,111 DEV : loss 0.042595215141773224 - f1-score (micro avg)  0.9767
2022-07-30 19:37:43,429 BAD EPOCHS (no improvement): 4
2022-07-30 19:37:43,438 ----------------------------------------------------------------------------------------------------
2022-07-30 19:41:37,987 epoch 37 - iter 270/2703 - loss 0.21516245 - samples/sec: 4.61 - lr: 0.000001
2022-07-30 19:45:35,204 epoch 37 - iter 540/2703 - loss 0.21151454 - samples/sec: 4.55 - lr: 0.000001
2022-07-30 19:49:42,697 epoch 37 - iter 810/2703 - loss 0.20779573 - samples/sec: 4.36 - lr: 0.000001
2022-07-30 19:53:41,864 epoch 37 - iter 1080/2703 - loss 0.20867967 - samples/sec: 4.52 - lr: 0.000001
2022-07-30 19:57:43,333 epoch 37 - iter 1350/2703 - loss 0.20771225 - samples/sec: 4.47 - lr: 0.000000
2022-07-30 20:01:40,511 epoch 37 - iter 1620/2703 - loss 0.20796057 - samples/sec: 4.55 - lr: 0.000000
2022-07-30 20:05:35,667 epoch 37 - iter 1890/2703 - loss 0.20707213 - samples/sec: 4.59 - lr: 0.000000
2022-07-30 20:09:27,093 epoch 37 - iter 2160/2703 - loss 0.20778200 - samples/sec: 4.67 - lr: 0.000000
2022-07-30 20:13:24,778 epoch 37 - iter 2430/2703 - loss 0.20802524 - samples/sec: 4.54 - lr: 0.000000
2022-07-30 20:17:14,600 epoch 37 - iter 2700/2703 - loss 0.20973600 - samples/sec: 4.70 - lr: 0.000000
2022-07-30 20:17:17,776 ----------------------------------------------------------------------------------------------------
2022-07-30 20:17:17,776 EPOCH 37 done: loss 0.2096 - lr 0.000000
2022-07-30 20:23:49,923 Evaluating as a multi-label problem: False
2022-07-30 20:23:49,972 DEV : loss 0.04274332895874977 - f1-score (micro avg)  0.9762
2022-07-30 20:23:50,294 BAD EPOCHS (no improvement): 4
2022-07-30 20:23:50,301 ----------------------------------------------------------------------------------------------------
2022-07-30 20:27:38,242 epoch 38 - iter 270/2703 - loss 0.21456612 - samples/sec: 4.74 - lr: 0.000000
2022-07-30 20:31:44,483 epoch 38 - iter 540/2703 - loss 0.21249749 - samples/sec: 4.39 - lr: 0.000000
2022-07-30 20:35:35,980 epoch 38 - iter 810/2703 - loss 0.21060071 - samples/sec: 4.67 - lr: 0.000000
2022-07-30 20:39:41,069 epoch 38 - iter 1080/2703 - loss 0.20892357 - samples/sec: 4.41 - lr: 0.000000
2022-07-30 20:43:38,692 epoch 38 - iter 1350/2703 - loss 0.20969626 - samples/sec: 4.55 - lr: 0.000000
2022-07-30 20:47:35,359 epoch 38 - iter 1620/2703 - loss 0.20938233 - samples/sec: 4.56 - lr: 0.000000
2022-07-30 20:51:22,232 epoch 38 - iter 1890/2703 - loss 0.20959589 - samples/sec: 4.76 - lr: 0.000000
2022-07-30 20:55:16,466 epoch 38 - iter 2160/2703 - loss 0.20869137 - samples/sec: 4.61 - lr: 0.000000
2022-07-30 20:59:10,292 epoch 38 - iter 2430/2703 - loss 0.20803619 - samples/sec: 4.62 - lr: 0.000000
2022-07-30 21:03:10,257 epoch 38 - iter 2700/2703 - loss 0.20793129 - samples/sec: 4.50 - lr: 0.000000
2022-07-30 21:03:12,443 ----------------------------------------------------------------------------------------------------
2022-07-30 21:03:12,443 EPOCH 38 done: loss 0.2079 - lr 0.000000
2022-07-30 21:09:38,474 Evaluating as a multi-label problem: False
2022-07-30 21:09:38,531 DEV : loss 0.04313035309314728 - f1-score (micro avg)  0.9765
2022-07-30 21:09:38,856 BAD EPOCHS (no improvement): 4
2022-07-30 21:09:38,859 ----------------------------------------------------------------------------------------------------
2022-07-30 21:13:32,753 epoch 39 - iter 270/2703 - loss 0.20243046 - samples/sec: 4.62 - lr: 0.000000
2022-07-30 21:17:31,378 epoch 39 - iter 540/2703 - loss 0.21099202 - samples/sec: 4.53 - lr: 0.000000
2022-07-30 21:21:26,131 epoch 39 - iter 810/2703 - loss 0.21326238 - samples/sec: 4.60 - lr: 0.000000
2022-07-30 21:25:26,405 epoch 39 - iter 1080/2703 - loss 0.21310802 - samples/sec: 4.50 - lr: 0.000000
2022-07-30 21:29:26,057 epoch 39 - iter 1350/2703 - loss 0.21216897 - samples/sec: 4.51 - lr: 0.000000
2022-07-30 21:33:22,876 epoch 39 - iter 1620/2703 - loss 0.21357678 - samples/sec: 4.56 - lr: 0.000000
2022-07-30 21:37:26,410 epoch 39 - iter 1890/2703 - loss 0.21410274 - samples/sec: 4.44 - lr: 0.000000
2022-07-30 21:41:24,244 epoch 39 - iter 2160/2703 - loss 0.21315948 - samples/sec: 4.54 - lr: 0.000000
2022-07-30 21:45:12,641 epoch 39 - iter 2430/2703 - loss 0.21239359 - samples/sec: 4.73 - lr: 0.000000
2022-07-30 21:49:12,923 epoch 39 - iter 2700/2703 - loss 0.21162155 - samples/sec: 4.50 - lr: 0.000000
2022-07-30 21:49:15,085 ----------------------------------------------------------------------------------------------------
2022-07-30 21:49:15,085 EPOCH 39 done: loss 0.2117 - lr 0.000000
2022-07-30 21:55:44,226 Evaluating as a multi-label problem: False
2022-07-30 21:55:44,276 DEV : loss 0.042696304619312286 - f1-score (micro avg)  0.9762
2022-07-30 21:55:44,600 BAD EPOCHS (no improvement): 4
2022-07-30 21:55:44,606 ----------------------------------------------------------------------------------------------------
2022-07-30 21:59:39,811 epoch 40 - iter 270/2703 - loss 0.21126548 - samples/sec: 4.59 - lr: 0.000000
2022-07-30 22:03:36,745 epoch 40 - iter 540/2703 - loss 0.21355691 - samples/sec: 4.56 - lr: 0.000000
2022-07-30 22:07:39,313 epoch 40 - iter 810/2703 - loss 0.21629176 - samples/sec: 4.45 - lr: 0.000000
2022-07-30 22:11:37,669 epoch 40 - iter 1080/2703 - loss 0.21345458 - samples/sec: 4.53 - lr: 0.000000
2022-07-30 22:15:46,631 epoch 40 - iter 1350/2703 - loss 0.21207679 - samples/sec: 4.34 - lr: 0.000000
2022-07-30 22:19:38,965 epoch 40 - iter 1620/2703 - loss 0.21109131 - samples/sec: 4.65 - lr: 0.000000
2022-07-30 22:23:34,090 epoch 40 - iter 1890/2703 - loss 0.21127361 - samples/sec: 4.59 - lr: 0.000000
2022-07-30 22:27:37,312 epoch 40 - iter 2160/2703 - loss 0.21149421 - samples/sec: 4.44 - lr: 0.000000
2022-07-30 22:31:33,845 epoch 40 - iter 2430/2703 - loss 0.21023827 - samples/sec: 4.57 - lr: 0.000000
2022-07-30 22:35:30,709 epoch 40 - iter 2700/2703 - loss 0.21063925 - samples/sec: 4.56 - lr: 0.000000
2022-07-30 22:35:32,897 ----------------------------------------------------------------------------------------------------
2022-07-30 22:35:32,897 EPOCH 40 done: loss 0.2107 - lr 0.000000
2022-07-30 22:42:03,531 Evaluating as a multi-label problem: False
2022-07-30 22:42:03,582 DEV : loss 0.04255972057580948 - f1-score (micro avg)  0.9762
2022-07-30 22:42:03,906 BAD EPOCHS (no improvement): 4
2022-07-30 22:42:07,464 ----------------------------------------------------------------------------------------------------
2022-07-30 22:42:07,466 loading file experiments/corpus_sentence_grid_search_roberta_docstart/an_wh_rs_False_dpt_0_emb_xlm-roberta-large-cased-context_FT_True_Ly_-1_seed_33_lr_5e-06_it_40_bs_4_opti_AdamW_pjct_emb_False_sdl_LinearSchedulerWithWarmup_use_crf_False_use_rnn_False_wup_0.1/0/best-model.pt
2022-07-30 22:42:18,072 SequenceTagger predicts: Dictionary with 89 tags: O, S-TERRITORIO, B-TERRITORIO, E-TERRITORIO, I-TERRITORIO, S-FECHAS, B-FECHAS, E-FECHAS, I-FECHAS, S-EDAD_SUJETO_ASISTENCIA, B-EDAD_SUJETO_ASISTENCIA, E-EDAD_SUJETO_ASISTENCIA, I-EDAD_SUJETO_ASISTENCIA, S-NOMBRE_SUJETO_ASISTENCIA, B-NOMBRE_SUJETO_ASISTENCIA, E-NOMBRE_SUJETO_ASISTENCIA, I-NOMBRE_SUJETO_ASISTENCIA, S-NOMBRE_PERSONAL_SANITARIO, B-NOMBRE_PERSONAL_SANITARIO, E-NOMBRE_PERSONAL_SANITARIO, I-NOMBRE_PERSONAL_SANITARIO, S-SEXO_SUJETO_ASISTENCIA, B-SEXO_SUJETO_ASISTENCIA, E-SEXO_SUJETO_ASISTENCIA, I-SEXO_SUJETO_ASISTENCIA, S-CALLE, B-CALLE, E-CALLE, I-CALLE, S-PAIS, B-PAIS, E-PAIS, I-PAIS, S-ID_SUJETO_ASISTENCIA, B-ID_SUJETO_ASISTENCIA, E-ID_SUJETO_ASISTENCIA, I-ID_SUJETO_ASISTENCIA, S-ID_TITULACION_PERSONAL_SANITARIO, B-ID_TITULACION_PERSONAL_SANITARIO, E-ID_TITULACION_PERSONAL_SANITARIO, I-ID_TITULACION_PERSONAL_SANITARIO, S-CORREO_ELECTRONICO, B-CORREO_ELECTRONICO, E-CORREO_ELECTRONICO, I-CORREO_ELECTRONICO, S-ID_ASEGURAMIENTO, B-ID_ASEGURAMIENTO, E-ID_ASEGURAMIENTO, I-ID_ASEGURAMIENTO, S-HOSPITAL
2022-07-30 22:48:32,449 Evaluating as a multi-label problem: False
2022-07-30 22:48:32,499 0.9723	0.9792	0.9757	0.9578
2022-07-30 22:48:32,500 
Results:
- F-score (micro) 0.9757
- F-score (macro) 0.8728
- Accuracy 0.9578

By class:
                                  precision    recall  f1-score   support

                      TERRITORIO     0.9811    0.9759    0.9785       956
                          FECHAS     0.9951    0.9967    0.9959       611
          EDAD_SUJETO_ASISTENCIA     0.9755    0.9981    0.9866       518
        NOMBRE_SUJETO_ASISTENCIA     0.9960    1.0000    0.9980       502
       NOMBRE_PERSONAL_SANITARIO     0.9940    0.9960    0.9950       501
          SEXO_SUJETO_ASISTENCIA     0.9892    0.9935    0.9913       461
                           CALLE     0.9569    0.9685    0.9627       413
                            PAIS     0.9731    0.9972    0.9850       363
            ID_SUJETO_ASISTENCIA     0.9791    0.9929    0.9860       283
              CORREO_ELECTRONICO     0.9920    0.9960    0.9940       249
ID_TITULACION_PERSONAL_SANITARIO     0.9957    1.0000    0.9979       234
                ID_ASEGURAMIENTO     1.0000    0.9949    0.9975       198
                        HOSPITAL     0.9583    0.8846    0.9200       130
    FAMILIARES_SUJETO_ASISTENCIA     0.7500    0.8148    0.7811        81
                     INSTITUCION     0.5652    0.5821    0.5735        67
         ID_CONTACTO_ASISTENCIAL     0.9268    0.9744    0.9500        39
                 NUMERO_TELEFONO     0.8889    0.9231    0.9057        26
                       PROFESION     0.4737    1.0000    0.6429         9
                      NUMERO_FAX     0.6364    1.0000    0.7778         7
                    CENTRO_SALUD     1.0000    0.8333    0.9091         6
         OTROS_SUJETO_ASISTENCIA     0.0000    0.0000    0.0000         7

                       micro avg     0.9723    0.9792    0.9757      5661
                       macro avg     0.8584    0.9011    0.8728      5661
                    weighted avg     0.9726    0.9792    0.9756      5661

2022-07-30 22:48:32,500 ----------------------------------------------------------------------------------------------------
